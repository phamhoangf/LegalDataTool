# ğŸ›ï¸ Legal Data Tool

CÃ´ng cá»¥ xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam vá»›i AI

## ğŸ“‹ Tá»•ng quan

**Legal Data Tool** lÃ  há»‡ thá»‘ng hoÃ n chá»‰nh Ä‘á»ƒ xá»­ lÃ½, phÃ¢n tÃ­ch vÃ  sinh dá»¯ liá»‡u tá»« vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam. Há»‡ thá»‘ng há»— trá»£ upload multi-format files, phÃ¢n tÃ­ch cáº¥u trÃºc phÃ¡p lÃ½, sinh QA tá»± Ä‘á»™ng vÃ  theo dÃµi Ä‘á»™ bao phá»§ dá»¯ liá»‡u.

## âœ¨ TÃ­nh nÄƒng chÃ­nh

### ğŸ“„ **Xá»­ lÃ½ vÄƒn báº£n Ä‘a dáº¡ng**
- **Upload files**: `.doc`, `.docx`, `.pdf`, `.txt`
- **Parse tá»± Ä‘á»™ng**: PhÃ¢n tÃ­ch cáº¥u trÃºc Ä‘iá»u, khoáº£n, Ä‘iá»ƒm
- **Import CSV**: TÃ­ch há»£p dá»¯ liá»‡u tá»« nguá»“n cÃ³ sáºµn
- **Text processing**: Xá»­ lÃ½ encoding tá»± Ä‘á»™ng

### ï¿½ **AI-Powered Generation**
- **Multi-LLM support**: Gemini, Qwen, HuggingFace Cloud
- **Batch generation**: Tá»‘i Æ°u API calls
- **Smart sampling**: Monte Carlo vá»›i weighted selection
- **Context-aware**: Sinh QA dá»±a trÃªn cáº¥u trÃºc phÃ¡p lÃ½

### ğŸ” **Advanced Analysis**
- **Hybrid search**: Vector + keyword search  
- **Coverage analysis**: Theo dÃµi Ä‘á»™ bao phá»§ QA
- **Similarity detection**: Lá»c trÃ¹ng láº·p thÃ´ng minh
- **Statistics dashboard**: BÃ¡o cÃ¡o chi tiáº¿t

### ğŸ“Š **Data Management**
- **Smart labeling**: Giao diá»‡n gÃ¡n nhÃ£n intuitive
- **Export formats**: JSONL, CSV, structured data
- **Version control**: Theo dÃµi changes vÃ  history
- **Quality assurance**: Validation vÃ  quality checks  

## ğŸš€ CÃ i Ä‘áº·t

### PhÆ°Æ¡ng phÃ¡p 1: Quick Start (Khuyáº¿n nghá»‹)
```powershell
# Windows PowerShell
.\setup.ps1      # Tá»± Ä‘á»™ng cÃ i Ä‘áº·t dependencies
.\start.bat      # Khá»Ÿi cháº¡y cáº£ frontend vÃ  backend
```

### PhÆ°Æ¡ng phÃ¡p 2: Manual Setup
```bash
# 1. Backend Setup
cd backend
pip install -r requirements.txt

# 2. Frontend Setup  
cd ../frontend
npm install

# 3. Environment Configuration
# Táº¡o .env vÃ  thÃªm API keys:
GOOGLE_API_KEY=your_gemini_api_key
GEMINI_API_KEY=your_gemini_api_key

# 4. Initialize Database
cd ../backend
python -c "from app import app, db; app.app_context().push(); db.create_all()"

# 5. Start Services
python app.py        # Backend: http://localhost:5000
cd ../frontend
npm start           # Frontend: http://localhost:3000
```

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
DataTool/
â”œâ”€â”€ backend/                    # ğŸ Flask API Server
â”‚   â”œâ”€â”€ app.py                    # Main application & routes
â”‚   â”œâ”€â”€ models.py                 # SQLAlchemy database models  
â”‚   â”œâ”€â”€ config.py                 # Environment configuration
â”‚   â”œâ”€â”€ data_generator.py         # ğŸ¤– AI data generation engine
â”‚   â”œâ”€â”€ document_parsers.py       # ğŸ“„ Legal document parsing
â”‚   â”œâ”€â”€ file_handler.py           # ğŸ“ Multi-format file processing
â”‚   â”œâ”€â”€ hybrid_search.py          # ğŸ” Vector + keyword search
â”‚   â”œâ”€â”€ similarity_checker.py     # ğŸ¯ Duplicate detection
â”‚   â”œâ”€â”€ coverage_analyzer.py      # ğŸ“Š Coverage analysis
â”‚   â”œâ”€â”€ vanban_csv.py            # ğŸ“‹ CSV data integration
â”‚   â””â”€â”€ instance/                 # SQLite database storage
â”‚       â””â”€â”€ legal_data.db
â”œâ”€â”€ frontend/                   # âš›ï¸ React UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/           # UI Components
â”‚   â”‚   â”‚   â”œâ”€â”€ HomePage.js         # Dashboard tá»•ng quan
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentManagement.js # Quáº£n lÃ½ vÄƒn báº£n
â”‚   â”‚   â”‚   â”œâ”€â”€ DataGeneration.js   # Sinh dá»¯ liá»‡u QA
â”‚   â”‚   â”‚   â”œâ”€â”€ DataLabeling.js     # GÃ¡n nhÃ£n dá»¯ liá»‡u
â”‚   â”‚   â”‚   â”œâ”€â”€ CoverageDemo.js     # PhÃ¢n tÃ­ch coverage
â”‚   â”‚   â”‚   â”œâ”€â”€ Statistics.js       # Thá»‘ng kÃª & bÃ¡o cÃ¡o
â”‚   â”‚   â”‚   â””â”€â”€ DataExport.js       # Xuáº¥t dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ services/             # API integration
â”‚   â”‚   â””â”€â”€ hooks/                # React hooks
â”‚   â””â”€â”€ public/                   # Static assets
â”œâ”€â”€ data/                       # ğŸ’¾ Data storage
â”‚   â”œâ”€â”€ van_ban_phap_luat_async.csv # Source legal documents
â”‚   â””â”€â”€ exports/                  # Generated datasets
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.ps1                  # Automated setup script
â”œâ”€â”€ start.bat                  # Launch script
â””â”€â”€ README.md                  # Documentation
```

## ğŸ¯ Workflow sá»­ dá»¥ng

### 1. ğŸ  **Dashboard Overview**
- Xem tá»•ng quan há»‡ thá»‘ng
- Theo dÃµi thá»‘ng kÃª documents, QA pairs
- Quick access cÃ¡c tÃ­nh nÄƒng chÃ­nh

### 2. ğŸ“ **Document Management**
- **Upload files**: Drag & drop `.doc`, `.docx`, `.pdf`, `.txt`
- **CSV integration**: Import tá»« nguá»“n dá»¯ liá»‡u cÃ³ sáºµn
- **Auto parsing**: Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¢n tÃ­ch cáº¥u trÃºc
- **View structure**: Xem hierarchy Ä‘iá»u/khoáº£n/Ä‘iá»ƒm

### 3. ğŸ¤– **Data Generation**
- **Select documents**: Chá»n vÄƒn báº£n cáº§n sinh QA
- **Configure parameters**: 
  - Sá»‘ lÆ°á»£ng cÃ¢u há»i (5-10 per batch)
  - Focus areas (specific articles/sections)
  - Question types (factual/analytical/procedural)
- **AI processing**: Multi-LLM generation vá»›i context
- **Real-time monitoring**: Track progress vÃ  quality

### 4. ğŸ·ï¸ **Data Labeling**
- **Review interface**: User-friendly labeling UI
- **Quality control**: Accept/Reject/Edit generated QA
- **Bulk operations**: Mass labeling vá»›i filters
- **Export ready data**: Chá»‰ export data Ä‘Ã£ Ä‘Æ°á»£c label

### 5. ğŸ“Š **Coverage Analysis**
- **Document coverage**: Xem % vÄƒn báº£n Ä‘Ã£ cÃ³ QA
- **Gap analysis**: Identify sections cáº§n thÃªm QA
- **Quality metrics**: Similarity scores, duplicates
- **Visual reports**: Charts vÃ  heatmaps

### 6. ğŸ“¤ **Data Export**
- **Multiple formats**: JSONL, CSV, structured JSON
- **Filter options**: By document, date, quality score
- **Preview before export**: Kiá»ƒm tra data trÆ°á»›c khi táº£i
- **Batch download**: Export large datasets

## ğŸš€ Quick Start Guide

```bash
# 1. Launch system
.\start.bat

# 2. Access web interface
# Frontend: http://localhost:3000
# Backend API: http://localhost:5000

# 3. Upload your first document
# Go to Document Management â†’ Upload File

# 4. Generate QA pairs
# Go to Data Generation â†’ Select Document â†’ Generate

# 5. Label and export
# Go to Data Labeling â†’ Review â†’ Export Data
```

## ğŸ“‹ Data Formats & Examples

### QA Pairs (Question-Answer)
```json
{
  "id": "qa_001",
  "question": "Má»©c pháº¡t tiá»n Ä‘á»‘i vá»›i hÃ nh vi lÃ¡i xe khÃ´ng cÃ³ giáº¥y phÃ©p lÃ  bao nhiÃªu?",
  "answer": "Theo Nghá»‹ Ä‘á»‹nh 100/2019, má»©c pháº¡t tá»« 8.000.000Ä‘ Ä‘áº¿n 12.000.000Ä‘",
  "source": {
    "document_title": "Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP",
    "unit_path": "Äiá»u 6, Khoáº£n 1, Äiá»ƒm a",
    "unit_id": "dieu_6_khoan_1_diem_a"
  },
  "metadata": {
    "generated_at": "2025-09-19T10:30:00Z",
    "model": "gemini-1.5-pro",
    "similarity_score": 0.95,
    "labeled": true,
    "label_status": "approved"
  }
}
```

### Document Structure
```json
{
  "document_id": "doc_001",
  "title": "Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™ 2008",
  "parsed_structure": {
    "articles": [
      {
        "article_number": "1",
        "title": "Pháº¡m vi Ä‘iá»u chá»‰nh",
        "content": "Luáº­t nÃ y quy Ä‘á»‹nh vá»...",
        "clauses": [
          {
            "clause_number": "1", 
            "content": "Giao thÃ´ng Ä‘Æ°á»ng bá»™ bao gá»“m...",
            "points": []
          }
        ]
      }
    ]
  },
  "articles_count": 85,
  "file_type": "docx",
  "uploaded_at": "2025-09-19T10:00:00Z"
}
```

### Export Formats

**JSONL for Training:**
```jsonl
{"instruction": "Question 1", "output": "Answer 1", "source": "Article 1"}
{"instruction": "Question 2", "output": "Answer 2", "source": "Article 2"}
```

**CSV for Analysis:**
```csv
id,question,answer,document,article,similarity_score,label_status
1,"Question 1","Answer 1","Doc 1","Article 1",0.95,approved
2,"Question 2","Answer 2","Doc 1","Article 2",0.87,pending
```

## ğŸ”Œ API Reference

### Document Management
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/documents` | List all documents |
| `POST` | `/api/documents` | Create new document |
| `POST` | `/api/documents/upload` | Upload file (multi-format) |
| `GET` | `/api/documents/{id}` | Get document details |
| `DELETE` | `/api/documents/{id}` | Delete document |

### Data Generation
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/generate` | Generate QA pairs |
| `GET` | `/api/qa-pairs` | List generated QA pairs |
| `GET` | `/api/qa-pairs/{id}` | Get QA pair details |
| `PUT` | `/api/qa-pairs/{id}/label` | Update QA pair label |

### Analysis & Export
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/coverage` | Get coverage analysis |
| `GET` | `/api/statistics` | System statistics |
| `POST` | `/api/export` | Export data (JSONL/CSV) |
| `GET` | `/api/search/documents` | Search documents |
| `GET` | `/api/search/vanban` | Search CSV data |

### System
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/health` | API health check |
| `GET` | `/api/supported-formats` | Supported file formats |

## âš™ï¸ Configuration

### Environment Variables
Create `.env` file in project root:
```bash
# AI Model Configuration
GOOGLE_API_KEY=your_gemini_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Application Settings
FLASK_ENV=development
DATABASE_URL=sqlite:///legal_data.db
SECRET_KEY=your_secret_key_here

# File Upload Settings
UPLOAD_FOLDER=uploads
MAX_CONTENT_LENGTH=16777216  # 16MB

# CORS Settings
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
```

### AI Model Setup
**Gemini API:**
1. Visit [Google AI Studio](https://aistudio.google.com)
2. Create new API key
3. Add to `.env` file

**HuggingFace Cloud (Optional):**
- Configure in `test_ngrok.py` for cloud inference
- Used when local GPU unavailable

## ğŸ”§ Technical Architecture

### Backend Components
```
ğŸ Flask Application (app.py)
â”œâ”€â”€ ğŸ“Š Models (models.py)
â”‚   â”œâ”€â”€ LegalDocument
â”‚   â”œâ”€â”€ QAPair  
â”‚   â”œâ”€â”€ LegalTopic
â”‚   â””â”€â”€ TopicDocument
â”œâ”€â”€ ğŸ¤– AI Engine (data_generator.py)
â”‚   â”œâ”€â”€ Multi-LLM support
â”‚   â”œâ”€â”€ Batch generation
â”‚   â”œâ”€â”€ Monte Carlo sampling
â”‚   â””â”€â”€ Context management
â”œâ”€â”€ ğŸ“„ Document Processing
â”‚   â”œâ”€â”€ file_handler.py (multi-format)
â”‚   â”œâ”€â”€ document_parsers.py (structure)
â”‚   â””â”€â”€ vanban_csv.py (CSV integration)
â”œâ”€â”€ ğŸ” Analysis Engine
â”‚   â”œâ”€â”€ hybrid_search.py (vector + keyword)
â”‚   â”œâ”€â”€ similarity_checker.py (duplicates)
â”‚   â””â”€â”€ coverage_analyzer.py (gap analysis)
â””â”€â”€ ğŸ’¾ Database (SQLite)
    â””â”€â”€ legal_data.db
```

### Frontend Architecture
```
âš›ï¸ React Application
â”œâ”€â”€ ğŸ  HomePage (dashboard)
â”œâ”€â”€ ğŸ“ DocumentManagement (CRUD)
â”œâ”€â”€ ğŸ¤– DataGeneration (AI-powered)
â”œâ”€â”€ ğŸ·ï¸ DataLabeling (manual review)
â”œâ”€â”€ ï¿½ CoverageDemo (analysis)
â”œâ”€â”€ ğŸ“ˆ Statistics (reports)
â”œâ”€â”€ ğŸ“¤ DataExport (download)
â””â”€â”€ ğŸ”§ Services (API integration)
```

### Key Features
- **Multi-format Support**: `.doc`, `.docx`, `.pdf`, `.txt`
- **Smart Parsing**: Legal structure recognition
- **Batch Processing**: Optimized AI generation
- **Real-time Analysis**: Coverage vÃ  similarity
- **Responsive UI**: Modern React interface

## ğŸ§ª Testing & Development

### Initialize Sample Data
```bash
cd backend
python -c "from app import app, db; app.app_context().push(); db.create_all()"
```

### Test AI Models
```bash
# Test Gemini API
python test_ngrok.py

# Check supported file formats
python -c "from file_handler import get_supported_formats; print(get_supported_formats())"
```

### Development Tools
```bash
# Database reset (if needed)
rm instance/legal_data.db
python -c "from app import app, db; app.app_context().push(); db.create_all()"

# Check system status
curl http://localhost:5000/api/health

# View database contents
python -c "from app import app, db, LegalDocument; app.app_context().push(); print(LegalDocument.query.count())"
```

## ğŸ”§ Troubleshooting

### Common Issues

**ğŸš« Backend startup fails:**
```bash
# Install dependencies
pip install -r requirements.txt

# Check Python version (requires 3.8+)
python --version

# Verify database
python -c "from app import app, db; app.app_context().push(); db.create_all()"
```

**ğŸš« Frontend startup fails:**
```bash
cd frontend
npm install --force
npm audit fix
npm start
```

**ğŸš« File upload errors:**
- Check file format support (`.doc` requires `docx2txt`)
- Verify file size (< 16MB)
- Check upload folder permissions

**ğŸš« AI generation fails:**
- Verify `GOOGLE_API_KEY` in `.env`
- Check API quota at [Google AI Studio](https://aistudio.google.com)
- Review backend logs for specific errors

**ğŸš« Database issues:**
```bash
# Check database connection
python -c "from app import app, db; app.app_context().push(); print('DB connected:', db.engine.url)"

# Reset if corrupted
rm instance/legal_data.db
python -c "from app import app, db; app.app_context().push(); db.create_all()"
```

## ğŸ“š Documentation

### Project Structure Details
- **Backend**: Flask REST API with SQLAlchemy ORM
- **Frontend**: React SPA with Ant Design UI
- **Database**: SQLite for development, PostgreSQL for production
- **AI Models**: Multi-provider support (Gemini, Qwen, HuggingFace)

### Development Roadmap
- [ ] PostgreSQL production support
- [ ] User authentication & authorization  
- [ ] Advanced analytics dashboard
- [ ] Batch file processing
- [ ] API rate limiting
- [ ] Export scheduling

## ğŸ“„ License

MIT License - Free for educational and research purposes.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## Link Data
https://huggingface.co/datasets/phamhoangf/legal_generated_data

## ğŸ“ Support

For issues and questions:
- Create GitHub issue
- Check documentation
- Review troubleshooting guide

---

**Legal Data Tool** - Empowering legal document analysis with AI 
