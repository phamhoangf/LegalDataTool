from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from flask_sqlalchemy import SQLAlchemy
from dotenv import load_dotenv
import os
import json
import jsonlines
from datetime import datetime
from data_generator import DataGenerator
from coverage_analyzer import CoverageAnalyzer
from document_parsers import LegalDocumentParser
from models import db, LegalTopic, LegalDocument, TopicDocument, GeneratedData, LabeledData
from vanban_csv import VanBanCSVReader

# Load .env file t·ª´ parent directory
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL', 'sqlite:///legal_data.db')
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# Initialize extensions
CORS(app)
db.init_app(app)

# Initialize data generator with Google API key and similarity threshold
google_api_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
data_generator = DataGenerator(api_key=google_api_key, similarity_threshold=0.75)

# Initialize legal document parser
legal_parser = LegalDocumentParser()

# Initialize CSV reader
vanban_csv = VanBanCSVReader(os.path.join(os.path.dirname(__file__), "..", "data", "van_ban_phap_luat_async.csv"))

# Global variable ƒë·ªÉ l∆∞u coverage analyzer instances  
coverage_analyzers = {}

@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({"status": "healthy", "timestamp": datetime.now().isoformat()})

@app.route('/api/topics', methods=['GET'])
def get_topics():
    """L·∫•y danh s√°ch ch·ªß ƒë·ªÅ ph√°p l√Ω v·ªõi documents"""
    topics = LegalTopic.query.all()
    result = []
    
    for topic in topics:
        # L·∫•y t·∫•t c·∫£ documents li√™n quan
        documents = db.session.query(LegalDocument).join(TopicDocument).filter(
            TopicDocument.topic_id == topic.id
        ).all()
        
        # Aggregate legal text t·ª´ t·∫•t c·∫£ documents
        legal_text = '\n\n'.join([doc.content for doc in documents])
        
        result.append({
            'id': topic.id,
            'name': topic.name,
            'description': topic.description,
            'legal_text': legal_text,  # ƒê·ªÉ t∆∞∆°ng th√≠ch v·ªõi frontend
            'document_count': len(documents),
            'documents': [{'id': doc.id, 'title': doc.title} for doc in documents],
            'created_at': topic.created_at.isoformat()
        })
    
    return jsonify(result)

@app.route('/api/topics', methods=['POST'])
def create_topic():
    """T·∫°o ch·ªß ƒë·ªÅ ph√°p l√Ω m·ªõi"""
    data = request.get_json()
    
    topic = LegalTopic(
        name=data['name'],
        description=data.get('description', '')
    )
    
    db.session.add(topic)
    db.session.commit()
    
    return jsonify({
        'id': topic.id,
        'name': topic.name,
        'description': topic.description,
        'document_count': 0,
        'documents': [],
        'message': 'Ch·ªß ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng'
    }), 201

@app.route('/api/topics/<int:topic_id>', methods=['PUT'])
def update_topic(topic_id):
    """C·∫≠p nh·∫≠t ch·ªß ƒë·ªÅ ph√°p l√Ω"""
    topic = LegalTopic.query.get_or_404(topic_id)
    data = request.get_json()
    
    topic.name = data.get('name', topic.name)
    topic.description = data.get('description', topic.description)
    topic.legal_text = data.get('legal_text', topic.legal_text)
    
    db.session.commit()
    
    return jsonify({
        'id': topic.id,
        'name': topic.name,
        'description': topic.description,
        'legal_text': topic.legal_text,
        'message': 'Ch·ªß ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√†nh c√¥ng'
    })

@app.route('/api/topics/<int:topic_id>', methods=['DELETE'])
def delete_topic(topic_id):
    """X√≥a ch·ªß ƒë·ªÅ ph√°p l√Ω"""
    topic = LegalTopic.query.get_or_404(topic_id)
    
    # X√≥a t·∫•t c·∫£ d·ªØ li·ªáu li√™n quan
    TopicDocument.query.filter_by(topic_id=topic_id).delete()
    GeneratedData.query.filter_by(topic_id=topic_id).delete()
    
    db.session.delete(topic)
    db.session.commit()
    
    return jsonify({
        'message': 'Ch·ªß ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng'
    })

@app.route('/api/documents', methods=['GET'])
def get_documents():
    """L·∫•y danh s√°ch t√†i li·ªáu"""
    documents = LegalDocument.query.all()
    result = []
    
    for doc in documents:
        # L·∫•y c√°c topic li√™n k·∫øt
        topic_docs = TopicDocument.query.filter_by(document_id=doc.id).all()
        topics = []
        for td in topic_docs:
            topic = LegalTopic.query.get(td.topic_id)
            if topic:
                topics.append({
                    'id': topic.id,
                    'name': topic.name
                })
        
        # Parse articles count from parsed_structure
        articles_count = doc.articles_count or 0  # Use pre-calculated field

        result.append({
            'id': doc.id,
            'title': doc.title,
            'content': doc.content,
            'document_type': doc.document_type,
            'document_number': doc.document_number,
            'uploaded_at': doc.uploaded_at.isoformat(),
            'created_at': doc.uploaded_at.isoformat(),
            'articles_count': articles_count,
            'topics': topics
        })
    
    return jsonify(result)

@app.route('/api/documents/<int:doc_id>', methods=['GET'])
def get_document(doc_id):
    """L·∫•y th√¥ng tin chi ti·∫øt t√†i li·ªáu"""
    doc = LegalDocument.query.get(doc_id)
    if not doc:
        return jsonify({'error': 'Document not found'}), 404
    
    # L·∫•y c√°c topic li√™n k·∫øt
    topic_docs = TopicDocument.query.filter_by(document_id=doc.id).all()
    topics = []
    for td in topic_docs:
        topic = LegalTopic.query.get(td.topic_id)
        if topic:
            topics.append({
                'id': topic.id,
                'name': topic.name
            })
    
    result = {
        'id': doc.id,
        'title': doc.title,
        'content': doc.content,
        'document_type': doc.document_type,
        'document_number': doc.document_number,
        'uploaded_at': doc.uploaded_at.isoformat(),
        'created_at': doc.uploaded_at.isoformat(),
        'topics': topics
    }
    
    # Th√™m parsed_structure n·∫øu c√≥
    if doc.parsed_structure:
        try:
            import json
            result['parsed_structure'] = json.loads(doc.parsed_structure)
            result['parsed'] = True
        except:
            result['parsed'] = False
    else:
        result['parsed'] = False
    
    return jsonify(result)

@app.route('/api/documents', methods=['POST'])
def create_document():
    """T·∫°o t√†i li·ªáu m·ªõi"""
    data = request.get_json()
    
    # Parse document structure ngay khi t·∫°o
    parsed_structure = None
    articles_count = 0
    try:
        print(f"üîÑ Parsing document: {data['title']}")
        structure = legal_parser.parse_document(data['title'], data['content'])
        parsed_structure = json.dumps(structure, ensure_ascii=False)
        # Calculate articles count for performance
        articles_count = len(structure.get('articles', []))
        print(f"‚úÖ Document parsed successfully - {articles_count} ƒëi·ªÅu")
    except Exception as e:
        print(f"‚ö†Ô∏è Parsing failed: {str(e)}")
    
    document = LegalDocument(
        title=data['title'],
        content=data['content'],
        parsed_structure=parsed_structure,
        document_type=data.get('document_type', 'law'),
        document_number=data.get('document_number', ''),
        uploaded_by=data.get('uploaded_by', 'system'),
        articles_count=articles_count
    )
    
    db.session.add(document)
    db.session.commit()
    
    return jsonify({
        'id': document.id,
        'title': document.title,
        'parsed': parsed_structure is not None,
        'message': 'T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng'
    }), 201

@app.route('/api/topics/<int:topic_id>/documents/<int:document_id>', methods=['POST'])
def link_document_to_topic(topic_id, document_id):
    """Li√™n k·∫øt t√†i li·ªáu v·ªõi ch·ªß ƒë·ªÅ"""
    topic = LegalTopic.query.get_or_404(topic_id)
    document = LegalDocument.query.get_or_404(document_id)
    
    # Ki·ªÉm tra ƒë√£ li√™n k·∫øt ch∆∞a
    existing = TopicDocument.query.filter_by(
        topic_id=topic_id, 
        document_id=document_id
    ).first()
    
    if existing:
        return jsonify({'message': 'T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c li√™n k·∫øt v·ªõi ch·ªß ƒë·ªÅ n√†y'}), 400
    
    topic_doc = TopicDocument(
        topic_id=topic_id,
        document_id=document_id,
        relevance_score=request.get_json().get('relevance_score', 1.0)
    )
    
    db.session.add(topic_doc)
    db.session.commit()
    
    return jsonify({
        'message': 'ƒê√£ li√™n k·∫øt t√†i li·ªáu v·ªõi ch·ªß ƒë·ªÅ th√†nh c√¥ng'
    }), 201

@app.route('/api/documents/<int:document_id>', methods=['PUT'])
def update_document(document_id):
    """C·∫≠p nh·∫≠t t√†i li·ªáu"""
    document = LegalDocument.query.get_or_404(document_id)
    data = request.get_json()
    
    # Check if content is being updated to recalculate articles_count
    content_updated = False
    
    if 'title' in data:
        document.title = data['title']
    if 'content' in data:
        document.content = data['content']
        content_updated = True
    if 'document_type' in data:
        document.document_type = data['document_type']
    
    # Recalculate articles_count if content was updated
    if content_updated:
        try:
            structure = legal_parser.parse_document(document.title, document.content)
            document.parsed_structure = json.dumps(structure, ensure_ascii=False)
            document.articles_count = len(structure.get('articles', []))
            print(f"‚úÖ Document updated with {document.articles_count} ƒëi·ªÅu")
        except Exception as e:
            print(f"‚ö†Ô∏è Parsing failed during update: {str(e)}")
            document.articles_count = 0
    
    document.updated_at = datetime.utcnow()
    db.session.commit()
    
    return jsonify({
        'id': document.id,
        'title': document.title,
        'message': 'T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√†nh c√¥ng'
    })

@app.route('/api/documents/<int:document_id>', methods=['DELETE'])
def delete_document(document_id):
    """X√≥a t√†i li·ªáu"""
    document = LegalDocument.query.get_or_404(document_id)
    
    # X√≥a c√°c li√™n k·∫øt v·ªõi ch·ªß ƒë·ªÅ tr∆∞·ªõc
    TopicDocument.query.filter_by(document_id=document_id).delete()
    
    # X√≥a t√†i li·ªáu
    db.session.delete(document)
    db.session.commit()
    
    return jsonify({
        'message': 'ƒê√£ x√≥a t√†i li·ªáu th√†nh c√¥ng'
    })

@app.route('/api/topics/<int:topic_id>/documents/<int:document_id>', methods=['DELETE'])
def unlink_document_from_topic(topic_id, document_id):
    """H·ªßy li√™n k·∫øt t√†i li·ªáu v·ªõi ch·ªß ƒë·ªÅ"""
    topic_doc = TopicDocument.query.filter_by(
        topic_id=topic_id, 
        document_id=document_id
    ).first_or_404()
    
    db.session.delete(topic_doc)
    db.session.commit()
    
    return jsonify({
        'message': 'ƒê√£ h·ªßy li√™n k·∫øt t√†i li·ªáu v·ªõi ch·ªß ƒë·ªÅ'
    })

@app.route('/api/documents/upload', methods=['POST'])
def upload_document_file():
    """Upload file t√†i li·ªáu m√† kh√¥ng c·∫ßn li√™n k·∫øt v·ªõi ch·ªß ƒë·ªÅ"""
    if 'file' not in request.files:
        return jsonify({'error': 'Kh√¥ng c√≥ file ƒë∆∞·ª£c t·∫£i l√™n'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'T√™n file kh√¥ng h·ª£p l·ªá'}), 400
    
    title = request.form.get('title', file.filename)
    document_type = request.form.get('document_type', 'law')
    
    # ƒê·ªçc n·ªôi dung file
    content = file.read().decode('utf-8', errors='ignore')
    
    # T·∫°o document
    document = LegalDocument(
        title=title,
        content=content,
        document_type=document_type
    )
    
    db.session.add(document)
    db.session.commit()
    
    return jsonify({
        'id': document.id,
        'title': document.title,
        'message': 'T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n th√†nh c√¥ng'
    }), 201

@app.route('/api/upload', methods=['POST'])
def upload_legal_document():
    """T·∫£i l√™n vƒÉn b·∫£n lu·∫≠t v√† li√™n k·∫øt v·ªõi ch·ªß ƒë·ªÅ"""
    if 'file' not in request.files:
        return jsonify({'error': 'Kh√¥ng c√≥ file ƒë∆∞·ª£c t·∫£i l√™n'}), 400
    
    file = request.files['file']
    topic_id = request.form.get('topic_id')
    document_title = request.form.get('title', file.filename)
    document_type = request.form.get('document_type', 'law')
    
    if file.filename == '':
        return jsonify({'error': 'Kh√¥ng c√≥ file ƒë∆∞·ª£c ch·ªçn'}), 400
    
    # ƒê·ªçc n·ªôi dung file
    content = file.read().decode('utf-8')
    
    # Parse document structure ngay khi upload
    parsed_structure = None
    try:
        print(f"üîÑ Parsing uploaded document: {document_title}")
        structure = legal_parser.parse_document(document_title, content)
        parsed_structure = json.dumps(structure, ensure_ascii=False)
        print(f"‚úÖ Uploaded document parsed successfully")
    except Exception as e:
        print(f"‚ö†Ô∏è Parsing failed: {str(e)}")
    
    # T·∫°o document m·ªõi
    document = LegalDocument(
        title=document_title,
        content=content,
        parsed_structure=parsed_structure,
        document_type=document_type,
        uploaded_by='user'
    )
    
    db.session.add(document)
    db.session.flush()  # ƒê·ªÉ c√≥ ID
    
    # Li√™n k·∫øt v·ªõi topic n·∫øu c√≥
    if topic_id:
        topic = LegalTopic.query.get(topic_id)
        if topic:
            topic_doc = TopicDocument(
                topic_id=int(topic_id),
                document_id=document.id,
                relevance_score=1.0
            )
            db.session.add(topic_doc)
    
    db.session.commit()
    
    return jsonify({
        'message': 'File ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n v√† li√™n k·∫øt th√†nh c√¥ng',
        'document_id': document.id,
        'content_length': len(content)
    })

@app.route('/api/generate', methods=['POST'])
def generate_training_data():
    """Sinh d·ªØ li·ªáu hu·∫•n luy·ªán v·ªõi 4 lo·∫°i reasoning"""
    data = request.get_json()
    
    topic_id = data.get('topic_id')
    data_type = data.get('data_type')
    num_samples = data.get('num_samples', 10)
    llm_type = data.get('llm_type', 'gemini')  # Default to gemini
    
    # Validate data_type - ch·ªâ ch·∫•p nh·∫≠n 4 lo·∫°i m·ªõi
    valid_types = ['word_matching', 'concept_understanding', 'multi_paragraph_reading', 'multi_hop_reasoning']
    valid_llm_types = ['gemini', 'huggingface']
    
    if not topic_id or not data_type:
        return jsonify({'error': 'Thi·∫øu topic_id ho·∫∑c data_type'}), 400
    
    if data_type not in valid_types:
        return jsonify({
            'error': f'data_type kh√¥ng h·ª£p l·ªá. Ch·ªâ ch·∫•p nh·∫≠n: {", ".join(valid_types)}'
        }), 400
    
    if llm_type not in valid_llm_types:
        return jsonify({
            'error': f'llm_type kh√¥ng h·ª£p l·ªá. Ch·ªâ ch·∫•p nh·∫≠n: {", ".join(valid_llm_types)}'
        }), 400
    
    topic = LegalTopic.query.get(topic_id)
    if not topic:
        return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y ch·ªß ƒë·ªÅ'}), 404
    
    # L·∫•y t·∫•t c·∫£ documents li√™n quan ƒë·∫øn topic
    documents = db.session.query(LegalDocument).join(TopicDocument).filter(
        TopicDocument.topic_id == topic_id
    ).all()
    
    if not documents:
        return jsonify({'error': 'Ch·ªß ƒë·ªÅ ch∆∞a c√≥ t√†i li·ªáu ph√°p lu·∫≠t n√†o'}), 400
    
    try:
        # C·∫≠p nh·∫≠t similarity corpus v·ªõi c√¢u h·ªèi hi·ªán c√≥
        existing_data = GeneratedData.query.filter_by(topic_id=topic_id).all()
        existing_questions = []
        for item in existing_data:
            existing_questions.append({
                'id': item.id,
                'data_type': item.data_type,
                'content': item.content
            })
        
        data_generator.update_similarity_corpus(existing_questions)
        
        # Initialize HuggingFace model n·∫øu c·∫ßn
        if llm_type == 'huggingface':
            try:
                if not hasattr(data_generator, 'hf_model') or data_generator.hf_model is None:
                    print(f"ü§ñ Initializing HuggingFace model for {llm_type}")
                    data_generator.init_huggingface_model()
            except Exception as e:
                return jsonify({'error': f'Kh√¥ng th·ªÉ kh·ªüi t·∫°o model HuggingFace: {str(e)}'}), 500
        
        # S·ª≠ d·ª•ng method m·ªõi v·ªõi article-based generation
        generated_samples = data_generator.generate_from_multiple_documents(
            documents, topic.name, data_type, num_samples, llm_type
        )
        
        # L∆∞u v√†o database v·ªõi metadata chi ti·∫øt
        for sample in generated_samples:
            # Extract metadata n·∫øu c√≥
            metadata = sample.pop('metadata', {})
            
            generated_data = GeneratedData(
                topic_id=topic_id,
                data_type=data_type,
                content=json.dumps({
                    **sample,
                    'metadata': metadata
                }, ensure_ascii=False)
            )
            db.session.add(generated_data)
        
        db.session.commit()
        
        # T·∫°o summary v·ªÅ document distribution
        document_summary = {}
        for sample in generated_samples:
            if 'metadata' in sample and 'source_document' in sample['metadata']:
                doc_name = sample['metadata']['source_document']
                document_summary[doc_name] = document_summary.get(doc_name, 0) + 1
        
        return jsonify({
            'message': f'ƒê√£ sinh {len(generated_samples)} m·∫´u d·ªØ li·ªáu {data_type} b·∫±ng {llm_type}',
            'total_samples': len(generated_samples),
            'llm_type': llm_type,
            'document_distribution': document_summary,
            'documents_used': [{'title': doc.title, 'id': doc.id} for doc in documents],
            'samples': generated_samples[:5]  # Ch·ªâ hi·ªÉn th·ªã 5 samples ƒë·∫ßu ƒë·ªÉ preview
        })
        
    except Exception as e:
        return jsonify({'error': f'L·ªói khi sinh d·ªØ li·ªáu: {str(e)}'}), 500

@app.route('/api/data/<int:topic_id>', methods=['GET'])
def get_generated_data(topic_id):
    """L·∫•y d·ªØ li·ªáu ƒë√£ sinh cho ch·ªß ƒë·ªÅ"""
    data_type = request.args.get('type')
    
    query = GeneratedData.query.filter_by(topic_id=topic_id)
    if data_type:
        query = query.filter_by(data_type=data_type)
    
    # S·∫Øp x·∫øp theo ID gi·∫£m d·∫ßn ƒë·ªÉ l·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t tr∆∞·ªõc
    data = query.order_by(GeneratedData.id.desc()).all()
    
    result = []
    for item in data:
        # Ki·ªÉm tra xem c√≥ label kh√¥ng
        has_label = LabeledData.query.filter_by(generated_data_id=item.id).first() is not None
        
        result.append({
            'id': item.id,
            'data_type': item.data_type,
            'content': json.loads(item.content),
            'created_at': item.created_at.isoformat(),
            'is_labeled': has_label
        })
    
    return jsonify(result)

@app.route('/api/label', methods=['POST'])
def label_data():
    """G√°n nh√£n cho d·ªØ li·ªáu"""
    data = request.get_json()
    
    data_id = data.get('data_id')
    label = data.get('label')  # 'accept', 'reject', 'modify'
    modified_content = data.get('modified_content')
    notes = data.get('notes', '')
    
    if not data_id or not label:
        return jsonify({'error': 'Thi·∫øu data_id ho·∫∑c label'}), 400
    
    # Ki·ªÉm tra xem ƒë√£ c√≥ label ch∆∞a
    existing_label = LabeledData.query.filter_by(generated_data_id=data_id).first()
    
    if existing_label:
        # C·∫≠p nh·∫≠t label hi·ªán c√≥
        existing_label.label = label
        existing_label.modified_content = modified_content
        existing_label.notes = notes
    else:
        # T·∫°o label m·ªõi
        labeled_data = LabeledData(
            generated_data_id=data_id,
            label=label,
            modified_content=modified_content,
            notes=notes
        )
        db.session.add(labeled_data)
    
    db.session.commit()
    
    return jsonify({'message': 'Nh√£n ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√†nh c√¥ng'})

@app.route('/api/export/<data_type>', methods=['GET'])
def export_data(data_type):
    """Xu·∫•t d·ªØ li·ªáu ƒë√£ g√°n nh√£n"""
    topic_id = request.args.get('topic_id')
    
    # Query ƒë∆°n gi·∫£n: l·∫•y t·∫•t c·∫£ d·ªØ li·ªáu ƒë∆∞·ª£c ch·∫•p nh·∫≠n ho·∫∑c s·ª≠a ƒë·ªïi
    query = db.session.query(GeneratedData, LabeledData).join(
        LabeledData, GeneratedData.id == LabeledData.generated_data_id
    ).filter(
        GeneratedData.data_type == data_type,
        LabeledData.label.in_(['accept', 'modify'])
    )
    
    if topic_id:
        query = query.filter(GeneratedData.topic_id == topic_id)
    
    results = query.all()
    
    # T·∫°o file JSONL
    export_data = []
    for generated, labeled in results:
        content = json.loads(generated.content)
        # N·∫øu c√≥ modified_content, ∆∞u ti√™n d√πng modified
        if labeled.modified_content:
            try:
                modified = json.loads(labeled.modified_content)
                content.update(modified)
            except:
                pass  # N·∫øu l·ªói parse JSON th√¨ d√πng content g·ªëc
        export_data.append(content)
    
    # L∆∞u file
    filename = f"{data_type}_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
    filepath = os.path.join('data', 'exports', filename)
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    with jsonlines.open(filepath, mode='w') as writer:
        for item in export_data:
            writer.write(item)
    
    return send_file(filepath, as_attachment=True)

@app.route('/api/stats', methods=['GET'])
def get_statistics():
    """Th·ªëng k√™ d·ªØ li·ªáu"""
    total_topics = LegalTopic.query.count()
    total_generated = GeneratedData.query.count()
    total_labeled = LabeledData.query.count()
    
    # Th·ªëng k√™ theo lo·∫°i d·ªØ li·ªáu
    data_type_stats = db.session.query(
        GeneratedData.data_type,
        db.func.count(GeneratedData.id)
    ).group_by(GeneratedData.data_type).all()
    
    # Th·ªëng k√™ nh√£n
    label_stats = db.session.query(
        LabeledData.label,
        db.func.count(LabeledData.id)
    ).group_by(LabeledData.label).all()
    
    return jsonify({
        'total_topics': total_topics,
        'total_generated': total_generated,
        'total_labeled': total_labeled,
        'data_type_distribution': dict(data_type_stats),
        'label_distribution': dict(label_stats)
    })

@app.route('/api/topics/<int:topic_id>/coverage', methods=['POST'])
def analyze_topic_coverage(topic_id):
    """Ph√¢n t√≠ch ƒë·ªô bao ph·ªß c·ªßa b·ªô c√¢u h·ªèi cho m·ªôt ch·ªß ƒë·ªÅ"""
    try:
        data = request.get_json()
        unit_type = data.get('unit_type', 'sentence')  # sentence, paragraph
        threshold = data.get('threshold', 0.3)
        
        # Validate unit_type
        if unit_type not in ['sentence', 'paragraph']:
            return jsonify({'error': 'unit_type ph·∫£i l√† "sentence" ho·∫∑c "paragraph"'}), 400
        
        # L·∫•y topic v√† documents
        topic = LegalTopic.query.get_or_404(topic_id)
        documents = db.session.query(LegalDocument).join(TopicDocument).filter(
            TopicDocument.topic_id == topic_id
        ).all()
        
        if not documents:
            return jsonify({'error': 'Kh√¥ng c√≥ document n√†o cho ch·ªß ƒë·ªÅ n√†y'}), 400
        
        # Chu·∫©n b·ªã documents data
        documents_data = []
        for doc in documents:
            documents_data.append({
                'id': doc.id,
                'title': doc.title,
                'content': doc.content
            })
        
        # L·∫•y c√¢u h·ªèi ƒë√£ sinh cho topic n√†y
        questions = GeneratedData.query.filter_by(topic_id=topic_id).all()
        questions_data = []
        for q in questions:
            questions_data.append({
                'id': q.id,
                'data_type': q.data_type,
                'content': q.content
            })
        
        if not questions_data:
            return jsonify({'error': 'Kh√¥ng c√≥ c√¢u h·ªèi n√†o ƒë∆∞·ª£c sinh cho ch·ªß ƒë·ªÅ n√†y'}), 400
        
        # Ph√¢n t√≠ch coverage ƒë·ªìng b·ªô (ƒë∆°n gi·∫£n)
        analyzer = CoverageAnalyzer(coverage_threshold=threshold)
        coverage_analyzers[topic_id] = analyzer  # L∆∞u ƒë·ªÉ c√≥ th·ªÉ d·ª´ng
        
        analyzer.prepare_coverage_analysis(documents_data, questions_data, unit_type)
        coverage_result = analyzer.analyze_coverage()
        
        # Cleanup analyzer sau khi xong
        if topic_id in coverage_analyzers:
            del coverage_analyzers[topic_id]
        
        doc_summary = analyzer.get_coverage_summary_by_document(coverage_result)
        
        # Th√™m th√¥ng tin topic
        coverage_result['topic_info'] = {
            'id': topic.id,
            'name': topic.name,
            'description': topic.description
        }
        
        coverage_result['document_summary'] = doc_summary
        coverage_result['analysis_settings'] = {
            'unit_type': unit_type,
            'threshold': threshold,
            'total_documents': len(documents_data),
            'total_questions': len(questions_data)
        }

        return jsonify(coverage_result)
    
    except Exception as e:
        return jsonify({'error': f'L·ªói ph√¢n t√≠ch coverage: {str(e)}'}), 500

@app.route('/api/topics/<int:topic_id>/coverage/stop', methods=['POST'])
def stop_coverage_analysis(topic_id):
    """D·ª´ng ph√¢n t√≠ch coverage cho topic c·ª• th·ªÉ"""
    try:
        if topic_id in coverage_analyzers:
            coverage_analyzers[topic_id].stop_analysis()
            return jsonify({'message': f'ƒê√£ y√™u c·∫ßu d·ª´ng ph√¢n t√≠ch coverage cho topic {topic_id}'})
        else:
            return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y ph√¢n t√≠ch coverage ƒëang ch·∫°y cho topic n√†y'}), 404
    
    except Exception as e:
        return jsonify({'error': f'L·ªói d·ª´ng ph√¢n t√≠ch: {str(e)}'}), 500

@app.route('/api/coverage/batch', methods=['POST'])
def analyze_batch_coverage():
    """Ph√¢n t√≠ch coverage cho nhi·ªÅu topics c√πng l√∫c"""
    try:
        data = request.get_json()
        topic_ids = data.get('topic_ids', [])
        unit_type = data.get('unit_type', 'sentence')
        threshold = data.get('threshold', 0.3)
        
        if not topic_ids:
            return jsonify({'error': 'C·∫ßn cung c·∫•p danh s√°ch topic_ids'}), 400
        
        results = {}
        
        for topic_id in topic_ids:
            try:
                topic = LegalTopic.query.get(topic_id)
                if not topic:
                    results[topic_id] = {'error': f'Topic {topic_id} kh√¥ng t·ªìn t·∫°i'}
                    continue
                
                documents = db.session.query(LegalDocument).join(TopicDocument).filter(
                    TopicDocument.topic_id == topic_id
                ).all()
                
                questions = GeneratedData.query.filter_by(topic_id=topic_id).all()
                
                if not documents or not questions:
                    results[topic_id] = {
                        'error': 'Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch',
                        'documents_count': len(documents),
                        'questions_count': len(questions)
                    }
                    continue
                
                # Ph√¢n t√≠ch coverage cho topic n√†y
                documents_data = [{'id': doc.id, 'title': doc.title, 'content': doc.content} for doc in documents]
                questions_data = [{'id': q.id, 'data_type': q.data_type, 'content': q.content} for q in questions]
                
                analyzer = CoverageAnalyzer(coverage_threshold=threshold)
                analyzer.prepare_coverage_analysis(documents_data, questions_data, unit_type)
                coverage_result = analyzer.analyze_coverage()
                
                # Ch·ªâ l∆∞u summary, kh√¥ng l∆∞u chi ti·∫øt units
                results[topic_id] = {
                    'topic_name': topic.name,
                    'total_units': coverage_result['total_units'],
                    'covered_units': coverage_result['covered_units'],
                    'coverage_percentage': coverage_result['coverage_percentage'],
                    'documents_count': len(documents),
                    'questions_count': len(questions)
                }
                
            except Exception as e:
                results[topic_id] = {'error': f'L·ªói ph√¢n t√≠ch topic {topic_id}: {str(e)}'}
        
        return jsonify({
            'analysis_settings': {
                'unit_type': unit_type,
                'threshold': threshold
            },
            'results': results
        })
    
    except Exception as e:
        return jsonify({'error': f'L·ªói ph√¢n t√≠ch batch coverage: {str(e)}'}), 500

# ==================== CSV DOCUMENT MANAGEMENT ====================

@app.route('/api/csv/search', methods=['GET'])
def search_csv_documents():
    """T√¨m ki·∫øm t√†i li·ªáu trong CSV file"""
    try:
        search_query = request.args.get('q', '')
        limit = int(request.args.get('limit', 20))
        offset = int(request.args.get('offset', 0))
        
        # T√¨m ki·∫øm trong CSV
        results = vanban_csv.search_documents(search_query, limit=limit, offset=offset)
        
        return jsonify({
            'results': results,
            'query': search_query,
            'total': len(results),
            'limit': limit,
            'offset': offset
        })
    
    except Exception as e:
        return jsonify({'error': f'L·ªói t√¨m ki·∫øm CSV: {str(e)}'}), 500

@app.route('/api/csv/document/<int:document_index>', methods=['GET'])
def get_csv_document_preview(document_index):
    """L·∫•y preview t√†i li·ªáu t·ª´ CSV"""
    try:
        doc_data = vanban_csv.get_document_content(document_index)
        
        if not doc_data:
            return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y t√†i li·ªáu'}), 404
        
        content = doc_data.get('content', '')
        title = doc_data.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        
        # Tr·∫£ v·ªÅ preview (1000 k√Ω t·ª± ƒë·∫ßu) ho·∫∑c full content
        full = request.args.get('full', 'false').lower() == 'true'
        
        if full:
            return jsonify({
                'index': document_index,
                'title': title,
                'content': content,
                'content_length': len(content),
                'so_hieu': doc_data.get('so_hieu', ''),
                'url': doc_data.get('url', '')
            })
        else:
            preview = content[:1000] + "..." if len(content) > 1000 else content
            
            return jsonify({
                'index': document_index,
                'title': title,
                'preview': preview,
                'full_length': len(content),
                'has_more': len(content) > 1000,
                'so_hieu': doc_data.get('so_hieu', ''),
                'url': doc_data.get('url', '')
            })
    
    except Exception as e:
        return jsonify({'error': f'L·ªói l·∫•y preview: {str(e)}'}), 500

@app.route('/api/csv/import/<int:document_index>', methods=['POST'])
def import_csv_document(document_index):
    """Import t√†i li·ªáu t·ª´ CSV v√†o database"""
    try:
        data = request.get_json() or {}
        doc_data = vanban_csv.get_document_content(document_index)
        
        if not doc_data:
            return jsonify({'error': 'Kh√¥ng t√¨m th·∫•y t√†i li·ªáu trong CSV'}), 404
        
        content = doc_data.get('content', '')
        title = data.get('title') or doc_data.get('title', f'T√†i li·ªáu CSV {document_index}')
        
        # Ki·ªÉm tra tr√πng l·∫∑p
        existing = LegalDocument.query.filter_by(title=title).first()
        if existing:
            return jsonify({
                'error': 'T√†i li·ªáu ƒë√£ t·ªìn t·∫°i',
                'existing_id': existing.id
            }), 409
        
        # Parse document structure
        parsed_structure = None
        articles_count = 0
        try:
            print(f"üîÑ Parsing imported CSV document: {title}")
            structure = legal_parser.parse_document(title, content)
            parsed_structure = json.dumps(structure, ensure_ascii=False)
            # Calculate articles count for performance
            articles_count = len(structure.get('articles', []))
            print(f"‚úÖ CSV document parsed successfully - {articles_count} ƒëi·ªÅu")
        except Exception as e:
            print(f"‚ö†Ô∏è CSV parsing failed: {str(e)}")
        
        # T·∫°o document m·ªõi v·ªõi ƒë√∫ng document_type
        document = LegalDocument(
            title=title,
            content=content,
            parsed_structure=parsed_structure,
            document_type='law',  # Set ƒë√∫ng type cho vƒÉn b·∫£n ph√°p lu·∫≠t
            document_number=doc_data.get('so_hieu', ''),
            uploaded_by='csv_import',
            articles_count=articles_count
        )
        
        db.session.add(document)
        db.session.commit()
        
        return jsonify({
            'id': document.id,
            'title': document.title,
            'parsed': parsed_structure is not None,
            'content_length': len(content),
            'message': 'T√†i li·ªáu CSV ƒë√£ ƒë∆∞·ª£c import th√†nh c√¥ng'
        }), 201
    
    except Exception as e:
        db.session.rollback()
        return jsonify({'error': f'L·ªói import t√†i li·ªáu: {str(e)}'}), 500

@app.route('/api/csv/stats', methods=['GET'])
def get_csv_stats():
    """L·∫•y th·ªëng k√™ v·ªÅ CSV file"""
    try:
        # L·∫•y th·ªëng k√™ c∆° b·∫£n
        stats = {
            'csv_file_path': vanban_csv.csv_file_path,
            'file_exists': os.path.exists(vanban_csv.csv_file_path),
            'total_documents': 0,
            'sample_titles': []
        }
        
        if stats['file_exists']:
            # ƒê·ªçc m·ªôt chunk nh·ªè ƒë·ªÉ l·∫•y th·ªëng k√™
            chunk = next(vanban_csv._read_csv_chunks(chunk_size=100))
            stats['total_documents'] = len(chunk)
            stats['sample_titles'] = chunk['title'].head(10).tolist() if 'title' in chunk.columns else []
        
        return jsonify(stats)
    
    except Exception as e:
        return jsonify({'error': f'L·ªói l·∫•y th·ªëng k√™ CSV: {str(e)}'}), 500

if __name__ == '__main__':
    with app.app_context():
        db.create_all()
    app.run(debug=True, host='0.0.0.0', port=5000)
