#!/usr/bin/env python3
"""
Coverage Analyzer - Äo Ä‘á»™ bao phá»§ cá»§a bá»™ cÃ¢u há»i Ä‘á»‘i vá»›i vÄƒn báº£n gá»‘c
Uses hybrid search for optimal accuracy
"""

import re
import json
from typing import List, Dict, Any, Tuple, Optional
from hybrid_search import HybridSearchEngine, create_hybrid_search_engine
from document_parsers import LegalDocumentParser

class CoverageAnalyzer:
    """
    PhÃ¢n tÃ­ch Ä‘á»™ bao phá»§ cá»§a bá»™ cÃ¢u há»i Ä‘á»‘i vá»›i vÄƒn báº£n phÃ¡p luáº­t
    Sá»­ dá»¥ng Hybrid Search (BM25 + Semantic)
    """
    
    def __init__(self, 
                 coverage_threshold: float = 0.3,
                 hybrid_config: Optional[Dict[str, Any]] = None):
        """
        Args:
            coverage_threshold: NgÆ°á»¡ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh unit Ä‘Æ°á»£c bao phá»§ (0-1)
            hybrid_config: Configuration for hybrid search engine
        """
        self.coverage_threshold = coverage_threshold
        self.should_stop = False  # Flag Ä‘á»ƒ dá»«ng phÃ¢n tÃ­ch
        
        print(f"ğŸ§  Initializing hybrid coverage analyzer with threshold {coverage_threshold}")
        self.hybrid_engine = create_hybrid_search_engine(hybrid_config)
        
        self.text_units = []  # CÃ¡c Ä‘Æ¡n vá»‹ vÄƒn báº£n (cÃ¢u/Ä‘oáº¡n)
        self.questions = []   # CÃ¡c cÃ¢u há»i Ä‘Ã£ sinh
        
    def stop_analysis(self):
        """Dá»«ng quÃ¡ trÃ¬nh phÃ¢n tÃ­ch"""
        self.should_stop = True
        print("ğŸ›‘ ÄÃ£ yÃªu cáº§u dá»«ng phÃ¢n tÃ­ch coverage")
        
    def reset_stop_flag(self):
        """Reset flag dá»«ng Ä‘á»ƒ chuáº©n bá»‹ cho phÃ¢n tÃ­ch má»›i"""
        self.should_stop = False
        
    def preprocess_text(self, text: str) -> List[str]:
        """
        Tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t (tÆ°Æ¡ng tá»± similarity_checker)
        """
        if not text:
            return []
            
        # Chuyá»ƒn thÆ°á»ng
        text = text.lower()
        
        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i, sá»‘ vÃ  khoáº£ng tráº¯ng
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # TÃ¡ch tá»« Ä‘Æ¡n giáº£n (split by space)
        words = text.split()
        
        # Loáº¡i bá» tá»« dá»«ng Ä‘Æ¡n giáº£n
        stop_words = {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'trong', 'vá»›i', 'theo', 'Ä‘á»ƒ', 'Ä‘Æ°á»£c', 'cÃ¡c', 
            'má»™t', 'nÃ y', 'Ä‘Ã³', 'nhá»¯ng', 'tá»«', 'trÃªn', 'dÆ°á»›i', 'vá»', 'cho', 'hay',
            'báº±ng', 'nhÆ°', 'khi', 'nÃ o', 'gÃ¬', 'ai', 'Ä‘Ã¢u', 'sao', 'bao', 'nhiá»u'
        }
        
        words = [word for word in words if word not in stop_words and len(word) > 1]
        
        return words
    
    def split_into_units(self, document_title: str, text: str, unit_type: str = 'sentence') -> List[Dict[str, Any]]:
        """
        Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ (units) sá»­ dá»¥ng LegalDocumentParser chuáº©n
        
        Args:
            document_title: TiÃªu Ä‘á» tÃ i liá»‡u
            text: VÄƒn báº£n cáº§n chia
            unit_type: Loáº¡i Ä‘Æ¡n vá»‹ ('sentence' sá»­ dá»¥ng parser, 'paragraph' manual)
            
        Returns:
            List[Dict]: Danh sÃ¡ch cÃ¡c units vá»›i metadata
        """
        units = []
        
        if unit_type == 'sentence':
            # Sá»­ dá»¥ng LegalDocumentParser chuáº©n nhÆ° trong data_generator
            parser = LegalDocumentParser()
            print(f"ğŸ”„ Parsing document with LegalDocumentParser: {document_title}")
            parsed_data = parser.parse_document(document_title, text)
            print(f"   Articles parsed: {len(parsed_data.get('articles', []))}")
            
            # Láº¥y táº¥t cáº£ units tá»« parsed structure
            parser_units = parser.get_all_units(parsed_data)
            print(f"   Units generated: {len(parser_units)}")
            
            # Convert sang format cho coverage analyzer - chá»‰ giá»¯ cÃ¡c fields cáº§n thiáº¿t
            for unit in parser_units:
                units.append({
                    'id': f"unit_{unit['source_article']}_{unit['source_khoan']}_{unit['source_diem']}",
                    'type': 'content_unit',
                    'content': unit['content'],
                    'length': unit.get('content_length', len(unit['content'])),
                    'tokens': self.preprocess_text(unit['content']),
                    'path': unit['path'],
                    'document_title': document_title
                })
            
            print(f"âœ… Successfully parsed {len(units)} units using LegalDocumentParser")
                    
        elif unit_type == 'paragraph':
            # Chia thÃ nh Ä‘oáº¡n (giá»¯ logic cÅ© cho paragraph mode)
            paragraphs = text.split('\n\n')
            for i, paragraph in enumerate(paragraphs):
                paragraph = paragraph.strip()
                if len(paragraph) > 50:  # Lá»c Ä‘oáº¡n quÃ¡ ngáº¯n
                    units.append({
                        'id': f'para_{i}',
                        'type': 'paragraph', 
                        'content': paragraph,
                        'length': len(paragraph),
                        'tokens': self.preprocess_text(paragraph)
                    })
        
        return units
    
    def prepare_coverage_analysis(self, documents: List[Dict[str, Any]], questions_data: List[Dict[str, Any]], unit_type: str = 'sentence'):
        """
        Chuáº©n bá»‹ dá»¯ liá»‡u cho phÃ¢n tÃ­ch coverage
        
        Args:
            documents: List cÃ¡c documents vá»›i content
            questions_data: List cÃ¡c cÃ¢u há»i Ä‘Ã£ sinh vá»›i source information
            unit_type: Loáº¡i Ä‘Æ¡n vá»‹ Ä‘á»ƒ phÃ¢n tÃ­ch
        """
        # BÆ°á»›c 1: Chia vÄƒn báº£n thÃ nh units sá»­ dá»¥ng LegalDocumentParser
        self.text_units = []
        for doc in documents:
            doc_title = doc.get('title', 'Unknown Document')
            doc_units = self.split_into_units(doc_title, doc['content'], unit_type)
            
            # ThÃªm thÃ´ng tin document vÃ o má»—i unit
            for unit in doc_units:
                unit['document_id'] = doc.get('id')
                unit['document_title'] = doc_title
                self.text_units.append(unit)
        
        print(f"ğŸ“„ Chia thÃ nh {len(self.text_units)} {unit_type}s tá»« {len(documents)} documents")
        
        # BÆ°á»›c 2: Chuáº©n bá»‹ cÃ¢u há»i vá»›i source information
        self.questions = []
        for item in questions_data:
            if isinstance(item.get('content'), str):
                content = json.loads(item['content'])
            else:
                content = item.get('content', {})
            
            question = content.get('question', '')
            answer = content.get('answer', '')  # Extract answer
            sources = content.get('sources', [])  # Extract source information
            
            if question:
                self.questions.append({
                    'id': item.get('id'),
                    'question': question,
                    'answer': answer,  # Add answer field
                    'data_type': item.get('data_type'),
                    'sources': sources,  # Add source information
                    'tokens': self.preprocess_text(question)
                })
        
        print(f"â“ Chuáº©n bá»‹ {len(self.questions)} cÃ¢u há»i")
        
        # BÆ°á»›c 3: Khá»Ÿi táº¡o hybrid search engine
        unit_texts = [' '.join(unit['tokens']) for unit in self.text_units]
        
        if unit_texts and self.questions:
            # Use hybrid search for question-unit matching
            print("ğŸ§  Initializing hybrid search for coverage analysis...")
            self.hybrid_engine.index_documents(unit_texts, list(range(len(self.text_units))))
            print("âœ… Hybrid search ready for coverage analysis")
        else:
            print("âš ï¸ KhÃ´ng cÃ³ units hoáº·c questions Ä‘á»ƒ phÃ¢n tÃ­ch")
    
    def calculate_unit_coverage(self, unit: Dict[str, Any]) -> Dict[str, Any]:
        """
        TÃ­nh Ä‘á»™ bao phá»§ cho má»™t unit cá»¥ thá»ƒ, chá»‰ tÃ­nh vá»›i questions cÃ³ sources liÃªn quan
        Sá»­ dá»¥ng hybrid search cho accuracy tá»‘t hÆ¡n
        
        Args:
            unit: Unit cáº§n tÃ­nh coverage
            
        Returns:
            Dict: ThÃ´ng tin coverage cá»§a unit
        """
        base_result = {
            'is_covered': False,
            'max_similarity': 0.0,
            'best_question': None,
            'similarities': [],
            'relevant_questions_count': 0
        }
        
        if not self.questions:
            return base_result
        
        unit_tokens = unit['tokens']
        unit_text = ' '.join(unit_tokens)
        unit_path = unit.get('path', '')
        unit_doc_title = unit.get('document_title', 'Unknown')
        
        # Filter questions that reference this specific unit as source based on unit_path
        relevant_questions = []
        for i, question in enumerate(self.questions):
            # Check if this unit's path matches any source unit_path in the question
            for source in question.get('sources', []):
                # First priority: match by unit_path (new format)
                source_unit_path = source.get('unit_path', '')
                if source_unit_path and source_unit_path == unit_path:
                    relevant_questions.append((i, question))
                    break
                
                # STRICT: No fallback - chá»‰ tÃ­nh coverage cho units cÃ³ exact unit_path match
                # Data khÃ´ng cÃ³ unit_path sáº½ khÃ´ng Ä‘Æ°á»£c tÃ­nh coverage (correct behavior)
                # VÃ¬ coverage analysis cáº§n chÃ­nh xÃ¡c tá»«ng unit, khÃ´ng pháº£i document level
        
        # If no relevant questions, return no coverage
        if not relevant_questions:
            return base_result
        
        similarities = []
        
        # Use hybrid search to compute similarity between unit and relevant questions' answers
        for i, question in relevant_questions:
            try:
                # Compute similarity between unit text and answer (not question)
                answer_text = question['question']
                if not answer_text:
                    # Skip if no answer available
                    continue
                    
                similarity_result = self.hybrid_engine.compute_similarity(unit_text, answer_text)
                combined_score = similarity_result['combined_score']
                
                similarities.append({
                    'question_id': question['id'],
                    'question': question['question'],
                    'data_type': question['data_type'],
                    'bm25_score': similarity_result['bm25_score'],
                    'semantic_score': similarity_result['semantic_score'],
                    'tfidf_score': similarity_result['tfidf_score'],
                    'combined_score': combined_score
                })
            except Exception as e:
                print(f"âš ï¸ Error computing hybrid similarity: {e}")
                # Fallback to zero score
                similarities.append({
                    'question_id': question['id'],
                    'question': question['question'],
                    'data_type': question['data_type'],
                    'bm25_score': 0.0,
                    'semantic_score': 0.0,
                    'tfidf_score': 0.0,
                    'combined_score': 0.0
                })
        
        # TÃ¬m similarity cao nháº¥t
        max_similarity = max([s['combined_score'] for s in similarities], default=0.0)
        best_question = max(similarities, key=lambda x: x['combined_score'], default=None)
        
        is_covered = max_similarity >= self.coverage_threshold
        
        return {
            'is_covered': is_covered,
            'max_similarity': max_similarity,
            'best_question': best_question,
            'similarities': sorted(similarities, key=lambda x: x['combined_score'], reverse=True)[:3],  # Top 3
            'relevant_questions_count': len(relevant_questions)
        }
    
    def analyze_coverage(self) -> Dict[str, Any]:
        """
        PhÃ¢n tÃ­ch coverage cho táº¥t cáº£ units
        
        Returns:
            Dict: Káº¿t quáº£ phÃ¢n tÃ­ch coverage
        """
        if not self.text_units:
            return {
                'total_units': 0,
                'covered_units': 0,
                'coverage_percentage': 0.0,
                'units_analysis': []
            }
        
        print("ğŸ§  Báº¯t Ä‘áº§u phÃ¢n tÃ­ch coverage vá»›i Hybrid Search (optimized - chá»‰ tÃ­nh vá»›i relevant questions)...")
        self.reset_stop_flag()  # Reset flag khi báº¯t Ä‘áº§u
        
        covered_count = 0
        units_analysis = []
        total_relevant_questions = 0
        
        for i, unit in enumerate(self.text_units):
            # Kiá»ƒm tra náº¿u Ä‘Æ°á»£c yÃªu cáº§u dá»«ng
            if self.should_stop:
                print(f"ğŸ›‘ PhÃ¢n tÃ­ch bá»‹ dá»«ng táº¡i unit {i + 1}/{len(self.text_units)}")
                break
                
            coverage_info = self.calculate_unit_coverage(unit)
            
            if coverage_info['is_covered']:
                covered_count += 1
            
            # Track total relevant questions count
            total_relevant_questions += coverage_info.get('relevant_questions_count', 0)
            
            unit_analysis = {
                'unit_id': unit['id'],
                'unit_type': unit['type'],
                'document_title': unit['document_title'],
                'content_preview': unit['content'][:100] + '...' if len(unit['content']) > 100 else unit['content'],
                'length': unit['length'],
                **coverage_info
            }
            
            units_analysis.append(unit_analysis)
            
            if (i + 1) % 10 == 0:
                print(f"  ğŸ“Š ÄÃ£ phÃ¢n tÃ­ch {i + 1}/{len(self.text_units)} units... (relevant questions so far: {total_relevant_questions})")
        
        # TÃ­nh coverage dá»±a trÃªn sá»‘ unit Ä‘Ã£ xá»­ lÃ½
        processed_units = len(units_analysis)
        coverage_percentage = (covered_count / processed_units) * 100 if processed_units > 0 else 0
        
        result = {
            'total_units': len(self.text_units),
            'processed_units': processed_units,
            'covered_units': covered_count,
            'uncovered_units': processed_units - covered_count,
            'coverage_percentage': coverage_percentage,
            'threshold_used': self.coverage_threshold,
            'was_stopped': self.should_stop,
            'total_questions': len(self.questions),
            'total_relevant_calculations': total_relevant_questions,
            'optimization_ratio': f"{total_relevant_questions}/{processed_units * len(self.questions)} ({(total_relevant_questions / (processed_units * len(self.questions)) * 100):.1f}%)" if processed_units > 0 and len(self.questions) > 0 else "0/0 (0.0%)",
            'units_analysis': units_analysis
        }
        
        status_message = "ğŸ›‘ ÄÃ£ dá»«ng" if self.should_stop else "âœ… HoÃ n thÃ nh"
        print(f"{status_message} phÃ¢n tÃ­ch coverage: {coverage_percentage:.1f}% ({covered_count}/{processed_units} units Ä‘Ã£ xá»­ lÃ½)")
        
        if processed_units > 0 and len(self.questions) > 0:
            total_possible_calculations = processed_units * len(self.questions)
            optimization_saved = total_possible_calculations - total_relevant_questions
            optimization_percentage = (optimization_saved / total_possible_calculations * 100) if total_possible_calculations > 0 else 0.0
            
            if optimization_percentage > 0.1:  # Only show if significant optimization
                print(f"ğŸš€ Optimization: TÃ­nh {total_relevant_questions} similarities thay vÃ¬ {total_possible_calculations} (tiáº¿t kiá»‡m {optimization_percentage:.1f}%)")
            else:
                print(f"ğŸ” Analysis: TÃ­nh {total_relevant_questions} similarity calculations cho {processed_units} units")
        
        return result
    
    def get_coverage_summary_by_document(self, coverage_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        TÃ³m táº¯t coverage theo tá»«ng document
        """
        doc_stats = {}
        
        for unit in coverage_result['units_analysis']:
            doc_title = unit['document_title']
            
            if doc_title not in doc_stats:
                doc_stats[doc_title] = {
                    'total_units': 0,
                    'covered_units': 0,
                    'uncovered_units': 0,
                    'coverage_percentage': 0.0
                }
            
            doc_stats[doc_title]['total_units'] += 1
            if unit['is_covered']:
                doc_stats[doc_title]['covered_units'] += 1
            else:
                doc_stats[doc_title]['uncovered_units'] += 1
        
        # TÃ­nh percentage cho tá»«ng doc
        for doc_title, stats in doc_stats.items():
            if stats['total_units'] > 0:
                stats['coverage_percentage'] = (stats['covered_units'] / stats['total_units']) * 100
        
        return doc_stats

        return doc_stats