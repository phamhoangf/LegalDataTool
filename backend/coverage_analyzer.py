#!/usr/bin/env python3
"""
Coverage Analyzer - Äo Ä‘á»™ bao phá»§ cá»§a bá»™ cÃ¢u há»i Ä‘á»‘i vá»›i vÄƒn báº£n gá»‘c
Uses hybrid search for optimal accuracy
"""

import re
import json
from typing import List, Dict, Any, Tuple, Optional
from hybrid_search import HybridSearchEngine, create_hybrid_search_engine

class CoverageAnalyzer:
    """
    PhÃ¢n tÃ­ch Ä‘á»™ bao phá»§ cá»§a bá»™ cÃ¢u há»i Ä‘á»‘i vá»›i vÄƒn báº£n phÃ¡p luáº­t
    Sá»­ dá»¥ng Hybrid Search (BM25 + Semantic)
    """
    
    def __init__(self, 
                 coverage_threshold: float = 0.3,
                 hybrid_config: Optional[Dict[str, Any]] = None):
        """
        Args:
            coverage_threshold: NgÆ°á»¡ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh unit Ä‘Æ°á»£c bao phá»§ (0-1)
            hybrid_config: Configuration for hybrid search engine
        """
        self.coverage_threshold = coverage_threshold
        self.should_stop = False  # Flag Ä‘á»ƒ dá»«ng phÃ¢n tÃ­ch
        
        print(f"ğŸ§  Initializing hybrid coverage analyzer with threshold {coverage_threshold}")
        self.hybrid_engine = create_hybrid_search_engine(hybrid_config)
        
        self.text_units = []  # CÃ¡c Ä‘Æ¡n vá»‹ vÄƒn báº£n (cÃ¢u/Ä‘oáº¡n)
        self.questions = []   # CÃ¡c cÃ¢u há»i Ä‘Ã£ sinh
        
    def stop_analysis(self):
        """Dá»«ng quÃ¡ trÃ¬nh phÃ¢n tÃ­ch"""
        self.should_stop = True
        print("ğŸ›‘ ÄÃ£ yÃªu cáº§u dá»«ng phÃ¢n tÃ­ch coverage")
        
    def reset_stop_flag(self):
        """Reset flag dá»«ng Ä‘á»ƒ chuáº©n bá»‹ cho phÃ¢n tÃ­ch má»›i"""
        self.should_stop = False
        
    def preprocess_text(self, text: str) -> List[str]:
        """
        Tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t (tÆ°Æ¡ng tá»± similarity_checker)
        """
        if not text:
            return []
            
        # Chuyá»ƒn thÆ°á»ng
        text = text.lower()
        
        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i, sá»‘ vÃ  khoáº£ng tráº¯ng
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # TÃ¡ch tá»« Ä‘Æ¡n giáº£n (split by space)
        words = text.split()
        
        # Loáº¡i bá» tá»« dá»«ng Ä‘Æ¡n giáº£n
        stop_words = {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'trong', 'vá»›i', 'theo', 'Ä‘á»ƒ', 'Ä‘Æ°á»£c', 'cÃ¡c', 
            'má»™t', 'nÃ y', 'Ä‘Ã³', 'nhá»¯ng', 'tá»«', 'trÃªn', 'dÆ°á»›i', 'vá»', 'cho', 'hay',
            'báº±ng', 'nhÆ°', 'khi', 'nÃ o', 'gÃ¬', 'ai', 'Ä‘Ã¢u', 'sao', 'bao', 'nhiá»u'
        }
        
        words = [word for word in words if word not in stop_words and len(word) > 1]
        
        return words
    
    def split_into_units(self, text: str, unit_type: str = 'sentence') -> List[Dict[str, Any]]:
        """
        Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ (units)
        
        Args:
            text: VÄƒn báº£n cáº§n chia
            unit_type: Loáº¡i Ä‘Æ¡n vá»‹ ('sentence', 'paragraph')
            
        Returns:
            List[Dict]: Danh sÃ¡ch cÃ¡c units vá»›i metadata
        """
        units = []
        
        if unit_type == 'sentence':
            # Chia thÃ nh cÃ¡c Ä‘iá»u luáº­t theo pattern "Äiá»u X."
            # Sá»­ dá»¥ng split approach Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£
            parts = re.split(r'(?=Äiá»u\s+\d+\.)', text, flags=re.IGNORECASE)
            
            article_count = 0
            for i, part in enumerate(parts):
                part = part.strip()
                if len(part) < 30:  # Loáº¡i bá» pháº§n quÃ¡ ngáº¯n (header, footer)
                    continue
                    
                # TÃ¬m sá»‘ Ä‘iá»u vÃ  tÃªn Ä‘iá»u
                article_match = re.search(r'Äiá»u\s+(\d+)\.\s*([^\r\n]*)', part, re.IGNORECASE)
                if article_match:
                    article_num = article_match.group(1)
                    article_title = article_match.group(2).strip()
                    article_id = f'dieu_{article_num}'
                    type_label = 'article'
                    article_count += 1
                else:
                    # Pháº§n khÃ´ng cÃ³ "Äiá»u X." (cÃ³ thá»ƒ lÃ  pháº§n Ä‘áº§u vÄƒn báº£n)
                    article_id = f'part_{i}'
                    article_title = part[:50].replace('\n', ' ').strip()
                    type_label = 'section'
                
                units.append({
                    'id': article_id,
                    'type': type_label,
                    'content': part,
                    'length': len(part),
                    'tokens': self.preprocess_text(part),
                    'article_number': int(article_match.group(1)) if article_match else None,
                    'article_title': article_title
                })
            
            print(f"ğŸ“‹ ÄÃ£ chia thÃ nh {len(units)} units, trong Ä‘Ã³ {article_count} Ä‘iá»u luáº­t")
                    
        elif unit_type == 'paragraph':
            # Chia thÃ nh Ä‘oáº¡n
            paragraphs = text.split('\n\n')
            for i, paragraph in enumerate(paragraphs):
                paragraph = paragraph.strip()
                if len(paragraph) > 50:  # Lá»c Ä‘oáº¡n quÃ¡ ngáº¯n
                    units.append({
                        'id': f'para_{i}',
                        'type': 'paragraph', 
                        'content': paragraph,
                        'length': len(paragraph),
                        'tokens': self.preprocess_text(paragraph)
                    })
        
        return units
    
    def prepare_coverage_analysis(self, documents: List[Dict[str, Any]], questions_data: List[Dict[str, Any]], unit_type: str = 'sentence'):
        """
        Chuáº©n bá»‹ dá»¯ liá»‡u cho phÃ¢n tÃ­ch coverage
        
        Args:
            documents: List cÃ¡c documents vá»›i content
            questions_data: List cÃ¡c cÃ¢u há»i Ä‘Ã£ sinh vá»›i source information
            unit_type: Loáº¡i Ä‘Æ¡n vá»‹ Ä‘á»ƒ phÃ¢n tÃ­ch
        """
        # BÆ°á»›c 1: Chia vÄƒn báº£n thÃ nh units
        self.text_units = []
        for doc in documents:
            doc_units = self.split_into_units(doc['content'], unit_type)
            
            # ThÃªm thÃ´ng tin document vÃ o má»—i unit
            for unit in doc_units:
                unit['document_id'] = doc.get('id')
                unit['document_title'] = doc.get('title', 'Unknown')
                self.text_units.append(unit)
        
        print(f"ğŸ“„ Chia thÃ nh {len(self.text_units)} {unit_type}s tá»« {len(documents)} documents")
        
        # BÆ°á»›c 2: Chuáº©n bá»‹ cÃ¢u há»i vá»›i source information
        self.questions = []
        for item in questions_data:
            if isinstance(item.get('content'), str):
                content = json.loads(item['content'])
            else:
                content = item.get('content', {})
            
            question = content.get('question', '')
            sources = content.get('sources', [])  # Extract source information
            
            if question:
                self.questions.append({
                    'id': item.get('id'),
                    'question': question,
                    'data_type': item.get('data_type'),
                    'sources': sources,  # Add source information
                    'tokens': self.preprocess_text(question)
                })
        
        print(f"â“ Chuáº©n bá»‹ {len(self.questions)} cÃ¢u há»i")
        
        # BÆ°á»›c 3: Khá»Ÿi táº¡o hybrid search engine
        unit_texts = [' '.join(unit['tokens']) for unit in self.text_units]
        
        if unit_texts and self.questions:
            # Use hybrid search for question-unit matching
            print("ğŸ§  Initializing hybrid search for coverage analysis...")
            self.hybrid_engine.index_documents(unit_texts, list(range(len(self.text_units))))
            print("âœ… Hybrid search ready for coverage analysis")
        else:
            print("âš ï¸ KhÃ´ng cÃ³ units hoáº·c questions Ä‘á»ƒ phÃ¢n tÃ­ch")
    
    def calculate_unit_coverage(self, unit: Dict[str, Any]) -> Dict[str, Any]:
        """
        TÃ­nh Ä‘á»™ bao phá»§ cho má»™t unit cá»¥ thá»ƒ, chá»‰ tÃ­nh vá»›i questions cÃ³ sources liÃªn quan
        Sá»­ dá»¥ng hybrid search cho accuracy tá»‘t hÆ¡n
        
        Args:
            unit: Unit cáº§n tÃ­nh coverage
            
        Returns:
            Dict: ThÃ´ng tin coverage cá»§a unit
        """
        base_result = {
            'is_covered': False,
            'max_similarity': 0.0,
            'best_question': None,
            'similarities': [],
            'relevant_questions_count': 0
        }
        
        if not self.questions:
            return base_result
        
        unit_tokens = unit['tokens']
        unit_text = ' '.join(unit_tokens)
        unit_doc_title = unit.get('document_title', 'Unknown')
        
        # Filter questions that reference this unit's document as source
        relevant_questions = []
        for i, question in enumerate(self.questions):
            # Check if this unit's document is in the question's sources
            for source in question.get('sources', []):
                # Try different possible field names for document title
                source_title = (source.get('document_title') or 
                              source.get('title') or 
                              source.get('name') or 
                              source.get('article_title', ''))
                
                if source_title and source_title == unit_doc_title:
                    relevant_questions.append((i, question))
                    break
        
        # If no relevant questions, return no coverage
        if not relevant_questions:
            return base_result
        
        similarities = []
        
        # Use hybrid search to compute similarity between unit and relevant questions
        for i, question in relevant_questions:
            try:
                # Compute similarity between unit text and question
                similarity_result = self.hybrid_engine.compute_similarity(unit_text, question['question'])
                combined_score = similarity_result['combined_score']
                
                similarities.append({
                    'question_id': question['id'],
                    'question': question['question'],
                    'data_type': question['data_type'],
                    'bm25_score': similarity_result['bm25_score'],
                    'semantic_score': similarity_result['semantic_score'],
                    'tfidf_score': similarity_result['tfidf_score'],
                    'combined_score': combined_score
                })
            except Exception as e:
                print(f"âš ï¸ Error computing hybrid similarity: {e}")
                # Fallback to zero score
                similarities.append({
                    'question_id': question['id'],
                    'question': question['question'],
                    'data_type': question['data_type'],
                    'bm25_score': 0.0,
                    'semantic_score': 0.0,
                    'tfidf_score': 0.0,
                    'combined_score': 0.0
                })
        
        # TÃ¬m similarity cao nháº¥t
        max_similarity = max([s['combined_score'] for s in similarities], default=0.0)
        best_question = max(similarities, key=lambda x: x['combined_score'], default=None)
        
        is_covered = max_similarity >= self.coverage_threshold
        
        return {
            'is_covered': is_covered,
            'max_similarity': max_similarity,
            'best_question': best_question,
            'similarities': sorted(similarities, key=lambda x: x['combined_score'], reverse=True)[:3],  # Top 3
            'relevant_questions_count': len(relevant_questions)
        }
    
    def analyze_coverage(self) -> Dict[str, Any]:
        """
        PhÃ¢n tÃ­ch coverage cho táº¥t cáº£ units
        
        Returns:
            Dict: Káº¿t quáº£ phÃ¢n tÃ­ch coverage
        """
        if not self.text_units:
            return {
                'total_units': 0,
                'covered_units': 0,
                'coverage_percentage': 0.0,
                'units_analysis': []
            }
        
        print("ğŸ§  Báº¯t Ä‘áº§u phÃ¢n tÃ­ch coverage vá»›i Hybrid Search (optimized - chá»‰ tÃ­nh vá»›i relevant questions)...")
        self.reset_stop_flag()  # Reset flag khi báº¯t Ä‘áº§u
        
        covered_count = 0
        units_analysis = []
        total_relevant_questions = 0
        
        for i, unit in enumerate(self.text_units):
            # Kiá»ƒm tra náº¿u Ä‘Æ°á»£c yÃªu cáº§u dá»«ng
            if self.should_stop:
                print(f"ğŸ›‘ PhÃ¢n tÃ­ch bá»‹ dá»«ng táº¡i unit {i + 1}/{len(self.text_units)}")
                break
                
            coverage_info = self.calculate_unit_coverage(unit)
            
            if coverage_info['is_covered']:
                covered_count += 1
            
            # Track total relevant questions count
            total_relevant_questions += coverage_info.get('relevant_questions_count', 0)
            
            unit_analysis = {
                'unit_id': unit['id'],
                'unit_type': unit['type'],
                'document_title': unit['document_title'],
                'content_preview': unit['content'][:100] + '...' if len(unit['content']) > 100 else unit['content'],
                'length': unit['length'],
                **coverage_info
            }
            
            units_analysis.append(unit_analysis)
            
            if (i + 1) % 10 == 0:
                print(f"  ğŸ“Š ÄÃ£ phÃ¢n tÃ­ch {i + 1}/{len(self.text_units)} units... (relevant questions so far: {total_relevant_questions})")
        
        # TÃ­nh coverage dá»±a trÃªn sá»‘ unit Ä‘Ã£ xá»­ lÃ½
        processed_units = len(units_analysis)
        coverage_percentage = (covered_count / processed_units) * 100 if processed_units > 0 else 0
        
        result = {
            'total_units': len(self.text_units),
            'processed_units': processed_units,
            'covered_units': covered_count,
            'uncovered_units': processed_units - covered_count,
            'coverage_percentage': coverage_percentage,
            'threshold_used': self.coverage_threshold,
            'was_stopped': self.should_stop,
            'total_questions': len(self.questions),
            'total_relevant_calculations': total_relevant_questions,
            'optimization_ratio': f"{total_relevant_questions}/{len(self.text_units) * len(self.questions)} ({(total_relevant_questions / (len(self.text_units) * len(self.questions)) * 100):.1f}%)",
            'units_analysis': units_analysis
        }
        
        status_message = "ğŸ›‘ ÄÃ£ dá»«ng" if self.should_stop else "âœ… HoÃ n thÃ nh"
        print(f"{status_message} phÃ¢n tÃ­ch coverage: {coverage_percentage:.1f}% ({covered_count}/{processed_units} units Ä‘Ã£ xá»­ lÃ½)")
        
        if processed_units > 0:
            print(f"ğŸš€ Optimization: TÃ­nh {total_relevant_questions} similarities thay vÃ¬ {processed_units * len(self.questions)} (tiáº¿t kiá»‡m {((processed_units * len(self.questions) - total_relevant_questions) / (processed_units * len(self.questions)) * 100):.1f}%)")
        
        return result
    
    def get_coverage_summary_by_document(self, coverage_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        TÃ³m táº¯t coverage theo tá»«ng document
        """
        doc_stats = {}
        
        for unit in coverage_result['units_analysis']:
            doc_title = unit['document_title']
            
            if doc_title not in doc_stats:
                doc_stats[doc_title] = {
                    'total_units': 0,
                    'covered_units': 0,
                    'uncovered_units': 0,
                    'coverage_percentage': 0.0
                }
            
            doc_stats[doc_title]['total_units'] += 1
            if unit['is_covered']:
                doc_stats[doc_title]['covered_units'] += 1
            else:
                doc_stats[doc_title]['uncovered_units'] += 1
        
        # TÃ­nh percentage cho tá»«ng doc
        for doc_title, stats in doc_stats.items():
            if stats['total_units'] > 0:
                stats['coverage_percentage'] = (stats['covered_units'] / stats['total_units']) * 100
        
        return doc_stats

        return doc_stats
