#!/usr/bin/env python3
"""
Coverage Analyzer - Äo Ä‘á»™ bao phá»§ cá»§a bá»™ cÃ¢u há»i Ä‘á»‘i vá»›i vÄƒn báº£n gá»‘c
"""

import re
import json
from typing import List, Dict, Any, Tuple
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

class CoverageAnalyzer:
    """
    PhÃ¢n tÃ­ch Ä‘á»™ bao phá»§ cá»§a bá»™ cÃ¢u há»i Ä‘á»‘i vá»›i vÄƒn báº£n phÃ¡p luáº­t
    """
    
    def __init__(self, coverage_threshold: float = 0.3):
        """
        Args:
            coverage_threshold: NgÆ°á»¡ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh unit Ä‘Æ°á»£c bao phá»§ (0-1)
        """
        self.coverage_threshold = coverage_threshold
        self.text_units = []  # CÃ¡c Ä‘Æ¡n vá»‹ vÄƒn báº£n (cÃ¢u/Ä‘oáº¡n)
        self.questions = []   # CÃ¡c cÃ¢u há»i Ä‘Ã£ sinh
        self.bm25 = None
        self.tfidf_vectorizer = None
        self.questions_tfidf = None
        
    def preprocess_text(self, text: str) -> List[str]:
        """
        Tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t (tÆ°Æ¡ng tá»± similarity_checker)
        """
        if not text:
            return []
            
        # Chuyá»ƒn thÆ°á»ng
        text = text.lower()
        
        # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ cÃ¡i, sá»‘ vÃ  khoáº£ng tráº¯ng
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # TÃ¡ch tá»« Ä‘Æ¡n giáº£n (split by space)
        words = text.split()
        
        # Loáº¡i bá» tá»« dá»«ng Ä‘Æ¡n giáº£n
        stop_words = {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'trong', 'vá»›i', 'theo', 'Ä‘á»ƒ', 'Ä‘Æ°á»£c', 'cÃ¡c', 
            'má»™t', 'nÃ y', 'Ä‘Ã³', 'nhá»¯ng', 'tá»«', 'trÃªn', 'dÆ°á»›i', 'vá»', 'cho', 'hay',
            'báº±ng', 'nhÆ°', 'khi', 'nÃ o', 'gÃ¬', 'ai', 'Ä‘Ã¢u', 'sao', 'bao', 'nhiá»u'
        }
        
        words = [word for word in words if word not in stop_words and len(word) > 1]
        
        return words
    
    def split_into_units(self, text: str, unit_type: str = 'sentence') -> List[Dict[str, Any]]:
        """
        Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ (units)
        
        Args:
            text: VÄƒn báº£n cáº§n chia
            unit_type: Loáº¡i Ä‘Æ¡n vá»‹ ('sentence', 'paragraph')
            
        Returns:
            List[Dict]: Danh sÃ¡ch cÃ¡c units vá»›i metadata
        """
        units = []
        
        if unit_type == 'sentence':
            # Chia thÃ nh cÃ¢u (Ä‘iá»u)
            sentences = re.split(r'[.!?]+', text)
            for i, sentence in enumerate(sentences):
                sentence = sentence.strip()
                if len(sentence) > 20:  # Lá»c cÃ¢u quÃ¡ ngáº¯n
                    units.append({
                        'id': f'sent_{i}',
                        'type': 'sentence',
                        'content': sentence,
                        'length': len(sentence),
                        'tokens': self.preprocess_text(sentence)
                    })
                    
        elif unit_type == 'paragraph':
            # Chia thÃ nh Ä‘oáº¡n
            paragraphs = text.split('\n\n')
            for i, paragraph in enumerate(paragraphs):
                paragraph = paragraph.strip()
                if len(paragraph) > 50:  # Lá»c Ä‘oáº¡n quÃ¡ ngáº¯n
                    units.append({
                        'id': f'para_{i}',
                        'type': 'paragraph', 
                        'content': paragraph,
                        'length': len(paragraph),
                        'tokens': self.preprocess_text(paragraph)
                    })
        
        return units
    
    def prepare_coverage_analysis(self, documents: List[Dict[str, Any]], questions_data: List[Dict[str, Any]], unit_type: str = 'sentence'):
        """
        Chuáº©n bá»‹ dá»¯ liá»‡u cho phÃ¢n tÃ­ch coverage
        
        Args:
            documents: List cÃ¡c documents vá»›i content
            questions_data: List cÃ¡c cÃ¢u há»i Ä‘Ã£ sinh
            unit_type: Loáº¡i Ä‘Æ¡n vá»‹ Ä‘á»ƒ phÃ¢n tÃ­ch
        """
        # BÆ°á»›c 1: Chia vÄƒn báº£n thÃ nh units
        self.text_units = []
        for doc in documents:
            doc_units = self.split_into_units(doc['content'], unit_type)
            
            # ThÃªm thÃ´ng tin document vÃ o má»—i unit
            for unit in doc_units:
                unit['document_id'] = doc.get('id')
                unit['document_title'] = doc.get('title', 'Unknown')
                self.text_units.append(unit)
        
        print(f"ğŸ“„ Chia thÃ nh {len(self.text_units)} {unit_type}s tá»« {len(documents)} documents")
        
        # BÆ°á»›c 2: Chuáº©n bá»‹ cÃ¢u há»i
        self.questions = []
        for item in questions_data:
            if isinstance(item.get('content'), str):
                content = json.loads(item['content'])
            else:
                content = item.get('content', {})
            
            question = content.get('question', '')
            if question:
                self.questions.append({
                    'id': item.get('id'),
                    'question': question,
                    'data_type': item.get('data_type'),
                    'tokens': self.preprocess_text(question)
                })
        
        print(f"â“ Chuáº©n bá»‹ {len(self.questions)} cÃ¢u há»i")
        
        # BÆ°á»›c 3: Khá»Ÿi táº¡o BM25 vÃ  TF-IDF cho units
        unit_texts = [' '.join(unit['tokens']) for unit in self.text_units]
        
        if unit_texts:
            # BM25 cho units
            unit_token_lists = [unit['tokens'] for unit in self.text_units]
            self.bm25 = BM25Okapi(unit_token_lists)
            
            # TF-IDF cho questions
            question_texts = [' '.join(q['tokens']) for q in self.questions]
            if question_texts:
                self.tfidf_vectorizer = TfidfVectorizer()
                # Fit trÃªn cáº£ units vÃ  questions
                all_texts = unit_texts + question_texts
                self.tfidf_vectorizer.fit(all_texts)
                
                # Transform questions
                self.questions_tfidf = self.tfidf_vectorizer.transform(question_texts)
                
                print("ğŸ” Khá»Ÿi táº¡o BM25 vÃ  TF-IDF hoÃ n thÃ nh")
            else:
                print("âš ï¸ KhÃ´ng cÃ³ cÃ¢u há»i Ä‘á»ƒ phÃ¢n tÃ­ch")
        else:
            print("âš ï¸ KhÃ´ng cÃ³ units Ä‘á»ƒ phÃ¢n tÃ­ch")
    
    def calculate_unit_coverage(self, unit: Dict[str, Any]) -> Dict[str, Any]:
        """
        TÃ­nh Ä‘á»™ bao phá»§ cho má»™t unit cá»¥ thá»ƒ
        
        Args:
            unit: Unit cáº§n tÃ­nh coverage
            
        Returns:
            Dict: ThÃ´ng tin coverage cá»§a unit
        """
        if not self.questions or not self.bm25:
            return {
                'is_covered': False,
                'max_similarity': 0.0,
                'best_question': None,
                'similarities': []
            }
        
        unit_tokens = unit['tokens']
        similarities = []
        
        # TÃ­nh similarity vá»›i táº¥t cáº£ cÃ¢u há»i
        for i, question in enumerate(self.questions):
            # BM25 score - truyá»n query dáº¡ng list cá»§a tokens
            bm25_score = 0.0
            if len(unit_tokens) > 0:
                bm25_scores = self.bm25.get_scores(unit_tokens)
                # Láº¥y max score tá»« táº¥t cáº£ documents
                bm25_score = max(bm25_scores) if len(bm25_scores) > 0 else 0.0
            
            # Normalize BM25 score
            if bm25_score > 0:
                bm25_score = min(bm25_score / 10.0, 1.0)  # Simple normalization
            
            # TF-IDF cosine similarity
            tfidf_score = 0.0
            if self.tfidf_vectorizer and self.questions_tfidf is not None:
                try:
                    unit_text = ' '.join(unit_tokens)
                    unit_tfidf = self.tfidf_vectorizer.transform([unit_text])
                    question_tfidf = self.questions_tfidf[i:i+1]
                    tfidf_score = cosine_similarity(unit_tfidf, question_tfidf)[0][0]
                except:
                    tfidf_score = 0.0
            
            # Káº¿t há»£p scores
            combined_score = 0.3 * bm25_score + 0.7 * tfidf_score
            
            similarities.append({
                'question_id': question['id'],
                'question': question['question'],
                'data_type': question['data_type'],
                'bm25_score': float(bm25_score),
                'tfidf_score': float(tfidf_score),
                'combined_score': float(combined_score)
            })
        
        # TÃ¬m similarity cao nháº¥t
        max_similarity = max([s['combined_score'] for s in similarities], default=0.0)
        best_question = max(similarities, key=lambda x: x['combined_score'], default=None)
        
        is_covered = max_similarity >= self.coverage_threshold
        
        return {
            'is_covered': is_covered,
            'max_similarity': max_similarity,
            'best_question': best_question,
            'similarities': sorted(similarities, key=lambda x: x['combined_score'], reverse=True)[:3]  # Top 3
        }
    
    def analyze_coverage(self) -> Dict[str, Any]:
        """
        PhÃ¢n tÃ­ch coverage cho táº¥t cáº£ units
        
        Returns:
            Dict: Káº¿t quáº£ phÃ¢n tÃ­ch coverage
        """
        if not self.text_units:
            return {
                'total_units': 0,
                'covered_units': 0,
                'coverage_percentage': 0.0,
                'units_analysis': []
            }
        
        print("ğŸ” Báº¯t Ä‘áº§u phÃ¢n tÃ­ch coverage...")
        
        covered_count = 0
        units_analysis = []
        
        for i, unit in enumerate(self.text_units):
            coverage_info = self.calculate_unit_coverage(unit)
            
            if coverage_info['is_covered']:
                covered_count += 1
            
            unit_analysis = {
                'unit_id': unit['id'],
                'unit_type': unit['type'],
                'document_title': unit['document_title'],
                'content_preview': unit['content'][:100] + '...' if len(unit['content']) > 100 else unit['content'],
                'length': unit['length'],
                **coverage_info
            }
            
            units_analysis.append(unit_analysis)
            
            if (i + 1) % 10 == 0:
                print(f"  ğŸ“Š ÄÃ£ phÃ¢n tÃ­ch {i + 1}/{len(self.text_units)} units...")
        
        coverage_percentage = (covered_count / len(self.text_units)) * 100
        
        result = {
            'total_units': len(self.text_units),
            'covered_units': covered_count,
            'uncovered_units': len(self.text_units) - covered_count,
            'coverage_percentage': coverage_percentage,
            'threshold_used': self.coverage_threshold,
            'units_analysis': units_analysis
        }
        
        print(f"âœ… HoÃ n thÃ nh phÃ¢n tÃ­ch coverage: {coverage_percentage:.1f}% ({covered_count}/{len(self.text_units)} units)")
        
        return result
    
    def get_coverage_summary_by_document(self, coverage_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        TÃ³m táº¯t coverage theo tá»«ng document
        """
        doc_stats = {}
        
        for unit in coverage_result['units_analysis']:
            doc_title = unit['document_title']
            
            if doc_title not in doc_stats:
                doc_stats[doc_title] = {
                    'total_units': 0,
                    'covered_units': 0,
                    'uncovered_units': 0,
                    'coverage_percentage': 0.0
                }
            
            doc_stats[doc_title]['total_units'] += 1
            if unit['is_covered']:
                doc_stats[doc_title]['covered_units'] += 1
            else:
                doc_stats[doc_title]['uncovered_units'] += 1
        
        # TÃ­nh percentage cho tá»«ng doc
        for doc_title, stats in doc_stats.items():
            if stats['total_units'] > 0:
                stats['coverage_percentage'] = (stats['covered_units'] / stats['total_units']) * 100
        
        return doc_stats

def test_coverage_analyzer():
    """Test function cho coverage analyzer"""
    
    # Mock data
    documents = [
        {
            'id': 1,
            'title': 'Luáº­t test',
            'content': '''Äiá»u 1. Quy Ä‘á»‹nh chung vá» giao thÃ´ng.
            Giao thÃ´ng Ä‘Æ°á»ng bá»™ pháº£i tuÃ¢n theo luáº­t phÃ¡p.
            
            Äiá»u 2. Vá» Ä‘á»™ tuá»•i lÃ¡i xe.
            NgÆ°á»i lÃ¡i xe pháº£i Ä‘á»§ 18 tuá»•i trá»Ÿ lÃªn.
            Äá»‘i vá»›i xe mÃ´ tÃ´ thÃ¬ tá»« 16 tuá»•i.'''
        }
    ]
    
    questions_data = [
        {
            'id': 1,
            'data_type': 'word_matching',
            'content': json.dumps({
                'question': 'Äá»™ tuá»•i tá»‘i thiá»ƒu Ä‘á»ƒ lÃ¡i xe Ã´ tÃ´ lÃ  bao nhiÃªu?',
                'answer': '18 tuá»•i'
            })
        },
        {
            'id': 2,
            'data_type': 'concept_understanding',
            'content': json.dumps({
                'question': 'Giao thÃ´ng Ä‘Æ°á»ng bá»™ cáº§n tuÃ¢n theo quy Ä‘á»‹nh gÃ¬?',
                'answer': 'TuÃ¢n theo luáº­t phÃ¡p'
            })
        }
    ]
    
    # Test coverage
    analyzer = CoverageAnalyzer(coverage_threshold=0.3)
    analyzer.prepare_coverage_analysis(documents, questions_data, unit_type='sentence')
    
    result = analyzer.analyze_coverage()
    
    print(f"\nğŸ“Š COVERAGE ANALYSIS RESULT:")
    print(f"Total units: {result['total_units']}")
    print(f"Covered units: {result['covered_units']}")
    print(f"Coverage: {result['coverage_percentage']:.1f}%")
    
    print(f"\nğŸ“‹ DETAILED ANALYSIS:")
    for unit in result['units_analysis'][:3]:  # Show first 3
        print(f"Unit: {unit['content_preview']}")
        print(f"  Covered: {unit['is_covered']} (similarity: {unit['max_similarity']:.3f})")
        if unit['best_question']:
            print(f"  Best match: {unit['best_question']['question'][:50]}...")

if __name__ == "__main__":
    test_coverage_analyzer()
