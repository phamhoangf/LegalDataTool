#!/usr/bin/env python3
"""
Coverage Analyzer - ƒêo ƒë·ªô bao ph·ªß c·ªßa b·ªô c√¢u h·ªèi ƒë·ªëi v·ªõi vƒÉn b·∫£n g·ªëc
Uses hybrid search for optimal accuracy
"""

import re
import json
from typing import List, Dict, Any, Tuple, Optional
from hybrid_search import HybridSearchEngine, create_hybrid_search_engine
from document_parsers import LegalDocumentParser

class CoverageAnalyzer:
    """
    Ph√¢n t√≠ch ƒë·ªô bao ph·ªß c·ªßa b·ªô c√¢u h·ªèi ƒë·ªëi v·ªõi vƒÉn b·∫£n ph√°p lu·∫≠t
    S·ª≠ d·ª•ng Hybrid Search (BM25 + Semantic)
    """
    
    def __init__(self, 
                 coverage_threshold: float = 0.3,
                 hybrid_config: Optional[Dict[str, Any]] = None):
        """
        Args:
            coverage_threshold: Ng∆∞·ª°ng ƒë·ªÉ x√°c ƒë·ªãnh unit ƒë∆∞·ª£c bao ph·ªß (0-1)
            hybrid_config: Configuration for hybrid search engine
        """
        self.coverage_threshold = coverage_threshold
        self.should_stop = False  # Flag ƒë·ªÉ d·ª´ng ph√¢n t√≠ch
        
        print(f"üß† Initializing hybrid coverage analyzer with threshold {coverage_threshold}")
        self.hybrid_engine = create_hybrid_search_engine(hybrid_config)
        
        self.text_units = []  # C√°c ƒë∆°n v·ªã vƒÉn b·∫£n (c√¢u/ƒëo·∫°n)
        self.questions = []   # C√°c c√¢u h·ªèi ƒë√£ sinh
        
    def stop_analysis(self):
        """D·ª´ng qu√° tr√¨nh ph√¢n t√≠ch"""
        self.should_stop = True
        print("üõë ƒê√£ y√™u c·∫ßu d·ª´ng ph√¢n t√≠ch coverage")
        
    def reset_stop_flag(self):
        """Reset flag d·ª´ng ƒë·ªÉ chu·∫©n b·ªã cho ph√¢n t√≠ch m·ªõi"""
        self.should_stop = False
        
    def preprocess_text(self, text: str) -> List[str]:
        """
        Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát (t∆∞∆°ng t·ª± similarity_checker)
        """
        if not text:
            return []
            
        # Chuy·ªÉn th∆∞·ªùng
        text = text.lower()
        
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë v√† kho·∫£ng tr·∫Øng
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'\s+', ' ', text).strip()
        
        # T√°ch t·ª´ ƒë∆°n gi·∫£n (split by space)
        words = text.split()
        
        # Lo·∫°i b·ªè t·ª´ d·ª´ng ƒë∆°n gi·∫£n
        stop_words = {
            'l√†', 'c·ªßa', 'v√†', 'c√≥', 'trong', 'v·ªõi', 'theo', 'ƒë·ªÉ', 'ƒë∆∞·ª£c', 'c√°c', 
            'm·ªôt', 'n√†y', 'ƒë√≥', 'nh·ªØng', 't·ª´', 'tr√™n', 'd∆∞·ªõi', 'v·ªÅ', 'cho', 'hay',
            'b·∫±ng', 'nh∆∞', 'khi', 'n√†o', 'g√¨', 'ai', 'ƒë√¢u', 'sao', 'bao', 'nhi·ªÅu'
        }
        
        words = [word for word in words if word not in stop_words and len(word) > 1]
        
        return words
    
    def split_into_units(self, document_title: str, text: str, unit_type: str = 'sentence') -> List[Dict[str, Any]]:
        """
        Chia vƒÉn b·∫£n th√†nh c√°c ƒë∆°n v·ªã (units) s·ª≠ d·ª•ng LegalDocumentParser chu·∫©n
        
        Args:
            document_title: Ti√™u ƒë·ªÅ t√†i li·ªáu
            text: VƒÉn b·∫£n c·∫ßn chia
            unit_type: Lo·∫°i ƒë∆°n v·ªã ('sentence' s·ª≠ d·ª•ng parser, 'paragraph' manual)
            
        Returns:
            List[Dict]: Danh s√°ch c√°c units v·ªõi metadata
        """
        units = []
        
        if unit_type == 'sentence':
            # S·ª≠ d·ª•ng LegalDocumentParser chu·∫©n nh∆∞ trong data_generator
            parser = LegalDocumentParser()
            print(f"üîÑ Parsing document with LegalDocumentParser: {document_title}")
            parsed_data = parser.parse_document(document_title, text)
            print(f"   Articles parsed: {len(parsed_data.get('articles', []))}")
            
            # L·∫•y t·∫•t c·∫£ units t·ª´ parsed structure
            parser_units = parser.get_all_units(parsed_data)
            print(f"   Units generated: {len(parser_units)}")
            
            # Convert sang format cho coverage analyzer - ch·ªâ gi·ªØ c√°c fields c·∫ßn thi·∫øt
            for unit in parser_units:
                units.append({
                    'id': f"unit_{unit['source_article']}_{unit['source_khoan']}_{unit['source_diem']}",
                    'type': 'content_unit',
                    'content': unit['content'],
                    'length': unit.get('content_length', len(unit['content'])),
                    'tokens': self.preprocess_text(unit['content']),
                    'path': unit['path'],
                    'document_title': document_title
                })
            
            print(f"‚úÖ Successfully parsed {len(units)} units using LegalDocumentParser")
                    
        elif unit_type == 'paragraph':
            # Chia th√†nh ƒëo·∫°n (gi·ªØ logic c≈© cho paragraph mode)
            paragraphs = text.split('\n\n')
            for i, paragraph in enumerate(paragraphs):
                paragraph = paragraph.strip()
                if len(paragraph) > 50:  # L·ªçc ƒëo·∫°n qu√° ng·∫Øn
                    units.append({
                        'id': f'para_{i}',
                        'type': 'paragraph', 
                        'content': paragraph,
                        'length': len(paragraph),
                        'tokens': self.preprocess_text(paragraph)
                    })
        
        return units
    
    def prepare_coverage_analysis(self, documents: List[Dict[str, Any]], questions_data: List[Dict[str, Any]], unit_type: str = 'sentence'):
        """
        Chu·∫©n b·ªã d·ªØ li·ªáu cho ph√¢n t√≠ch coverage
        
        Args:
            documents: List c√°c documents v·ªõi content
            questions_data: List c√°c c√¢u h·ªèi ƒë√£ sinh v·ªõi source information
            unit_type: Lo·∫°i ƒë∆°n v·ªã ƒë·ªÉ ph√¢n t√≠ch
        """
        # B∆∞·ªõc 1: Chia vƒÉn b·∫£n th√†nh units s·ª≠ d·ª•ng LegalDocumentParser
        self.text_units = []
        for doc in documents:
            doc_title = doc.get('title', 'Unknown Document')
            doc_units = self.split_into_units(doc_title, doc['content'], unit_type)
            
            # Th√™m th√¥ng tin document v√†o m·ªói unit
            for unit in doc_units:
                unit['document_id'] = doc.get('id')
                unit['document_title'] = doc_title
                self.text_units.append(unit)
        
        print(f"üìÑ Chia th√†nh {len(self.text_units)} {unit_type}s t·ª´ {len(documents)} documents")
        
        # B∆∞·ªõc 2: Chu·∫©n b·ªã c√¢u h·ªèi v·ªõi source information
        self.questions = []
        for item in questions_data:
            if isinstance(item.get('content'), str):
                content = json.loads(item['content'])
            else:
                content = item.get('content', {})
            
            question = content.get('question', '')
            answer = content.get('answer', '')  # Extract answer
            sources = content.get('sources', [])  # Extract source information
            
            if question:
                self.questions.append({
                    'id': item.get('id'),
                    'question': question,
                    'answer': answer,  # Add answer field
                    'data_type': item.get('data_type'),
                    'sources': sources,  # Add source information
                    'tokens': self.preprocess_text(question)
                })
        
        print(f"‚ùì Chu·∫©n b·ªã {len(self.questions)} c√¢u h·ªèi")
        
        # B∆∞·ªõc 3: Kh·ªüi t·∫°o hybrid search engine
        unit_texts = [' '.join(unit['tokens']) for unit in self.text_units]
        
        if unit_texts and self.questions:
            # Use hybrid search for question-unit matching
            print("üß† Initializing hybrid search for coverage analysis...")
            self.hybrid_engine.index_documents(unit_texts, list(range(len(self.text_units))))
            print("‚úÖ Hybrid search ready for coverage analysis")
        else:
            print("‚ö†Ô∏è Kh√¥ng c√≥ units ho·∫∑c questions ƒë·ªÉ ph√¢n t√≠ch")
    
    def calculate_unit_coverage(self, unit: Dict[str, Any]) -> Dict[str, Any]:
        """
        T√≠nh ƒë·ªô bao ph·ªß cho m·ªôt unit c·ª• th·ªÉ, ch·ªâ t√≠nh v·ªõi questions c√≥ sources li√™n quan
        S·ª≠ d·ª•ng hybrid search cho accuracy t·ªët h∆°n
        
        Args:
            unit: Unit c·∫ßn t√≠nh coverage
            
        Returns:
            Dict: Th√¥ng tin coverage c·ªßa unit
        """
        base_result = {
            'is_covered': False,
            'max_similarity': 0.0,
            'best_question': None,
            'similarities': [],
            'relevant_questions_count': 0
        }
        
        if not self.questions:
            return base_result
        
        unit_tokens = unit['tokens']
        unit_text = ' '.join(unit_tokens)
        unit_path = unit.get('path', '')
        unit_doc_title = unit.get('document_title', 'Unknown')
        
        # Filter questions that reference this specific unit as source based on unit_path
        relevant_questions = []
        for i, question in enumerate(self.questions):
            # Check if this unit's path matches any source unit_path in the question
            for source in question.get('sources', []):
                # First priority: match by unit_path (new format)
                source_unit_path = source.get('unit_path', '')
                if source_unit_path and source_unit_path == unit_path:
                    relevant_questions.append((i, question))
                    break
                
                # STRICT: No fallback - ch·ªâ t√≠nh coverage cho units c√≥ exact unit_path match
                # Data kh√¥ng c√≥ unit_path s·∫Ω kh√¥ng ƒë∆∞·ª£c t√≠nh coverage (correct behavior)
                # V√¨ coverage analysis c·∫ßn ch√≠nh x√°c t·ª´ng unit, kh√¥ng ph·∫£i document level
        
        # If no relevant questions, return no coverage
        if not relevant_questions:
            return base_result
        
        similarities = []
        
        # Use hybrid search to compute similarity between unit and relevant questions' answers
        for i, question in relevant_questions:
            try:
                # Compute similarity between unit text and answer (not question)
                answer_text = question['question']
                if not answer_text:
                    # Skip if no answer available
                    continue
                    
                similarity_result = self.hybrid_engine.compute_similarity(unit_text, answer_text)
                combined_score = similarity_result['combined_score']
                
                similarities.append({
                    'question_id': question['id'],
                    'question': question['question'],
                    'data_type': question['data_type'],
                    'bm25_score': similarity_result['bm25_score'],
                    'semantic_score': similarity_result['semantic_score'],
                    'tfidf_score': similarity_result['tfidf_score'],
                    'combined_score': combined_score
                })
            except Exception as e:
                print(f"‚ö†Ô∏è Error computing hybrid similarity: {e}")
                # Fallback to zero score
                similarities.append({
                    'question_id': question['id'],
                    'question': question['question'],
                    'data_type': question['data_type'],
                    'bm25_score': 0.0,
                    'semantic_score': 0.0,
                    'tfidf_score': 0.0,
                    'combined_score': 0.0
                })
        
        # T√¨m similarity cao nh·∫•t
        max_similarity = max([s['combined_score'] for s in similarities], default=0.0)
        best_question = max(similarities, key=lambda x: x['combined_score'], default=None)
        
        is_covered = max_similarity >= self.coverage_threshold
        
        return {
            'is_covered': is_covered,
            'max_similarity': max_similarity,
            'best_question': best_question,
            'similarities': sorted(similarities, key=lambda x: x['combined_score'], reverse=True)[:3],  # Top 3
            'relevant_questions_count': len(relevant_questions)
        }
    
    def analyze_coverage(self) -> Dict[str, Any]:
        """
        Ph√¢n t√≠ch coverage cho t·∫•t c·∫£ units
        
        Returns:
            Dict: K·∫øt qu·∫£ ph√¢n t√≠ch coverage
        """
        if not self.text_units:
            return {
                'total_units': 0,
                'covered_units': 0,
                'coverage_percentage': 0.0,
                'units_analysis': []
            }
        
        print("üß† B·∫Øt ƒë·∫ßu ph√¢n t√≠ch coverage v·ªõi Hybrid Search (optimized - ch·ªâ t√≠nh v·ªõi relevant questions)...")
        self.reset_stop_flag()  # Reset flag khi b·∫Øt ƒë·∫ßu
        
        covered_count = 0
        units_analysis = []
        total_relevant_questions = 0
        
        for i, unit in enumerate(self.text_units):
            # Ki·ªÉm tra n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu d·ª´ng
            if self.should_stop:
                print(f"üõë Ph√¢n t√≠ch b·ªã d·ª´ng t·∫°i unit {i + 1}/{len(self.text_units)}")
                break
                
            coverage_info = self.calculate_unit_coverage(unit)
            
            if coverage_info['is_covered']:
                covered_count += 1
            
            # Track total relevant questions count
            total_relevant_questions += coverage_info.get('relevant_questions_count', 0)
            
            unit_analysis = {
                'unit_id': unit['id'],
                'unit_type': unit['type'],
                'document_title': unit['document_title'],
                'content_preview': unit['content'][:100] + '...' if len(unit['content']) > 100 else unit['content'],
                'length': unit['length'],
                **coverage_info
            }
            
            units_analysis.append(unit_analysis)
            
            if (i + 1) % 10 == 0:
                print(f"  üìä ƒê√£ ph√¢n t√≠ch {i + 1}/{len(self.text_units)} units... (relevant questions so far: {total_relevant_questions})")
        
        # T√≠nh coverage d·ª±a tr√™n s·ªë unit ƒë√£ x·ª≠ l√Ω
        processed_units = len(units_analysis)
        coverage_percentage = (covered_count / processed_units) * 100 if processed_units > 0 else 0
        
        result = {
            'total_units': len(self.text_units),
            'processed_units': processed_units,
            'covered_units': covered_count,
            'uncovered_units': processed_units - covered_count,
            'coverage_percentage': coverage_percentage,
            'threshold_used': self.coverage_threshold,
            'was_stopped': self.should_stop,
            'total_questions': len(self.questions),
            'total_relevant_calculations': total_relevant_questions,
            'optimization_ratio': f"{total_relevant_questions}/{processed_units * len(self.questions)} ({(total_relevant_questions / (processed_units * len(self.questions)) * 100):.1f}%)" if processed_units > 0 and len(self.questions) > 0 else "0/0 (0.0%)",
            'units_analysis': units_analysis
        }
        
        status_message = "üõë ƒê√£ d·ª´ng" if self.should_stop else "‚úÖ Ho√†n th√†nh"
        print(f"{status_message} ph√¢n t√≠ch coverage: {coverage_percentage:.1f}% ({covered_count}/{processed_units} units ƒë√£ x·ª≠ l√Ω)")
        
        if processed_units > 0 and len(self.questions) > 0:
            total_possible_calculations = processed_units * len(self.questions)
            optimization_saved = total_possible_calculations - total_relevant_questions
            optimization_percentage = (optimization_saved / total_possible_calculations * 100) if total_possible_calculations > 0 else 0.0
            
            if optimization_percentage > 0.1:  # Only show if significant optimization
                print(f"üöÄ Optimization: T√≠nh {total_relevant_questions} similarities thay v√¨ {total_possible_calculations} (ti·∫øt ki·ªám {optimization_percentage:.1f}%)")
            else:
                print(f"üîç Analysis: T√≠nh {total_relevant_questions} similarity calculations cho {processed_units} units")
        
        return result
    
    def get_coverage_summary_by_document(self, coverage_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        T√≥m t·∫Øt coverage theo t·ª´ng document
        """
        doc_stats = {}
        
        for unit in coverage_result['units_analysis']:
            doc_title = unit['document_title']
            
            if doc_title not in doc_stats:
                doc_stats[doc_title] = {
                    'total_units': 0,
                    'covered_units': 0,
                    'uncovered_units': 0,
                    'coverage_percentage': 0.0
                }
            
            doc_stats[doc_title]['total_units'] += 1
            if unit['is_covered']:
                doc_stats[doc_title]['covered_units'] += 1
            else:
                doc_stats[doc_title]['uncovered_units'] += 1
        
        # T√≠nh percentage cho t·ª´ng doc
        for doc_title, stats in doc_stats.items():
            if stats['total_units'] > 0:
                stats['coverage_percentage'] = (stats['covered_units'] / stats['total_units']) * 100
        
        return doc_stats

        return doc_stats