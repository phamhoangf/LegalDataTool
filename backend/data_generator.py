from google import genai
from google.genai import types
import json
import random
import os
import re
import time  # Add time import for rate limiting
from typing import List, Dict, Any
from pydantic import BaseModel
from similarity_checker import QuestionSimilarityChecker
from document_parsers import LegalDocumentParser

# HuggingFace imports
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("‚ö†Ô∏è HuggingFace transformers not available. Install with: pip install transformers torch")

class SourceReference(BaseModel):
    """Tham chi·∫øu ƒë·∫øn ngu·ªìn c·ªßa th√¥ng tin"""
    article_number: str  # S·ªë ƒëi·ªÅu (v√≠ d·ª•: "60", "61")
    article_title: str   # Ti√™u ƒë·ªÅ ƒëi·ªÅu (v√≠ d·ª•: "ƒêi·ªÅu 60. ƒê·ªô tu·ªïi c·ªßa ng∆∞·ªùi l√°i xe")
    document_title: str  # T√™n t√†i li·ªáu (v√≠ d·ª•: "Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô 2008")

class LegalQA(BaseModel):
    """C·∫•u tr√∫c c√¢u h·ªèi-ƒë√°p √°n ph√°p l√Ω"""
    question: str
    answer: str

class LegalQAList(BaseModel):
    """Danh s√°ch c√¢u h·ªèi-ƒë√°p √°n (kh√¥ng c·∫ßn sources v√¨ ƒë√£ rule-based)"""
    qa_pairs: List[LegalQA]

class DataGenerator:
    """Class sinh d·ªØ li·ªáu hu·∫•n luy·ªán cho LegalSLM - Version g·ªçn g√†ng"""
    
    def __init__(self, api_key: str = None, similarity_threshold: float = 0.75):
        # Set API key
        if api_key:
            os.environ['GEMINI_API_KEY'] = api_key
        elif not os.environ.get('GEMINI_API_KEY'):
            google_key = os.environ.get('GOOGLE_API_KEY')
            if google_key:
                os.environ['GEMINI_API_KEY'] = google_key
        
        self.client = genai.Client()
        self.model = "gemini-2.5-flash"
        
        # Kh·ªüi t·∫°o similarity checker
        self.similarity_checker = QuestionSimilarityChecker(similarity_threshold=similarity_threshold)
        print(f"üîç Initialized similarity checker with threshold {similarity_threshold}")
        
        # HuggingFace model setup
        self.hf_model = None
        self.hf_tokenizer = None
        
        # Rate limiting for Gemini API (15 req/min = 4 seconds per request)
        self.last_api_call = 0
        self.min_interval = 4.0  # seconds between API calls
    
    def init_huggingface_model(self, model_name: str = "phamhoangf/qwen3-4b-generate-data"):
        """Initialize HuggingFace model"""
        if not HF_AVAILABLE:
            raise ImportError("HuggingFace transformers not available")
        
        try:
            print(f"ü§ñ Loading HuggingFace model: {model_name}")
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            self.hf_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.hf_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            print(f"‚úÖ HuggingFace model loaded on {device}")
            
        except Exception as e:
            print(f"‚ùå Failed to load HuggingFace model: {str(e)}")
            raise
    
    def generate_qa_with_gemini(self, prompt: str, temperature: float = 0.7) -> LegalQAList:
        """Sinh QA b·∫±ng Gemini API v·ªõi rate limiting"""
        # Rate limiting: ƒë·∫£m b·∫£o 15 req/min (4 gi√¢y/request)
        current_time = time.time()
        elapsed = current_time - self.last_api_call
        
        if elapsed < self.min_interval:
            sleep_time = self.min_interval - elapsed
            print(f"‚è≥ Rate limiting: sleeping {sleep_time:.1f}s...")
            time.sleep(sleep_time)
        
        self.last_api_call = time.time()
        
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=temperature,
                top_p=random.uniform(0.85, 0.95),
                max_output_tokens=3000,
                response_mime_type="application/json",
                response_schema=LegalQAList,
                seed=random.randint(1, 1000000)
            )
        )
        return response.parsed
    
    def generate_qa_with_huggingface(self, prompt: str, temperature: float = 0.7) -> LegalQAList:
        """Sinh QA b·∫±ng HuggingFace model"""
        if not self.hf_model:
            raise ValueError("HuggingFace model not initialized. Call init_huggingface_model() first")
        
        # Format prompt cho model
        formatted_prompt = f"<|system|>B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam. H√£y t·∫°o c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi t·ª´ vƒÉn b·∫£n ph√°p lu·∫≠t.<|end|>\n<|user|>{prompt}<|end|>\n<|assistant|>"
        
        inputs = self.hf_tokenizer.encode(formatted_prompt, return_tensors="pt")
        if self.hf_model.device.type == "cuda":
            inputs = inputs.to("cuda")
        
        with torch.no_grad():
            outputs = self.hf_model.generate(
                inputs,
                max_new_tokens=2048,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.hf_tokenizer.eos_token_id
            )
        
        response_text = self.hf_tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
        
        # Parse JSON response
        try:
            response_json = json.loads(response_text)
            return LegalQAList(**response_json)
        except json.JSONDecodeError:
            # Fallback - t·∫°o single QA n·∫øu kh√¥ng parse ƒë∆∞·ª£c
            return LegalQAList(qa_pairs=[LegalQA(question="Sample question", answer="Sample answer")])
    
    def generate_qa(self, prompt: str, llm_type: str = "gemini", temperature: float = 0.7) -> LegalQAList:
        """Sinh QA v·ªõi LLM ƒë∆∞·ª£c ch·ªçn"""
        if llm_type == "gemini":
            return self.generate_qa_with_gemini(prompt, temperature)
        elif llm_type == "huggingface":
            return self.generate_qa_with_huggingface(prompt, temperature)
        else:
            raise ValueError(f"Unsupported LLM type: {llm_type}")
    
    
    def get_rule_based_difficulty(self, data_type: str, num_sources: int) -> str:
        """Rule-based difficulty thay v√¨ y√™u c·∫ßu LLM t·∫°o ra"""
        if data_type == 'word_matching':
            return 'easy'
        elif data_type == 'concept_understanding':
            return 'easy' if num_sources == 1 else 'medium'
        elif data_type == 'multi_paragraph_reading':
            return 'medium' if num_sources <= 3 else 'hard'
        elif data_type == 'multi_hop_reasoning':
            return 'hard'
        else:
            return 'medium'

    def update_similarity_corpus(self, existing_questions_data: List[Dict[str, Any]]):
        """C·∫≠p nh·∫≠t corpus cho similarity checker v·ªõi d·ªØ li·ªáu hi·ªán c√≥"""
        self.similarity_checker.update_corpus(existing_questions_data)
    
    def filter_duplicate_questions(self, new_samples: List[Dict[str, Any]], verbose: bool = True) -> List[Dict[str, Any]]:
        """L·ªçc b·ªè c√°c c√¢u h·ªèi tr√πng l·∫∑p t·ª´ danh s√°ch samples m·ªõi"""
        if not new_samples:
            return []
        
        questions = [sample.get('question', '') for sample in new_samples]
        similarity_results = self.similarity_checker.check_batch_similarity(questions)
        
        filtered_samples = []
        duplicates_found = 0
        
        for i, (sample, result) in enumerate(zip(new_samples, similarity_results)):
            if not result['is_duplicate']:
                filtered_samples.append(sample)
            else:
                duplicates_found += 1
                if verbose:
                    print(f"üö´ Filtered duplicate question {i+1}:")
                    print(f"   Question: {result['question'][:80]}...")
                    print(f"   Max similarity: {result['max_similarity']:.3f}")
        
        if verbose and duplicates_found > 0:
            print(f"üîç Filtered {duplicates_found}/{len(new_samples)} duplicate questions")
        
        return filtered_samples

    def get_units_from_parsed_structure(self, document) -> List[Dict]:
        """L·∫•y units t·ª´ document s·ª≠ d·ª•ng LegalDocumentParser"""
        try:
            # S·ª≠ d·ª•ng parser ƒë·ªÉ l·∫•y units tr·ª±c ti·∫øp
            parser = LegalDocumentParser()
            
            # N·∫øu c√≥ parsed structure th√¨ s·ª≠ d·ª•ng
            if hasattr(document, 'parsed_structure') and document.parsed_structure:
                try:
                    parsed_data = json.loads(document.parsed_structure)
                    print(f"üîç Using existing parsed structure for {document.title}")
                    print(f"   Articles found: {len(parsed_data.get('articles', []))}")
                    units = parser.get_all_units(parsed_data)
                    print(f"   Units extracted: {len(units)}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to use parsed structure: {str(e)}, re-parsing...")
                    # Fallback: parse l·∫°i t·ª´ content
                    parsed_data = parser.parse_document(document.title, document.content)
                    print(f"   Re-parsed articles: {len(parsed_data.get('articles', []))}")
                    units = parser.get_all_units(parsed_data)
                    print(f"   Units from re-parse: {len(units)}")
            else:
                # Parse t·ª´ ƒë·∫ßu n·∫øu ch∆∞a c√≥ parsed structure
                print(f"üîÑ Parsing document from scratch: {document.title}")
                parsed_data = parser.parse_document(document.title, document.content)
                print(f"   Articles parsed: {len(parsed_data.get('articles', []))}")
                units = parser.get_all_units(parsed_data)
                print(f"   Units generated: {len(units)}")
            
            # Convert to format cho data generator
            converted_units = []
            for unit in units:
                converted_units.append({
                    "id": f"unit_{unit['source_article']}_{unit['source_khoan']}_{unit['source_diem']}",
                    "title": unit['path'].split(' > ')[-1] if ' > ' in unit['path'] else f"Unit {unit['source_article']}",
                    "content": unit['content'],
                    "document_title": document.title,
                    "path": unit['path'],
                    "metadata": {
                        "source_article": unit['source_article'],
                        "source_khoan": unit['source_khoan'], 
                        "source_diem": unit['source_diem'],
                        "source_document": document.title,
                        "unit_type": "content_unit",
                        "length": unit.get('content_length', len(unit['content']))
                    }
                })
            
            print(f"‚úÖ Extracted {len(converted_units)} units from {document.title}")
            return converted_units
            
        except Exception as e:
            print(f"‚ùå Failed to extract units from {document.title}: {str(e)}")
            return []

    def monte_carlo_sample_articles(self, all_articles: List[Dict], sample_size: int, iteration: int = 0) -> List[Dict]:
        """Monte Carlo sampling v·ªõi ƒë·ªô ng·∫´u nhi√™n cao h∆°n"""
        if not all_articles or sample_size <= 0:
            return []
            
        if sample_size >= len(all_articles):
            articles_copy = all_articles.copy()
            # Th√™m iteration seed ƒë·ªÉ m·ªói l·∫ßn g·ªçi kh√°c nhau
            random.seed(hash(f"shuffle_{iteration}_{time.time()}") % 100000)
            random.shuffle(articles_copy)
            random.seed()  # Reset seed
            return articles_copy
        
        # T·∫°o random seed kh√°c nhau cho m·ªói iteration
        random.seed(hash(f"sampling_{iteration}_{time.time()}_{random.randint(1, 10000)}") % 100000)
        
        # T√≠nh weights v·ªõi nhi·ªÅu y·∫øu t·ªë ng·∫´u nhi√™n h∆°n
        weights = []
        for i, article in enumerate(all_articles):
            # Base weight t·ª´ content length v·ªõi random factor l·ªõn h∆°n
            content_length = article.get('metadata', {}).get('length', len(article.get('content', '')))
            length_weight = min(content_length / 800, 2.0)
            
            # Position weight v·ªõi th√™m random
            position_weight = random.uniform(0.8, 1.5)  # Ho√†n to√†n random thay v√¨ d·ª±a v√†o article number
            
            # Iteration-based randomness ƒë·ªÉ ƒë·∫£m b·∫£o m·ªói l·∫ßn kh√°c nhau
            iteration_factor = random.uniform(0.5, 2.0) * (1 + 0.1 * (iteration % 10))
            
            # Index-based diversity ƒë·ªÉ tr√°nh bias v·ªã tr√≠
            index_factor = random.uniform(0.7, 1.3) * (1 + 0.05 * (i % 7))
            
            # Time-based randomness
            time_factor = random.uniform(0.8, 1.2) * (1 + 0.001 * (int(time.time()) % 1000))
            
            # K·∫øt h·ª£p t·∫•t c·∫£ factors
            final_weight = length_weight * position_weight * iteration_factor * index_factor * time_factor
            weights.append(max(final_weight, 0.1))
        
        # Th√™m noise v√†o weights ƒë·ªÉ tƒÉng entropy
        for i in range(len(weights)):
            noise = random.uniform(0.9, 1.1)
            weights[i] *= noise
        
        # Monte Carlo sampling v·ªõi multiple rounds ƒë·ªÉ tƒÉng randomness
        selected = []
        available_indices = list(range(len(all_articles)))
        available_weights = weights.copy()
        
        for round_idx in range(sample_size):
            if not available_indices:
                break
            
            # Th√™m round-based randomness
            round_factor = random.uniform(0.9, 1.1)
            adjusted_weights = [w * round_factor * random.uniform(0.95, 1.05) for w in available_weights]
            
            total_weight = sum(adjusted_weights)
            if total_weight == 0:
                chosen_idx = random.randint(0, len(available_indices) - 1)
            else:
                # Multiple random attempts ƒë·ªÉ tƒÉng entropy
                best_choice = None
                for attempt in range(3):  # Th·ª≠ 3 l·∫ßn, ch·ªçn l·∫ßn cu·ªëi
                    rand_val = random.uniform(0, total_weight)
                    cumsum = 0
                    for i, weight in enumerate(adjusted_weights):
                        cumsum += weight
                        if rand_val <= cumsum:
                            best_choice = i
                            break
                    if best_choice is None:
                        best_choice = len(adjusted_weights) - 1
                
                chosen_idx = best_choice if best_choice is not None else random.randint(0, len(available_indices) - 1)
            
            # Add selected article
            selected.append(all_articles[available_indices[chosen_idx]])
            
            # Remove from available
            available_indices.pop(chosen_idx)
            available_weights.pop(chosen_idx)
        
        # Final shuffle v·ªõi iteration seed
        random.seed(hash(f"final_{iteration}_{len(selected)}_{time.time()}") % 100000)
        random.shuffle(selected)
        random.seed()  # Reset seed
        
        print(f"üé≤ Monte Carlo sampling (iteration {iteration}): ch·ªçn {len(selected)}/{len(all_articles)} articles v·ªõi high entropy")
        return selected

    def generate_from_multiple_documents(self, documents, topic_name, data_type, num_samples, llm_type="gemini"):
        """Sinh d·ªØ li·ªáu t·ª´ nhi·ªÅu documents - main method"""
        if not documents:
            return []

        print(f"üîç Ph√¢n t√≠ch {len(documents)} documents...")

        # L·∫•y units t·ª´ parsed structure
        all_units = []
        for doc in documents:
            units = self.get_units_from_parsed_structure(doc)
            all_units.extend(units)
            print(f"  üìã {doc.title}: {len(units)} units")

        print(f"üìä T·ªïng c·ªông: {len(all_units)} units t·ª´ {len(documents)} t√†i li·ªáu")

        # Monte Carlo sampling
        max_units = min(len(all_units), max(num_samples // 2, 10))
        selected_units = self.monte_carlo_sample_articles(all_units, max_units)
        print(f"  üéØ ƒê√£ ch·ªçn {len(selected_units)} units")

        # Sinh d·ªØ li·ªáu
        all_samples = self.generate_samples_from_units(selected_units, topic_name, data_type, num_samples, llm_type)

        # L·ªçc tr√πng l·∫∑p
        print(f"üîç Ki·ªÉm tra t∆∞∆°ng ƒë·ªìng cho {len(all_samples)} samples...")
        filtered_samples = self.filter_duplicate_questions(all_samples, verbose=True)

        print(f"‚úÖ Ho√†n th√†nh: {len(filtered_samples)} samples (ƒë√£ l·ªçc {len(all_samples) - len(filtered_samples)} tr√πng l·∫∑p)")
        return filtered_samples[:num_samples]

    def generate_samples_from_units(self, units, topic, data_type, num_samples, llm_type="gemini"):
        """Sinh d·ªØ li·ªáu ƒë∆°n gi·∫£n v·ªõi sources chung cho t·∫•t c·∫£ c√¢u h·ªèi"""
        if not units:
            return []
        
        # X√°c ƒë·ªãnh s·ªë sources theo y√™u c·∫ßu
        num_sources_map = {
            'word_matching': min(1, len(units)),
            'concept_understanding': min(1, len(units)),
            'multi_paragraph_reading': min(2, len(units)),
            'multi_hop_reasoning': min(3, len(units))
        }
        num_sources = num_sources_map.get(data_type, min(3, len(units)))
        
        # T·∫°o c√¢u h·ªèi - m·ªói iteration t·ª± Monte Carlo ch·ªçn units
        all_samples = []
        
        # Rate limiting info
        if llm_type == "gemini":
            estimated_time = num_samples * self.min_interval / 60  # minutes
            print(f"‚è≥ Estimated time for {num_samples} samples with Gemini: {estimated_time:.1f} minutes")
        
        for i in range(num_samples):
            print(f"üîÑ Generating sample {i+1}/{num_samples}...")
            
            # Monte Carlo sampling cho iteration n√†y
            selected_units = self.monte_carlo_sample_articles(units, num_sources, iteration=i)
            
            # T·∫°o sources v√† content cho iteration n√†y
            iteration_sources = []
            combined_content = []
            
            for unit in selected_units:
                source_ref = SourceReference(
                    article_number=str(unit['metadata']['source_article']) if unit['metadata']['source_article'] else "unknown",
                    article_title=unit['title'],
                    document_title=unit['document_title']
                )
                iteration_sources.append(source_ref)
                unit_path = unit.get('path', unit['title'])
                combined_content.append(f"--- {unit['title']} ({unit_path}) ---\n{unit['content']}")

            combined_text = "\n\n".join(combined_content)
            
            # Rule-based difficulty cho iteration n√†y
            difficulty = self.get_rule_based_difficulty(data_type, len(selected_units))
            
            # T·∫°o prompt cho iteration n√†y
            prompt = self.create_diverse_prompt(combined_text, topic, data_type, difficulty, i)
            
            try:
                temperature = random.uniform(0.6, 0.9)
                
                # S·ª≠ d·ª•ng LLM ƒë∆∞·ª£c ch·ªçn
                structured_data = self.generate_qa(prompt, llm_type, temperature)
                
                # Convert v·ªõi sources chung v√† rule-based difficulty
                for qa_pair in structured_data.qa_pairs:
                    sample = {
                        'question': qa_pair.question,
                        'answer': qa_pair.answer,
                        'difficulty': difficulty,  
                        'sources': [
                            {
                                'article_number': src.article_number,
                                'article_title': src.article_title,
                                'document_title': src.document_title
                            } for src in iteration_sources  # Sources cho iteration n√†y
                        ],
                        'metadata': {
                            'generation_method': 'per_iteration_monte_carlo',
                            'num_sources': len(selected_units),
                            'temperature': temperature,
                            'llm_type': llm_type
                        }
                    }
                    all_samples.append(sample)
                    
                print(f"‚úÖ Sample {i+1}/{num_samples} completed")
                    
            except Exception as e:
                print(f"‚ùå Generation failed for sample {i+1}/{num_samples}: {e}")
                continue
        
        return all_samples

    def create_diverse_prompt(self, content, topic, data_type, difficulty, iteration):
        """H√†m g·ªëc t·∫°o prompt ƒëa d·∫°ng - s·ª≠ d·ª•ng l√†m base cho c√°c lo·∫°i c√¢u h·ªèi"""
        # C·∫•u tr√∫c c√¢u h·ªèi ƒëa d·∫°ng
        question_starters = [
            "Khi n√†o", "Trong tr∆∞·ªùng h·ª£p n√†o", "Ai c√≥ tr√°ch nhi·ªám",
            "Vi·ªác...ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o", "ƒêi·ªÅu ki·ªán...l√† g√¨",
            "M·ª©c ph·∫°t...l√† bao nhi√™u", "Quy tr√¨nh...di·ªÖn ra ra sao",
            "T·∫°i sao", "V√¨ sao", "L√†m c√°ch n√†o", "B·∫±ng ph∆∞∆°ng th·ª©c n√†o",
            "C√≥ ƒë∆∞·ª£c ph√©p", "C√≥ b·∫Øt bu·ªôc", "C√≥ c·∫ßn thi·∫øt",
            "Th·ªß t·ª•c...nh∆∞ th·∫ø n√†o", "H√¨nh th·ª©c...l√† g√¨", "Ph·∫°m vi...ra sao"
        ]
        
        focus_areas = [
            "quy ƒë·ªãnh th·ª±c t·∫ø v√† ·ª©ng d·ª•ng c·ª• th·ªÉ",
            "tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá v√† ƒëi·ªÅu ki·ªán ƒë·∫∑c bi·ªát", 
            "nghƒ©a v·ª• v√† quy·ªÅn h·∫°n c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng",
            "m·ª©c ph·∫°t v√† h·∫≠u qu·∫£ ph√°p l√Ω",
            "quy tr√¨nh th·ªß t·ª•c ph√°p l√Ω chi ti·∫øt",
            "ƒë·ªãnh nghƒ©a thu·∫≠t ng·ªØ chuy√™n m√¥n",
            "th·∫©m quy·ªÅn v√† tr√°ch nhi·ªám qu·∫£n l√Ω"
        ]
        
        # Random selection v·ªõi seed t·ª´ iteration ƒë·ªÉ t·∫°o diversity
        random.seed(hash(f"{data_type}_{iteration}_{topic}") % 10000)
        starter = random.choice(question_starters)
        focus = random.choice(focus_areas)
        
        # Reset seed
        random.seed()
        
        # G·ªçi h√†m con t∆∞∆°ng ·ª©ng v·ªõi data_type
        if data_type == "word_matching":
            return self.create_word_matching_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "concept_understanding":
            return self.create_concept_understanding_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "multi_paragraph_reading":
            return self.create_multi_paragraph_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "multihop":
            return self.create_multihop_prompt(content, topic, starter, focus, difficulty)
        else:
            return self.create_concept_understanding_prompt(content, topic, starter, focus, difficulty)

    def create_word_matching_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho lo·∫°i Word Matching - t√¨m t·ª´ kh√≥a, thu·∫≠t ng·ªØ c·ª• th·ªÉ trong vƒÉn b·∫£n"""
        return f"""
    D∆∞·ªõi ƒë√¢y l√† c√°c ƒëi·ªÅu lu·∫≠t v·ªÅ ch·ªß ƒë·ªÅ "{topic}":

    {content}

    H√£y t·∫°o 1 c√¢u h·ªèi lo·∫°i WORD MATCHING (ƒë·ªô kh√≥ {difficulty}) t·∫≠p trung v√†o {focus}.

    ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI WORD MATCHING:
    - Y√™u c·∫ßu t√¨m t·ª´ kh√≥a, thu·∫≠t ng·ªØ c·ª• th·ªÉ 
    - H·ªèi v·ªÅ ƒë·ªãnh nghƒ©a ch√≠nh x√°c c·ªßa c√°c kh√°i ni·ªám ph√°p l√Ω
    - C√¢u tr·∫£ l·ªùi l√† th√¥ng tin c·ª• th·ªÉ, r√µ r√†ng
    - T·∫≠p trung v√†o thu·∫≠t ng·ªØ chuy√™n m√¥n, s·ªë li·ªáu c·ª• th·ªÉ

    Y√äU C·∫¶U QUAN TR·ªåNG:
    1. C√¢u h·ªèi v√† ƒë√°p √°n ph·∫£i HO√ÄN TO√ÄN ƒê·ªòC L·∫¨P - c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m√† kh√¥ng c·∫ßn context b√™n ngo√†i
    2. TUY·ªÜT ƒê·ªêI KH√îNG d√πng "d·ª±a tr√™n ƒëi·ªÅu lu·∫≠t tr√™n", "theo quy ƒë·ªãnh tr√™n", "cƒÉn c·ª© v√†o ƒëi·ªÅu tr√™n"
    3. Kh√¥ng c·∫ßn thi·∫øt ph·∫£i tr√≠ch d·∫´n, N·∫æU c·∫ßn tr√≠ch d·∫´n: ph·∫£i ghi ƒê·∫¶Y ƒê·ª¶ t√™n vƒÉn b·∫£n (v√≠ d·ª•: "Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô 2008, ƒêi·ªÅu 25") ho·∫∑c n·ªôi dung ph·∫ßn vƒÉn b·∫£n c·∫ßn tr√≠ch d·∫´n
    4. B·∫°n c√≥ th·ªÉ tham kh·∫£o b·∫Øt ƒë·∫ßu c√¢u h·ªèi b·∫±ng "{starter}..." 
    5. ƒê√°p √°n ph·∫£i DI·ªÑN GI·∫¢I ƒê·∫¶Y ƒê·ª¶, kh√¥ng c·ª•t l·ªßn nh∆∞ "11%" hay "Th·ªëng ƒë·ªëc"

    V√ç D·ª§ T·ªêT:
    Question: "ƒê·ªô tu·ªïi t·ªëi thi·ªÉu ƒë·ªÉ ƒë∆∞·ª£c c·∫•p b·∫±ng l√°i xe √¥ t√¥ l√† bao nhi√™u?"
    Answer: "ƒê·ªô tu·ªïi t·ªëi thi·ªÉu ƒë·ªÉ ƒë∆∞·ª£c c·∫•p b·∫±ng l√°i xe √¥ t√¥ l√† 18 tu·ªïi ƒë·ªëi v·ªõi xe √¥ t√¥ con v√† 21 tu·ªïi ƒë·ªëi v·ªõi xe t·∫£i, xe kh√°ch theo quy ƒë·ªãnh c·ªßa Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô."

    V√ç D·ª§ X·∫§U (TR√ÅNH):
    Answer: "D·ª±a tr√™n ƒëi·ªÅu lu·∫≠t tr√™n, ƒë·ªô tu·ªïi l√† 18 tu·ªïi."

    Tr·∫£ v·ªÅ output d∆∞·ªõi d·∫°ng JSON v·ªõi qa_pairs.
    """

    def create_concept_understanding_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho lo·∫°i Concept Understanding - hi·ªÉu kh√°i ni·ªám, nguy√™n t·∫Øc"""
        return f"""
    D∆∞·ªõi ƒë√¢y l√† c√°c ƒëi·ªÅu lu·∫≠t v·ªÅ ch·ªß ƒë·ªÅ "{topic}":

    {content}

    H√£y t·∫°o 1 c√¢u h·ªèi lo·∫°i CONCEPT UNDERSTANDING (ƒë·ªô kh√≥ {difficulty}) t·∫≠p trung v√†o {focus}.

    ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI CONCEPT UNDERSTANDING:
    - Ki·ªÉm tra hi·ªÉu bi·∫øt v·ªÅ kh√°i ni·ªám, nguy√™n t·∫Øc ph√°p l√Ω
    - Y√™u c·∫ßu gi·∫£i th√≠ch √Ω nghƒ©a, m·ª•c ƒë√≠ch c·ªßa quy ƒë·ªãnh
    - C√¢u tr·∫£ l·ªùi c·∫ßn di·ªÖn gi·∫£i, gi·∫£i th√≠ch r√µ r√†ng
    - T·∫≠p trung v√†o vi·ªác hi·ªÉu "t·∫°i sao" v√† "nh∆∞ th·∫ø n√†o"

    Y√äU C·∫¶U QUAN TR·ªåNG:
    1. C√¢u h·ªèi v√† ƒë√°p √°n ph·∫£i HO√ÄN TO√ÄN ƒê·ªòC L·∫¨P - c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m√† kh√¥ng c·∫ßn context b√™n ngo√†i
    2. TUY·ªÜT ƒê·ªêI KH√îNG d√πng "d·ª±a tr√™n ƒëi·ªÅu lu·∫≠t tr√™n", "theo quy ƒë·ªãnh tr√™n", "cƒÉn c·ª© v√†o ƒëi·ªÅu tr√™n"
    3. Kh√¥ng c·∫ßn thi·∫øt ph·∫£i tr√≠ch d·∫´n, N·∫æU c·∫ßn tr√≠ch d·∫´n: ph·∫£i ghi ƒê·∫¶Y ƒê·ª¶ t√™n vƒÉn b·∫£n (v√≠ d·ª•: "Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô 2008, ƒêi·ªÅu 25") ho·∫∑c n·ªôi dung ph·∫ßn vƒÉn b·∫£n c·∫ßn tr√≠ch d·∫´n
    4. B·∫°n c√≥ th·ªÉ tham kh·∫£o b·∫Øt ƒë·∫ßu c√¢u h·ªèi b·∫±ng "{starter}..."
    5. ƒê√°p √°n c·∫ßn gi·∫£i th√≠ch kh√°i ni·ªám ƒë·∫ßy ƒë·ªß, c√≥ th·ªÉ bao g·ªìm v√≠ d·ª• minh h·ªça

    V√ç D·ª§ T·ªêT:
    Question: "T·∫°i sao vi·ªác ki·ªÉm ƒë·ªãnh ƒë·ªãnh k·ª≥ ph∆∞∆°ng ti·ªán giao th√¥ng l√† b·∫Øt bu·ªôc?"
    Answer: "Vi·ªác ki·ªÉm ƒë·ªãnh ƒë·ªãnh k·ª≥ ph∆∞∆°ng ti·ªán giao th√¥ng l√† b·∫Øt bu·ªôc nh·∫±m ƒë·∫£m b·∫£o an to√†n giao th√¥ng, ki·ªÉm tra t√¨nh tr·∫°ng k·ªπ thu·∫≠t c·ªßa xe, ph√°t hi·ªán s·ªõm c√°c h∆∞ h·ªèng c√≥ th·ªÉ g√¢y tai n·∫°n, ƒë·ªìng th·ªùi ki·ªÉm so√°t kh√≠ th·∫£i b·∫£o v·ªá m√¥i tr∆∞·ªùng."

    Tr·∫£ v·ªÅ output d∆∞·ªõi d·∫°ng JSON v·ªõi qa_pairs.
    ƒê·ªäNH D·∫†NG OUTPUT B·∫ÆT BU·ªòC:
    Tr·∫£ v·ªÅ duy nh·∫•t m·ªôt kh·ªëi m√£ JSON h·ª£p l·ªá. Kh√¥ng th√™m b·∫•t k·ª≥ l·ªùi gi·∫£i th√≠ch hay vƒÉn b·∫£n n√†o kh√°c b√™n ngo√†i kh·ªëi JSON.

    ```json
    {{
    "qa_pairs": [
        {{
        "question": "N·ªôi dung c√¢u h·ªèi ƒë∆∞·ª£c t·∫°o ra ·ªü ƒë√¢y, b·∫Øt ƒë·∫ßu b·∫±ng '{starter}' v√† t·∫≠p trung v√†o ch·ªß ƒë·ªÅ '{focus}'.",
        "answer": "N·ªôi dung c√¢u tr·∫£ l·ªùi chi ti·∫øt, gi·∫£i th√≠ch r√µ r√†ng kh√°i ni·ªám, nguy√™n t·∫Øc v√† c√≥ th·ªÉ k√®m theo v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ minh h·ªça."
        }}
    ]
    }}
        """

    def create_multi_paragraph_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho lo·∫°i Multi-paragraph Reading - ƒë·ªçc hi·ªÉu nhi·ªÅu ƒëo·∫°n vƒÉn"""
        return f"""
    D∆∞·ªõi ƒë√¢y l√† c√°c ƒëi·ªÅu lu·∫≠t v·ªÅ ch·ªß ƒë·ªÅ "{topic}":

    {content}

    H√£y t·∫°o 1 c√¢u h·ªèi lo·∫°i MULTI-PARAGRAPH READING (ƒë·ªô kh√≥ {difficulty}) t·∫≠p trung v√†o {focus}.
    
    ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI MULTI-PARAGRAPH READING:
    - Y√™u c·∫ßu t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu quy ƒë·ªãnh kh√°c nhau
    - So s√°nh, ƒë·ªëi chi·∫øu c√°c ƒëi·ªÅu kho·∫£n
    - T√¨m m·ªëi li√™n h·ªá gi·ªØa c√°c quy ƒë·ªãnh
    - C√¢u tr·∫£ l·ªùi c·∫ßn k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn
    
    Y√äU C·∫¶U QUAN TR·ªåNG:
    1. C√ÇU H·ªéI PH·∫¢I C√ì ƒê·ªò BAO PH·ª¶ R·ªòNG: C·ªë g·∫Øng thi·∫øt k·∫ø c√¢u h·ªèi sao cho ng∆∞·ªùi tr·∫£ l·ªùi B·∫ÆT BU·ªòC ph·∫£i ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ T·∫§T C·∫¢ c√°c ƒëo·∫°n tr√≠ch ƒë√£ cho ƒë·ªÉ c√≥ th·ªÉ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß.
    2. C√ÇU H·ªéI N√äN B·∫ÆT ƒê·∫¶U T·ª™ M·ªòT T√åNH HU·ªêNG TH·ª∞C T·∫æ: H√£y t∆∞·ªüng t∆∞·ª£ng m·ªôt k·ªãch b·∫£n c·ª• th·ªÉ m√† m·ªôt ch·ªß th·ªÉ c√≥ th·ªÉ g·∫∑p ph·∫£i, sau ƒë√≥ ƒë·∫∑t c√¢u h·ªèi ph√°p l√Ω li√™n quan ƒë·∫øn k·ªãch b·∫£n ƒë√≥.
    3. C√¢u h·ªèi v√† ƒë√°p √°n ph·∫£i HO√ÄN TO√ÄN ƒê·ªòC L·∫¨P - c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m√† kh√¥ng c·∫ßn context b√™n ngo√†i
    4. TUY·ªÜT ƒê·ªêI KH√îNG d√πng "d·ª±a tr√™n ƒëi·ªÅu lu·∫≠t tr√™n", "theo c√°c quy ƒë·ªãnh tr√™n", "cƒÉn c·ª© v√†o ƒëi·ªÅu tr√™n"
    5. N·∫æU c·∫ßn tr√≠ch d·∫´n: ph·∫£i ghi ƒê·∫¶Y ƒê·ª¶ t√™n vƒÉn b·∫£n c·ª• th·ªÉ cho t·ª´ng ƒëi·ªÅu
    6. B·∫°n c√≥ th·ªÉ tham kh·∫£o b·∫Øt ƒë·∫ßu c√¢u h·ªèi b·∫±ng "{starter}..."
    7. ƒê√°p √°n ph·∫£i t·ªïng h·ª£p, so s√°nh r√µ r√†ng c√°c quy ƒë·ªãnh kh√°c nhau
    
    V√ç D·ª§ T·ªêT:
    Question: "N·∫øu m·ªôt ng∆∞·ªùi v·ª´a mu·ªën ƒëƒÉng k√Ω kinh doanh h·ªô c√° th·ªÉ, v·ª´a mu·ªën m·ªü m·ªôt doanh nghi·ªáp t∆∞ nh√¢n kh√°c, quy ƒë·ªãnh ph√°p lu·∫≠t v·ªÅ c√°c tr∆∞·ªùng h·ª£p n√†y c√≥ ƒëi·ªÉm g√¨ gi·ªëng v√† kh√°c nhau v·ªÅ quy·ªÅn v√† nghƒ©a v·ª•?"
    Answer: "ƒêi·ªÉm gi·ªëng nhau l√† c·∫£ hai ƒë·ªÅu do m·ªôt c√° nh√¢n l√†m ch·ªß v√† ch·ªãu tr√°ch nhi·ªám v√¥ h·∫°n. Tuy nhi√™n, ƒëi·ªÉm kh√°c bi·ªát l·ªõn l√† theo quy ƒë·ªãnh, m·ªôt c√° nh√¢n ch·ªâ ƒë∆∞·ª£c th√†nh l·∫≠p m·ªôt doanh nghi·ªáp t∆∞ nh√¢n, nh∆∞ng c√≥ th·ªÉ ƒë·ªìng th·ªùi l√† ch·ªß h·ªô kinh doanh. Do ƒë√≥, ng∆∞·ªùi n√†y c√≥ th·ªÉ ti·∫øp t·ª•c ƒëƒÉng k√Ω h·ªô kinh doanh nh∆∞ng kh√¥ng th·ªÉ m·ªü th√™m doanh nghi·ªáp t∆∞ nh√¢n th·ª© hai."
    
    Tr·∫£ v·ªÅ output d∆∞·ªõi d·∫°ng JSON v·ªõi qa_pairs.
        """

    def create_multihop_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho lo·∫°i Multihop - suy lu·∫≠n qua nhi·ªÅu b∆∞·ªõc"""
        return f"""
    D∆∞·ªõi ƒë√¢y l√† c√°c ƒëi·ªÅu lu·∫≠t v·ªÅ ch·ªß ƒë·ªÅ "{topic}":

    {content}

    H√£y t·∫°o 1 c√¢u h·ªèi lo·∫°i MULTIHOP (ƒë·ªô kh√≥ {difficulty}) t·∫≠p trung v√†o {focus}.
    
    ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI MULTIHOP:
    - Y√™u c·∫ßu suy lu·∫≠n logic qua nhi·ªÅu b∆∞·ªõc
    - K·∫øt h·ª£p nhi·ªÅu quy ƒë·ªãnh ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n
    - √Åp d·ª•ng quy t·∫Øc v√†o t√¨nh hu·ªëng ph·ª©c t·∫°p, th·ª±c t·∫ø
    - C√¢u tr·∫£ l·ªùi c·∫ßn chu·ªói suy lu·∫≠n c√≥ logic r√µ r√†ng
    
    Y√äU C·∫¶U QUAN TR·ªåNG:
    1. C√ÇU H·ªéI PH·∫¢I C√ì ƒê·ªò BAO PH·ª¶ R·ªòNG: C·ªë g·∫Øng thi·∫øt k·∫ø c√¢u h·ªèi sao cho ng∆∞·ªùi tr·∫£ l·ªùi B·∫ÆT BU·ªòC ph·∫£i ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ T·∫§T C·∫¢ c√°c ƒëo·∫°n tr√≠ch ƒë√£ cho ƒë·ªÉ c√≥ th·ªÉ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß.
    2. C√ÇU H·ªéI N√äN B·∫ÆT ƒê·∫¶U T·ª™ M·ªòT T√åNH HU·ªêNG TH·ª∞C T·∫æ: H√£y t∆∞·ªüng t∆∞·ª£ng m·ªôt k·ªãch b·∫£n c·ª• th·ªÉ m√† m·ªôt ch·ªß th·ªÉ c√≥ th·ªÉ g·∫∑p ph·∫£i, sau ƒë√≥ ƒë·∫∑t c√¢u h·ªèi ph√°p l√Ω li√™n quan ƒë·∫øn k·ªãch b·∫£n ƒë√≥.                                                        
    3. C√¢u h·ªèi v√† ƒë√°p √°n ph·∫£i HO√ÄN TO√ÄN ƒê·ªòC L·∫¨P - c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m√† kh√¥ng c·∫ßn context b√™n ngo√†i
    4. TUY·ªÜT ƒê·ªêI KH√îNG d√πng "d·ª±a tr√™n ƒëi·ªÅu lu·∫≠t tr√™n", "theo c√°c quy ƒë·ªãnh tr√™n", "cƒÉn c·ª© v√†o ƒëi·ªÅu tr√™n"
    5. N·∫æU c·∫ßn tr√≠ch d·∫´n: ph·∫£i ghi ƒê·∫¶Y ƒê·ª¶ t√™n vƒÉn b·∫£n v√† ƒëi·ªÅu c·ª• th·ªÉ
    6. B·∫°n c√≥ th·ªÉ tham kh·∫£o b·∫Øt ƒë·∫ßu c√¢u h·ªèi b·∫±ng "{starter}..."
    7. ƒê√°p √°n c·∫ßn c√≥ chu·ªói suy lu·∫≠n t·ª´ng b∆∞·ªõc: t√¨nh hu·ªëng ‚Üí quy ƒë·ªãnh √°p d·ª•ng ‚Üí k·∫øt lu·∫≠n
    
    V√ç D·ª§ T·ªêT:
    Question: "M·ªôt t√†i x·∫ø l√°i xe t·∫£i ch·ªü h√†ng qu√° t·∫£i 50% v√† kh√¥ng c√≥ b·∫±ng l√°i ph√π h·ª£p s·∫Ω b·ªã x·ª≠ l√Ω nh∆∞ th·∫ø n√†o?"
    Answer: "T√†i x·∫ø n√†y s·∫Ω b·ªã x·ª≠ ph·∫°t k√©p: ƒë·∫ßu ti√™n b·ªã ph·∫°t ti·ªÅn 12-15 tri·ªáu ƒë·ªìng v√† t∆∞·ªõc b·∫±ng l√°i 2-4 th√°ng do ch·ªü qu√° t·∫£i theo Ngh·ªã ƒë·ªãnh 100/2019, ƒë·ªìng th·ªùi b·ªã ph·∫°t 16-18 tri·ªáu v√† t∆∞·ªõc b·∫±ng l√°i 10-12 th√°ng do kh√¥ng c√≥ b·∫±ng l√°i ph√π h·ª£p. T·ªïng c·ªông c√≥ th·ªÉ b·ªã ph·∫°t ƒë·∫øn 33 tri·ªáu ƒë·ªìng v√† t∆∞·ªõc b·∫±ng l√°i t·ªëi ƒëa 16 th√°ng."
    
    V√ç D·ª§ X·∫§U (TR√ÅNH):
    Answer: "CƒÉn c·ª© v√†o c√°c ƒëi·ªÅu lu·∫≠t tr√™n, t√†i x·∫ø s·∫Ω b·ªã x·ª≠ ph·∫°t..."
    
    Tr·∫£ v·ªÅ output d∆∞·ªõi d·∫°ng JSON v·ªõi qa_pairs.
        """