from google import genai
from google.genai import types
import json
import random
import os
import re
from typing import List, Dict, Any
from pydantic import BaseModel
from similarity_checker import QuestionSimilarityChecker
from document_parsers import LegalDocumentParser

class SourceReference(BaseModel):
    """Tham chiáº¿u Ä‘áº¿n nguá»“n cá»§a thÃ´ng tin"""
    article_number: str  # Sá»‘ Ä‘iá»u (vÃ­ dá»¥: "60", "61")
    article_title: str   # TiÃªu Ä‘á» Ä‘iá»u (vÃ­ dá»¥: "Äiá»u 60. Äá»™ tuá»•i cá»§a ngÆ°á»i lÃ¡i xe")
    document_title: str  # TÃªn tÃ i liá»‡u (vÃ­ dá»¥: "Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™ 2008")

class LegalQA(BaseModel):
    """Cáº¥u trÃºc cÃ¢u há»i-Ä‘Ã¡p Ã¡n phÃ¡p lÃ½"""
    question: str
    answer: str

class LegalQAList(BaseModel):
    """Danh sÃ¡ch cÃ¢u há»i-Ä‘Ã¡p Ã¡n (khÃ´ng cáº§n sources vÃ¬ Ä‘Ã£ rule-based)"""
    qa_pairs: List[LegalQA]

class DataGenerator:
    """Class sinh dá»¯ liá»‡u huáº¥n luyá»‡n cho LegalSLM - Version gá»n gÃ ng"""
    
    def __init__(self, api_key: str = None, similarity_threshold: float = 0.75):
        # Set API key
        if api_key:
            os.environ['GEMINI_API_KEY'] = api_key
        elif not os.environ.get('GEMINI_API_KEY'):
            google_key = os.environ.get('GOOGLE_API_KEY')
            if google_key:
                os.environ['GEMINI_API_KEY'] = google_key
        
        self.client = genai.Client()
        self.model = "gemini-2.5-flash"
        
        # Khá»Ÿi táº¡o similarity checker
        self.similarity_checker = QuestionSimilarityChecker(similarity_threshold=similarity_threshold)
        print(f"ðŸ” Initialized similarity checker with threshold {similarity_threshold}")
    
    def get_rule_based_difficulty(self, data_type: str, num_sources: int) -> str:
        """Rule-based difficulty thay vÃ¬ yÃªu cáº§u LLM táº¡o ra"""
        if data_type == 'word_matching':
            return 'easy'
        elif data_type == 'concept_understanding':
            return 'easy' if num_sources == 1 else 'medium'
        elif data_type == 'multi_paragraph_reading':
            return 'medium' if num_sources <= 3 else 'hard'
        elif data_type == 'multi_hop_reasoning':
            return 'hard'
        else:
            return 'medium'

    def update_similarity_corpus(self, existing_questions_data: List[Dict[str, Any]]):
        """Cáº­p nháº­t corpus cho similarity checker vá»›i dá»¯ liá»‡u hiá»‡n cÃ³"""
        self.similarity_checker.update_corpus(existing_questions_data)
    
    def filter_duplicate_questions(self, new_samples: List[Dict[str, Any]], verbose: bool = True) -> List[Dict[str, Any]]:
        """Lá»c bá» cÃ¡c cÃ¢u há»i trÃ¹ng láº·p tá»« danh sÃ¡ch samples má»›i"""
        if not new_samples:
            return []
        
        questions = [sample.get('question', '') for sample in new_samples]
        similarity_results = self.similarity_checker.check_batch_similarity(questions)
        
        filtered_samples = []
        duplicates_found = 0
        
        for i, (sample, result) in enumerate(zip(new_samples, similarity_results)):
            if not result['is_duplicate']:
                filtered_samples.append(sample)
            else:
                duplicates_found += 1
                if verbose:
                    print(f"ðŸš« Filtered duplicate question {i+1}:")
                    print(f"   Question: {result['question'][:80]}...")
                    print(f"   Max similarity: {result['max_similarity']:.3f}")
        
        if verbose and duplicates_found > 0:
            print(f"ðŸ” Filtered {duplicates_found}/{len(new_samples)} duplicate questions")
        
        return filtered_samples

    def get_articles_from_parsed_structure(self, document) -> List[Dict]:
        """Láº¥y articles tá»« parsed structure hoáº·c fallback"""
        # Kiá»ƒm tra parsed structure
        if hasattr(document, 'parsed_structure') and document.parsed_structure:
            try:
                parsed_data = json.loads(document.parsed_structure)
                parser = LegalDocumentParser()
                articles = parser.get_all_articles(parsed_data)
                
                # Convert to format cho data generator
                units = []
                for article in articles:
                    units.append({
                        "id": f"article_{article['number']}",
                        "title": f"Äiá»u {article['number']}. {article['title']}",
                        "content": article['content'],
                        "document_title": document.title,
                        "metadata": {
                            "article_number": article['number'],
                            "source_document": document.title,
                            "unit_type": "article",
                            "length": article['content_length']
                        }
                    })
                
                print(f"âœ… Using parsed structure: {len(units)} articles from {document.title}")
                return units
                
            except Exception as e:
                print(f"âš ï¸ Failed to use parsed structure: {str(e)}, fallback to simple parsing")
        
        # Fallback - simple article extraction
        return self.split_law_by_article(document.content, document.title)

    def split_law_by_article(self, text: str, document_title: str = "") -> List[Dict]:
        """TÃ¡ch vÄƒn báº£n luáº­t thÃ nh cÃ¡c Ä‘iá»u"""
        units = []
        split_pattern = r'(?m)(?=^\s*Äiá»u \d+\.)'
        chunks = re.split(split_pattern, text.strip())
        
        for chunk in chunks:
            chunk = chunk.strip()
            lines = chunk.split('\n')
            dieu_line = None
            for line in lines:
                if re.match(r'^\s*Äiá»u \d+\.', line):
                    dieu_line = line.strip()
                    break
            
            if dieu_line:
                match = re.search(r'Äiá»u (\d+)', dieu_line)
                if match:
                    article_number = match.group(1)
                    units.append({
                        "id": f"article_{article_number}",
                        "title": dieu_line,
                        "content": chunk,
                        "document_title": document_title,
                        "metadata": {
                            "article_number": article_number,
                            "source_document": document_title,
                            "unit_type": "article",
                            "length": len(chunk)
                        }
                    })
                
        return units

    def monte_carlo_sample_articles(self, all_articles: List[Dict], sample_size: int) -> List[Dict]:
        """Monte Carlo sampling Ä‘Æ¡n giáº£n vá»›i weights dá»±a trÃªn content length vÃ  position"""
        if not all_articles or sample_size <= 0:
            return []
            
        if sample_size >= len(all_articles):
            articles_copy = all_articles.copy()
            random.shuffle(articles_copy)
            return articles_copy
        
        # TÃ­nh weights Ä‘Æ¡n giáº£n
        weights = []
        for article in all_articles:
            # Base weight tá»« content length
            content_length = article.get('metadata', {}).get('length', len(article.get('content', '')))
            length_weight = min(content_length / 800, 2.0)
            
            # Position weight (strategic articles)
            article_num = article.get('metadata', {}).get('article_number')
            position_weight = 1.0
            if article_num:
                try:
                    num = int(article_num)
                    if num <= 5 or num % 20 == 0:
                        position_weight = 1.5
                    elif num <= 20:
                        position_weight = 1.2
                except:
                    pass
            
            # Random factor cho diversity
            random_factor = random.uniform(0.7, 1.3)
            final_weight = length_weight * position_weight * random_factor
            weights.append(max(final_weight, 0.1))
        
        # Monte Carlo sampling
        selected = []
        available_indices = list(range(len(all_articles)))
        available_weights = weights.copy()
        
        for _ in range(sample_size):
            if not available_indices:
                break
            
            total_weight = sum(available_weights)
            if total_weight == 0:
                chosen_idx = random.randint(0, len(available_indices) - 1)
            else:
                rand_val = random.uniform(0, total_weight)
                cumsum = 0
                chosen_idx = len(available_weights) - 1
                
                for i, weight in enumerate(available_weights):
                    cumsum += weight
                    if rand_val <= cumsum:
                        chosen_idx = i
                        break
            
            # Add selected article
            selected.append(all_articles[available_indices[chosen_idx]])
            
            # Remove from available
            available_indices.pop(chosen_idx)
            available_weights.pop(chosen_idx)
        
        random.shuffle(selected)
        print(f"ðŸŽ² Monte Carlo sampling: chá»n {len(selected)}/{len(all_articles)} articles")
        return selected

    def generate_from_multiple_documents(self, documents, topic_name, data_type, num_samples):
        """Sinh dá»¯ liá»‡u tá»« nhiá»u documents - main method"""
        if not documents:
            return []

        print(f"ðŸ” PhÃ¢n tÃ­ch {len(documents)} documents...")

        # Láº¥y articles tá»« parsed structure
        all_articles = []
        for doc in documents:
            articles = self.get_articles_from_parsed_structure(doc)
            all_articles.extend(articles)
            print(f"  ðŸ“‹ {doc.title}: {len(articles)} Ä‘iá»u")

        print(f"ðŸ“Š Tá»•ng cá»™ng: {len(all_articles)} Ä‘iá»u tá»« {len(documents)} tÃ i liá»‡u")

        # Monte Carlo sampling
        max_articles = min(len(all_articles), max(num_samples // 2, 10))
        selected_articles = self.monte_carlo_sample_articles(all_articles, max_articles)
        print(f"  ðŸŽ¯ ÄÃ£ chá»n {len(selected_articles)} articles")

        # Sinh dá»¯ liá»‡u
        all_samples = self.generate_samples_from_articles(selected_articles, topic_name, data_type, num_samples)

        # Lá»c trÃ¹ng láº·p
        print(f"ðŸ” Kiá»ƒm tra tÆ°Æ¡ng Ä‘á»“ng cho {len(all_samples)} samples...")
        filtered_samples = self.filter_duplicate_questions(all_samples, verbose=True)

        print(f"âœ… HoÃ n thÃ nh: {len(filtered_samples)} samples (Ä‘Ã£ lá»c {len(all_samples) - len(filtered_samples)} trÃ¹ng láº·p)")
        return filtered_samples[:num_samples]

    def generate_samples_from_articles(self, articles, topic, data_type, num_samples):
        """Sinh dá»¯ liá»‡u Ä‘Æ¡n giáº£n vá»›i sources chung cho táº¥t cáº£ cÃ¢u há»i"""
        if not articles:
            return []
        
        # XÃ¡c Ä‘á»‹nh sá»‘ sources theo yÃªu cáº§u
        num_sources_map = {
            'word_matching': min(1, len(articles)),
            'concept_understanding': min(1, len(articles)),
            'multi_paragraph_reading': min(2, len(articles)),
            'multi_hop_reasoning': min(3, len(articles))
        }
        num_sources = num_sources_map.get(data_type, min(3, len(articles)))
        
        # Táº¡o cÃ¢u há»i - má»—i iteration tá»± Monte Carlo chá»n articles
        all_samples = []
        for i in range(num_samples):
            # Monte Carlo sampling cho iteration nÃ y
            selected_articles = self.monte_carlo_sample_articles(articles, num_sources)
            
            # Táº¡o sources vÃ  content cho iteration nÃ y
            iteration_sources = []
            combined_content = []
            
            for article in selected_articles:
                source_ref = SourceReference(
                    article_number=str(article['metadata']['article_number']) if article['metadata']['article_number'] else "unknown",
                    article_title=article['title'],
                    document_title=article['document_title']
                )
                iteration_sources.append(source_ref)
                article_path = article.get('path', article['title'])
                combined_content.append(f"--- {article['title']} ({article_path}) ---\n{article['content']}")

            combined_text = "\n\n".join(combined_content)
            
            # Rule-based difficulty cho iteration nÃ y
            difficulty = self.get_rule_based_difficulty(data_type, len(selected_articles))
            
            # Táº¡o prompt cho iteration nÃ y
            prompt = self.create_diverse_prompt(combined_text, topic, data_type, difficulty, i)
            
            try:
                temperature = random.uniform(0.6, 0.9)
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=temperature,
                        top_p=random.uniform(0.85, 0.95),
                        max_output_tokens=3000,
                        response_mime_type="application/json",
                        response_schema=LegalQAList,
                        seed=random.randint(1, 1000000)
                    )
                )
                
                structured_data: LegalQAList = response.parsed
                
                # Convert vá»›i sources chung vÃ  rule-based difficulty
                for qa_pair in structured_data.qa_pairs:
                    sample = {
                        'question': qa_pair.question,
                        'answer': qa_pair.answer,
                        'difficulty': difficulty,  
                        'sources': [
                            {
                                'article_number': src.article_number,
                                'article_title': src.article_title,
                                'document_title': src.document_title
                            } for src in iteration_sources  # Sources cho iteration nÃ y
                        ],
                        'metadata': {
                            'generation_method': 'per_iteration_monte_carlo',
                            'num_sources': len(selected_articles),
                            'temperature': temperature
                        }
                    }
                    all_samples.append(sample)
                    
            except Exception as e:
                print(f"âŒ Generation failed for sample {i+1}: {e}")
                continue
        
        return all_samples

    def create_diverse_prompt(self, content, topic, data_type, difficulty, iteration):
        """HÃ m gá»‘c táº¡o prompt Ä‘a dáº¡ng - sá»­ dá»¥ng lÃ m base cho cÃ¡c loáº¡i cÃ¢u há»i"""
        # Cáº¥u trÃºc cÃ¢u há»i Ä‘a dáº¡ng
        question_starters = [
            "Khi nÃ o", "Trong trÆ°á»ng há»£p nÃ o", "Ai cÃ³ trÃ¡ch nhiá»‡m",
            "Viá»‡c...Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° tháº¿ nÃ o", "Äiá»u kiá»‡n...lÃ  gÃ¬",
            "Má»©c pháº¡t...lÃ  bao nhiÃªu", "Quy trÃ¬nh...diá»…n ra ra sao",
            "Táº¡i sao", "VÃ¬ sao", "LÃ m cÃ¡ch nÃ o", "Báº±ng phÆ°Æ¡ng thá»©c nÃ o",
            "CÃ³ Ä‘Æ°á»£c phÃ©p", "CÃ³ báº¯t buá»™c", "CÃ³ cáº§n thiáº¿t",
            "Thá»§ tá»¥c...nhÆ° tháº¿ nÃ o", "HÃ¬nh thá»©c...lÃ  gÃ¬", "Pháº¡m vi...ra sao"
        ]
        
        focus_areas = [
            "quy Ä‘á»‹nh thá»±c táº¿ vÃ  á»©ng dá»¥ng cá»¥ thá»ƒ",
            "trÆ°á»ng há»£p ngoáº¡i lá»‡ vÃ  Ä‘iá»u kiá»‡n Ä‘áº·c biá»‡t", 
            "nghÄ©a vá»¥ vÃ  quyá»n háº¡n cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng",
            "má»©c pháº¡t vÃ  háº­u quáº£ phÃ¡p lÃ½",
            "quy trÃ¬nh thá»§ tá»¥c phÃ¡p lÃ½ chi tiáº¿t",
            "Ä‘á»‹nh nghÄ©a thuáº­t ngá»¯ chuyÃªn mÃ´n",
            "tháº©m quyá»n vÃ  trÃ¡ch nhiá»‡m quáº£n lÃ½"
        ]
        
        # Random selection vá»›i seed tá»« iteration Ä‘á»ƒ táº¡o diversity
        random.seed(hash(f"{data_type}_{iteration}_{topic}") % 10000)
        starter = random.choice(question_starters)
        focus = random.choice(focus_areas)
        
        # Reset seed
        random.seed()
        
        # Gá»i hÃ m con tÆ°Æ¡ng á»©ng vá»›i data_type
        if data_type == "word_matching":
            return self.create_word_matching_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "concept_understanding":
            return self.create_concept_understanding_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "multi_paragraph_reading":
            return self.create_multi_paragraph_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "multihop":
            return self.create_multihop_prompt(content, topic, starter, focus, difficulty)
        else:
            return self.create_concept_understanding_prompt(content, topic, starter, focus, difficulty)

    def create_word_matching_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Word Matching - tÃ¬m tá»« khÃ³a, thuáº­t ngá»¯ cá»¥ thá»ƒ trong vÄƒn báº£n"""
        return f"""
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":

{content}

HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i WORD MATCHING (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.

Äáº¶C ÄIá»‚M CÃ‚U Há»ŽI WORD MATCHING:
- YÃªu cáº§u tÃ¬m tá»« khÃ³a, thuáº­t ngá»¯ cá»¥ thá»ƒ trong vÄƒn báº£n
- Há»i vá» Ä‘á»‹nh nghÄ©a chÃ­nh xÃ¡c cá»§a cÃ¡c khÃ¡i niá»‡m phÃ¡p lÃ½
- CÃ¢u tráº£ lá»i lÃ  tá»«/cá»¥m tá»« xuáº¥t hiá»‡n trá»±c tiáº¿p trong vÄƒn báº£n
- Táº­p trung vÃ o thuáº­t ngá»¯ chuyÃªn mÃ´n, sá»‘ liá»‡u cá»¥ thá»ƒ

YÃŠU Cáº¦U QUAN TRá»ŒNG:
1. Háº¡n cháº¿ dÃ¹ng "Theo Äiá»u X cá»§a Luáº­t..."
2. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..." hoáº·c cáº¥u trÃºc tÆ°Æ¡ng tá»±
3. CÃ¢u há»i pháº£i Ä‘á»™c láº­p, khÃ´ng nháº¯c Ä‘áº¿n tÃªn Ä‘iá»u luáº­t cá»¥ thá»ƒ
4. ÄÃ¡p Ã¡n pháº£i DIá»„N GIáº¢I Äáº¦Y Äá»¦ ná»™i dung tráº£ lá»i cho cÃ¢u há»i, khÃ´ng chá»‰ nÃªu con sá»‘ hoáº·c tá»«, cá»¥m tá»« ngáº¯n gá»n
5. TrÃ¡nh Ä‘Ã¡p Ã¡n cá»¥t lá»§n nhÆ°: "11%", "Thá»‘ng Ä‘á»‘c NgÃ¢n hÃ ng NhÃ  nÆ°á»›c"

VÃ Dá»¤ CÃ‚U Há»ŽI WORD MATCHING:
- "Äá»™ tuá»•i tá»‘i thiá»ƒu Ä‘á»ƒ Ä‘Æ°á»£c cáº¥p báº±ng lÃ¡i xe Ã´ tÃ´ lÃ ?"
- "Thuáº­t ngá»¯ nÃ o Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ chá»‰ phÆ°Æ¡ng tiá»‡n khÃ´ng cÃ³ Ä‘á»™ng cÆ¡?"
- "Má»©c pháº¡t tá»‘i Ä‘a cho vi pháº¡m tá»‘c Ä‘á»™ lÃ  bao nhiÃªu?"

Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
        """

    def create_concept_understanding_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Concept Understanding - hiá»ƒu khÃ¡i niá»‡m, nguyÃªn táº¯c"""
        return f"""
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":

{content}

HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i CONCEPT UNDERSTANDING (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.

Äáº¶C ÄIá»‚M CÃ‚U Há»ŽI CONCEPT UNDERSTANDING:
- Kiá»ƒm tra hiá»ƒu biáº¿t vá» khÃ¡i niá»‡m, nguyÃªn táº¯c phÃ¡p lÃ½
- YÃªu cáº§u giáº£i thÃ­ch Ã½ nghÄ©a, má»¥c Ä‘Ã­ch cá»§a quy Ä‘á»‹nh
- CÃ¢u tráº£ lá»i cáº§n diá»…n giáº£i, khÃ´ng chá»‰ trÃ­ch dáº«n nguyÃªn vÄƒn
- Táº­p trung vÃ o viá»‡c hiá»ƒu "táº¡i sao" vÃ  "nhÆ° tháº¿ nÃ o"

YÃŠU Cáº¦U QUAN TRá»ŒNG:
1. Háº¡n cháº¿ dÃ¹ng "Theo Äiá»u X cá»§a Luáº­t..."
2. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..." hoáº·c cáº¥u trÃºc tÆ°Æ¡ng tá»±
3. CÃ¢u há»i pháº£i Ä‘á»™c láº­p, khÃ´ng nháº¯c Ä‘áº¿n tÃªn Ä‘iá»u luáº­t cá»¥ thá»ƒ
4. ÄÃ¡p Ã¡n cáº§n giáº£i thÃ­ch khÃ¡i niá»‡m, khÃ´ng chá»‰ liá»‡t kÃª

VÃ Dá»¤ CÃ‚U Há»ŽI CONCEPT UNDERSTANDING:
- "Táº¡i sao viá»‡c kiá»ƒm Ä‘á»‹nh Ä‘á»‹nh ká»³ phÆ°Æ¡ng tiá»‡n lÃ  báº¯t buá»™c?"
- "NguyÃªn táº¯c an toÃ n giao thÃ´ng Ä‘Æ°á»£c thá»ƒ hiá»‡n nhÆ° tháº¿ nÃ o?"
- "VÃ¬ sao cáº§n phÃ¢n loáº¡i báº±ng lÃ¡i xe theo tá»«ng háº¡ng?"

Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
        """

    def create_multi_paragraph_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Multi-paragraph Reading - Ä‘á»c hiá»ƒu nhiá»u Ä‘oáº¡n vÄƒn"""
        return f"""
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":S

{content}

HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i MULTI-PARAGRAPH READING (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.

Äáº¶C ÄIá»‚M CÃ‚U Há»ŽI MULTI-PARAGRAPH READING:
- YÃªu cáº§u Ä‘á»c vÃ  tá»•ng há»£p thÃ´ng tin tá»« nhiá»u Ä‘oáº¡n vÄƒn
- So sÃ¡nh, Ä‘á»‘i chiáº¿u cÃ¡c quy Ä‘á»‹nh khÃ¡c nhau
- TÃ¬m má»‘i liÃªn há»‡ giá»¯a cÃ¡c Ä‘iá»u khoáº£n
- CÃ¢u tráº£ lá»i cáº§n káº¿t há»£p thÃ´ng tin tá»« nhiá»u nguá»“n trong vÄƒn báº£n

YÃŠU Cáº¦U QUAN TRá»ŒNG:
1. Háº¡n cháº¿ dÃ¹ng "Theo Äiá»u X cá»§a Luáº­t..."
2. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..." hoáº·c cáº¥u trÃºc tÆ°Æ¡ng tá»±
3. CÃ¢u há»i pháº£i Ä‘á»™c láº­p, khÃ´ng nháº¯c Ä‘áº¿n tÃªn Ä‘iá»u luáº­t cá»¥ thá»ƒ
4. ÄÃ¡p Ã¡n pháº£i tá»•ng há»£p tá»« nhiá»u pháº§n khÃ¡c nhau cá»§a vÄƒn báº£n

VÃ Dá»¤ CÃ‚U Há»ŽI MULTI-PARAGRAPH READING:
- "So sÃ¡nh quy Ä‘á»‹nh vá» báº±ng lÃ¡i xe cho ngÆ°á»i dÃ¢n thÆ°á»ng vÃ  lá»±c lÆ°á»£ng vÅ© trang?"
- "CÃ¡c trÆ°á»ng há»£p Ä‘Æ°á»£c miá»…n phÃ­ Ä‘Äƒng kÃ½ xe bao gá»“m nhá»¯ng gÃ¬?"
- "Quy trÃ¬nh xá»­ pháº¡t vi pháº¡m giao thÃ´ng khÃ¡c nhau tháº¿ nÃ o giá»¯a cÃ¡c má»©c Ä‘á»™?"

Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
        """

    def create_multihop_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Multihop - suy luáº­n qua nhiá»u bÆ°á»›c"""
        return f"""
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":

{content}

HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i MULTIHOP (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.

Äáº¶C ÄIá»‚M CÃ‚U Há»ŽI MULTIHOP:
- YÃªu cáº§u suy luáº­n logic qua nhiá»u bÆ°á»›c
- Káº¿t há»£p nhiá»u quy Ä‘á»‹nh Ä‘á»ƒ Ä‘Æ°a ra káº¿t luáº­n
- Ãp dá»¥ng quy táº¯c vÃ o tÃ¬nh huá»‘ng phá»©c táº¡p, thá»±c táº¿
- CÃ¢u tráº£ lá»i cáº§n tráº£i qua chuá»—i suy luáº­n cÃ³ logic

YÃŠU Cáº¦U QUAN TRá»ŒNG:
1. Háº¡n cháº¿ dÃ¹ng "Theo Äiá»u X cá»§a Luáº­t..."
2. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..." hoáº·c cáº¥u trÃºc tÆ°Æ¡ng tá»±
3. CÃ¢u há»i pháº£i Ä‘á»™c láº­p, khÃ´ng nháº¯c Ä‘áº¿n tÃªn Ä‘iá»u luáº­t cá»¥ thá»ƒ
4. ÄÃ¡p Ã¡n cáº§n cÃ³ chuá»—i suy luáº­n rÃµ rÃ ng, khÃ´ng chá»‰ káº¿t luáº­n

VÃ Dá»¤ CÃ‚U Há»ŽI MULTIHOP:
- "Trong trÆ°á»ng há»£p nÃ o má»™t doanh nghiá»‡p váº­n táº£i cÃ³ thá»ƒ bá»‹ thu há»“i giáº¥y phÃ©p vÃ  pháº£i lÃ m gÃ¬ Ä‘á»ƒ Ä‘Æ°á»£c cáº¥p láº¡i?"
- "LÃ m cÃ¡ch nÃ o Ä‘á»ƒ xÃ¡c Ä‘á»‹nh má»©c pháº¡t cá»¥ thá»ƒ cho má»™t vi pháº¡m giao thÃ´ng cÃ³ nhiá»u tÃ¬nh tiáº¿t tÄƒng náº·ng?"
- "VÃ¬ sao viá»‡c váº­n chuyá»ƒn hÃ ng nguy hiá»ƒm cáº§n tuÃ¢n thá»§ Ä‘á»“ng thá»i nhiá»u quy Ä‘á»‹nh khÃ¡c nhau?"

Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
        """