from google import genai
from google.genai import types
import json
import random
import os
import re
from typing import List, Dict, Any
from pydantic import BaseModel
from similarity_checker import QuestionSimilarityChecker

class SourceReference(BaseModel):
    """Tham chi·∫øu ƒë·∫øn ngu·ªìn c·ªßa th√¥ng tin"""
    article_number: str  # S·ªë ƒëi·ªÅu (v√≠ d·ª•: "60", "61")
    article_title: str   # Ti√™u ƒë·ªÅ ƒëi·ªÅu (v√≠ d·ª•: "ƒêi·ªÅu 60. ƒê·ªô tu·ªïi c·ªßa ng∆∞·ªùi l√°i xe")
    document_title: str  # T√™n t√†i li·ªáu (v√≠ d·ª•: "Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô 2008")
    document_number: str # S·ªë hi·ªáu vƒÉn b·∫£n (v√≠ d·ª•: "23/2008/QH12")

class LegalQA(BaseModel):
    """C·∫•u tr√∫c c√¢u h·ªèi-ƒë√°p √°n ph√°p l√Ω"""
    question: str
    answer: str
    difficulty: str
    sources: List[SourceReference]  # Danh s√°ch c√°c ngu·ªìn tham chi·∫øu

class LegalQAResponse(BaseModel):
    """Response ch·ª©a danh s√°ch QA"""
    qa_pairs: List[LegalQA]

class DataGenerator:
    """Class sinh d·ªØ li·ªáu hu·∫•n luy·ªán cho LegalSLM theo ƒë·ªô kh√≥ reasoning"""
    
    def __init__(self, api_key: str = None, similarity_threshold: float = 0.75):
        # Set API key t·ª´ parameter ho·∫∑c environment variable
        if api_key:
            os.environ['GEMINI_API_KEY'] = api_key
        elif not os.environ.get('GEMINI_API_KEY'):
            # Fallback to GOOGLE_API_KEY
            google_key = os.environ.get('GOOGLE_API_KEY')
            if google_key:
                os.environ['GEMINI_API_KEY'] = google_key
        
        self.client = genai.Client()
        self.model = "gemini-2.0-flash-exp"
        
        # Kh·ªüi t·∫°o similarity checker
        self.similarity_checker = QuestionSimilarityChecker(similarity_threshold=similarity_threshold)
        print(f"üîç Initialized similarity checker with threshold {similarity_threshold}")
    
    def update_similarity_corpus(self, existing_questions_data: List[Dict[str, Any]]):
        """
        C·∫≠p nh·∫≠t corpus cho similarity checker v·ªõi d·ªØ li·ªáu hi·ªán c√≥
        
        Args:
            existing_questions_data: List c√°c dict ch·ª©a c√¢u h·ªèi t·ª´ database
        """
        self.similarity_checker.update_corpus(existing_questions_data)
    
    def filter_duplicate_questions(self, new_samples: List[Dict[str, Any]], verbose: bool = True) -> List[Dict[str, Any]]:
        """
        L·ªçc b·ªè c√°c c√¢u h·ªèi tr√πng l·∫∑p t·ª´ danh s√°ch samples m·ªõi
        
        Args:
            new_samples: List c√°c samples m·ªõi ƒë∆∞·ª£c generate
            verbose: In th√¥ng tin chi ti·∫øt
            
        Returns:
            List[Dict]: Danh s√°ch samples sau khi l·ªçc
        """
        if not new_samples:
            return []
        
        questions = [sample.get('question', '') for sample in new_samples]
        similarity_results = self.similarity_checker.check_batch_similarity(questions)
        
        filtered_samples = []
        duplicates_found = 0
        
        for i, (sample, result) in enumerate(zip(new_samples, similarity_results)):
            if not result['is_duplicate']:
                filtered_samples.append(sample)
            else:
                duplicates_found += 1
                if verbose:
                    print(f"üö´ Filtered duplicate question {i+1}:")
                    print(f"   Question: {result['question'][:80]}...")
                    print(f"   Max similarity: {result['max_similarity']:.3f}")
                    if result['similar_questions']:
                        best_match = result['similar_questions'][0]
                        print(f"   Similar to: {best_match['question'][:60]}...")
        
        if verbose and duplicates_found > 0:
            print(f"üîç Filtered {duplicates_found}/{len(new_samples)} duplicate questions")
        elif verbose:
            print(f"‚úÖ No duplicates found in {len(new_samples)} questions")
        
        return filtered_samples
    
    def split_law_by_article(self, text: str, document_title: str = "") -> List[Dict]:
        """
        T√°ch vƒÉn b·∫£n lu·∫≠t th√†nh c√°c ƒë∆°n v·ªã, m·ªói ƒë∆°n v·ªã l√† m·ªôt 'ƒêi·ªÅu'.
        H√†m s·∫Ω b·ªè qua c√°c ph·∫ßn kh√¥ng ph·∫£i l√† ƒêi·ªÅu (nh∆∞ ti√™u ƒë·ªÅ Ch∆∞∆°ng).
        
        Args:
            text (str): Chu·ªói vƒÉn b·∫£n lu·∫≠t c·∫ßn t√°ch.
            document_title (str): Ti√™u ƒë·ªÅ t√†i li·ªáu ƒë·ªÉ th√™m v√†o metadata
            
        Returns:
            list[dict]: M·ªôt danh s√°ch c√°c t·ª´ ƒëi·ªÉn, m·ªói t·ª´ ƒëi·ªÉn ƒë·∫°i di·ªán cho m·ªôt ƒêi·ªÅu.
        """
        units = []
        
        # Pattern ƒë·ªÉ t√¨m c√°c d√≤ng c√≥ "ƒêi·ªÅu X." (c√≥ th·ªÉ c√≥ spaces tr∆∞·ªõc)
        split_pattern = r'(?m)(?=^\s*ƒêi·ªÅu \d+\.)'
        
        # T√°ch vƒÉn b·∫£n th√†nh c√°c kh·ªëi (chunks)
        chunks = re.split(split_pattern, text.strip())
        
        # Duy·ªát qua c√°c kh·ªëi v√† ch·ªâ x·ª≠ l√Ω nh·ªØng kh·ªëi ch·ª©a "ƒêi·ªÅu"
        for chunk in chunks:
            chunk = chunk.strip()
            # T√¨m d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng "ƒêi·ªÅu" (c√≥ th·ªÉ c√≥ spaces)
            lines = chunk.split('\n')
            dieu_line = None
            for line in lines:
                if re.match(r'^\s*ƒêi·ªÅu \d+\.', line):
                    dieu_line = line.strip()
                    break
            
            if dieu_line:
                # Tr√≠ch xu·∫•t s·ªë hi·ªáu c·ªßa ƒêi·ªÅu ƒë·ªÉ l√†m ID
                match = re.search(r'ƒêi·ªÅu (\d+)', dieu_line)
                if match:
                    article_number = match.group(1)
                    unit_id = f"article_{article_number}"
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y s·ªë, t·∫°o ID d·ª± ph√≤ng
                    unit_id = f"unknown_article_{len(units) + 1}"
                    
                units.append({
                    "id": unit_id,
                    "title": dieu_line,
                    "content": chunk,
                    "document_title": document_title,
                    "metadata": {
                        "article_number": article_number if match else None,
                        "source_document": document_title,
                        "unit_type": "article",
                        "length": len(chunk)
                    }
                })
                
        return units
    
    def generate_from_multiple_documents(self, documents, topic_name, data_type, num_samples):
        """
        Sinh d·ªØ li·ªáu t·ª´ nhi·ªÅu documents b·∫±ng c√°ch t√°ch theo ƒêi·ªÅu v√† ph√¢n b·ªë c√¥ng b·∫±ng
        
        Args:
            documents: List of document objects (c√≥ .title v√† .content)
            topic_name: T√™n ch·ªß ƒë·ªÅ
            data_type: Lo·∫°i d·ªØ li·ªáu c·∫ßn sinh
            num_samples: S·ªë l∆∞·ª£ng samples c·∫ßn sinh
            
        Returns:
            List[Dict]: Danh s√°ch samples v·ªõi metadata ƒë·∫ßy ƒë·ªß
        """
        if not documents:
            return []
        
        print(f"üîç Ph√¢n t√≠ch {len(documents)} documents...")
        
        # B∆∞·ªõc 1: T√°ch t·∫•t c·∫£ documents th√†nh articles
        all_articles = []
        document_stats = {}
        
        for doc in documents:
            # T√°ch document th√†nh c√°c ƒêi·ªÅu
            articles = self.split_law_by_article(doc.content, doc.title)
            all_articles.extend(articles)
            
            document_stats[doc.title] = {
                'total_articles': len(articles),
                'total_length': len(doc.content)
            }
            
            print(f"  üìã {doc.title}: {len(articles)} ƒëi·ªÅu")
        
        print(f"üìä T·ªïng c·ªông: {len(all_articles)} ƒëi·ªÅu t·ª´ {len(documents)} t√†i li·ªáu")
        
        # B∆∞·ªõc 2: Ch·ªçn articles ƒë·ªÉ sinh d·ªØ li·ªáu (round-robin ho·∫∑c random)
        selected_articles = self._select_articles_for_generation(all_articles, num_samples)
        
        # B∆∞·ªõc 3: Sinh d·ªØ li·ªáu t·ª´ c√°c articles ƒë√£ ch·ªçn v·ªõi multi-source cho t·∫•t c·∫£ lo·∫°i
        all_samples = []
        
        # S·ª≠ d·ª•ng multi-source generation cho t·∫•t c·∫£ data types
        print(f"  ÔøΩ S·ª≠ d·ª•ng multi-source generation cho {data_type}")
        all_samples = self.generate_multi_source_data_from_articles(selected_articles, topic_name, data_type, num_samples)
        
        # B∆∞·ªõc 4: L·ªçc c√°c c√¢u h·ªèi tr√πng l·∫∑p
        print(f"üîç Ki·ªÉm tra t∆∞∆°ng ƒë·ªìng cho {len(all_samples)} samples...")
        filtered_samples = self.filter_duplicate_questions(all_samples, verbose=True)
        
        print(f"‚úÖ Ho√†n th√†nh: {len(filtered_samples)} samples (ƒë√£ l·ªçc {len(all_samples) - len(filtered_samples)} tr√πng l·∫∑p)")
        return filtered_samples[:num_samples]  # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° s·ªë l∆∞·ª£ng y√™u c·∫ßu
    
    def _select_articles_for_generation(self, all_articles, num_samples):
        """Ch·ªçn articles ƒë·ªÉ sinh d·ªØ li·ªáu"""
        if not all_articles:
            return []
        
        # L·ªçc articles c√≥ content ƒë·ªß d√†i (t·ªëi thi·ªÉu 100 chars)
        valid_articles = [art for art in all_articles if len(art['content']) >= 100]
        
        if not valid_articles:
            return all_articles[:num_samples]  # Fallback
        
        # S·∫Øp x·∫øp theo ƒë·ªô d√†i (∆∞u ti√™n articles d√†i h∆°n)
        valid_articles.sort(key=lambda x: len(x['content']), reverse=True)
        
        # Ch·ªçn s·ªë l∆∞·ª£ng articles ph√π h·ª£p
        max_articles = min(len(valid_articles), max(num_samples // 2, 5))
        
        return valid_articles[:max_articles]
    
    def _create_article_context(self, article):
        """T·∫°o context t·ª´ m·ªôt article"""
        return f"""--- {article['title']} (t·ª´ {article['document_title']}) ---
{article['content']}"""
    
    def _call_generation_method(self, context, topic, data_type, num_samples):
        """G·ªçi method generation ph√π h·ª£p"""
        try:
            if data_type == 'word_matching':
                return self.generate_word_matching_data(context, topic, num_samples)
            elif data_type == 'concept_understanding':
                return self.generate_concept_understanding_data(context, topic, num_samples)
            elif data_type == 'multi_paragraph_reading':
                return self.generate_multi_paragraph_reading_data(context, topic, num_samples)
            elif data_type == 'multi_hop_reasoning':
                return self.generate_multi_hop_reasoning_data(context, topic, num_samples)
            else:
                return []
        except Exception as e:
            print(f"‚ùå L·ªói khi sinh d·ªØ li·ªáu: {e}")
            return []
    
    def generate_multi_source_data_from_articles(self, articles, topic, data_type, num_samples):
        """Sinh d·ªØ li·ªáu t·ª´ nhi·ªÅu articles cho multi-paragraph v√† multi-hop reasoning"""
        
        # Group articles by document for better diversity
        articles_by_doc = {}
        for article in articles:
            doc_title = article['document_title']
            if doc_title not in articles_by_doc:
                articles_by_doc[doc_title] = []
            articles_by_doc[doc_title].append(article)
        
        all_samples = []
        
        for i in range(num_samples):
            # Ch·ªçn s·ªë ngu·ªìn ph√π h·ª£p v·ªõi t·ª´ng lo·∫°i c√¢u h·ªèi
            if data_type == 'word_matching':
                num_sources = 1  # Word matching c√≥ th·ªÉ t·ª´ 1 ngu·ªìn
            elif data_type == 'concept_understanding':
                num_sources = 2  # Concept understanding t·ª´ 2 ngu·ªìn
            elif data_type == 'multi_paragraph_reading':
                num_sources = 2  # Multi-paragraph t·ª´ 2-3 ngu·ªìn
            elif data_type == 'multi_hop_reasoning':
                num_sources = 3  # Multi-hop t·ª´ 3 ngu·ªìn
            else:
                num_sources = 2  # Default
                
            selected_articles = self._select_diverse_articles(articles_by_doc, num_sources)
            
            # Fallback n·∫øu kh√¥ng ƒë·ªß articles
            if len(selected_articles) < 1:
                if articles:
                    samples = self.generate_structured_data_from_article(articles[0], topic, data_type, 1)
                    all_samples.extend(samples)
                continue
            
            # T·∫°o source references t·ª´ c√°c articles ƒë√£ ch·ªçn
            source_refs = []
            combined_content = []
            
            for article in selected_articles:
                source_ref = SourceReference(
                    article_number=str(article['metadata']['article_number']) if article['metadata']['article_number'] else "unknown",
                    article_title=article['title'],
                    document_title=article['document_title'],
                    document_number=article.get('document_number', 'unknown')
                )
                source_refs.append(source_ref)
                combined_content.append(f"--- {article['title']} (t·ª´ {article['document_title']}) ---\n{article['content']}")
            
            # T·∫°o prompt v·ªõi multiple sources
            difficulty_description = {
                'word_matching': 'Word Matching - c√¢u h·ªèi ƒë∆°n gi·∫£n, c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng t√¨m ki·∫øm t·ª´ kh√≥a trong vƒÉn b·∫£n. N·∫øu c√≥ nhi·ªÅu ngu·ªìn, h·ªèi v·ªÅ th√¥ng tin c√≥ trong m·ªôt trong c√°c ngu·ªìn',
                'concept_understanding': 'Concept Understanding - c·∫ßn hi·ªÉu kh√°i ni·ªám ph√°p l√Ω c∆° b·∫£n. C√≥ th·ªÉ k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ kh√°i ni·ªám',
                'multi_paragraph_reading': 'Multi-Paragraph Reading - c·∫ßn ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ƒëo·∫°n/ƒëi·ªÅu kh√°c nhau',
                'multi_hop_reasoning': 'Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n v√† k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn'
            }
            
            combined_text = "\n\n".join(combined_content)
            
            # T·∫°o instruction ph√π h·ª£p v·ªõi s·ªë ngu·ªìn
            if len(selected_articles) == 1:
                instruction = f"H√£y t·∫°o 1 c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}."
            else:
                if data_type in ['word_matching', 'concept_understanding']:
                    instruction = f"H√£y t·∫°o 1 c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}. C√¢u h·ªèi c√≥ th·ªÉ d·ª±a tr√™n th√¥ng tin t·ª´ m·ªôt ho·∫∑c nhi·ªÅu ngu·ªìn ƒë·ªÉ tƒÉng ƒë·ªô phong ph√∫."
                else:
                    instruction = f"H√£y t·∫°o 1 c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}. C√¢u h·ªèi ph·∫£i Y√äU C·∫¶U TH√îNG TIN T·ª™ NHI·ªÄU ƒêI·ªÄU/T√ÄI LI·ªÜU ƒê√É CHO."
            
            prompt = f"""
            D·ª±a tr√™n c√°c ƒëi·ªÅu lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
            
            {combined_text}
            
            {instruction}
            
            Y√äU C√ÇU QUAN TR·ªåNG:
            1. C√¢u h·ªèi ph·∫£i ƒê·ªòC L·∫¨P, r√µ r√†ng, c√≥ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
            2. KH√îNG d√πng "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y", "ƒëi·ªÅu n√†y" - ph·∫£i n√≥i r√µ t√™n
            3. C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c t·ª´ n·ªôi dung c√°c ƒëi·ªÅu lu·∫≠t
            4. {self._get_source_requirement(data_type, len(selected_articles))}
            5. Ph·∫£i ghi r√µ ngu·ªìn tham chi·∫øu ƒë∆∞·ª£c s·ª≠ d·ª•ng
            
            Th√¥ng tin c√°c ngu·ªìn:
            {chr(10).join([f"- ƒêi·ªÅu {ref.article_number}: {ref.article_title} (t·ª´ {ref.document_title})" for ref in source_refs])}
            
            Tr·∫£ v·ªÅ theo format JSON v·ªõi sources ch·ª©a c√°c ngu·ªìn ƒë√£ s·ª≠ d·ª•ng.
            """
            
            try:
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=0.3,
                        max_output_tokens=4000,
                        response_mime_type="application/json",
                        response_schema=LegalQAResponse
                    )
                )
                
                # Parse structured response
                structured_data: LegalQAResponse = response.parsed
                
                # Convert to legacy format v·ªõi metadata t·ª´ nhi·ªÅu ngu·ªìn
                for qa_pair in structured_data.qa_pairs:
                    legacy_sample = {
                        'question': qa_pair.question,
                        'answer': qa_pair.answer,
                        'difficulty': qa_pair.difficulty,
                        'sources': [
                            {
                                'article_number': src.article_number,
                                'article_title': src.article_title,
                                'document_title': src.document_title,
                                'document_number': src.document_number
                            } for src in source_refs  # S·ª≠ d·ª•ng t·∫•t c·∫£ sources
                        ],
                        'metadata': {
                            'source_articles': [art['title'] for art in selected_articles],
                            'source_documents': list(set([art['document_title'] for art in selected_articles])),
                            'article_ids': [art['id'] for art in selected_articles],
                            'article_numbers': [art['metadata']['article_number'] for art in selected_articles],
                            'generation_method': 'multi_source_structured',
                            'num_sources': len(selected_articles)
                        }
                    }
                    all_samples.append(legacy_sample)
                    
            except Exception as e:
                print(f"‚ùå Multi-source generation failed for sample {i+1}: {e}")
                # Fallback to single article
                if selected_articles:
                    samples = self.generate_structured_data_from_article(selected_articles[0], topic, data_type, 1)
                    all_samples.extend(samples)
        
        return all_samples[:num_samples]
    
    def _select_diverse_articles(self, articles_by_doc, num_sources):
        """Ch·ªçn articles t·ª´ c√°c documents kh√°c nhau ƒë·ªÉ ƒë·∫£m b·∫£o diversity"""
        selected = []
        doc_names = list(articles_by_doc.keys())
        
        # ∆Øu ti√™n ch·ªçn t·ª´ c√°c documents kh√°c nhau
        for i in range(min(num_sources, len(doc_names))):
            doc_name = doc_names[i]
            if articles_by_doc[doc_name]:
                # Ch·ªçn article t·ªët nh·∫•t t·ª´ document n√†y (d√†i nh·∫•t)
                best_article = max(articles_by_doc[doc_name], key=lambda x: len(x['content']))
                selected.append(best_article)
        
        # N·∫øu c·∫ßn th√™m v√† c√≤n articles
        while len(selected) < num_sources:
            remaining_articles = []
            for doc_articles in articles_by_doc.values():
                for article in doc_articles:
                    if article not in selected:
                        remaining_articles.append(article)
            
            if not remaining_articles:
                break
                
            # Ch·ªçn article d√†i nh·∫•t c√≤n l·∫°i
            best_remaining = max(remaining_articles, key=lambda x: len(x['content']))
            selected.append(best_remaining)
        
        return selected
    
    def _get_source_requirement(self, data_type: str, num_sources: int) -> str:
        """T·∫°o y√™u c·∫ßu v·ªÅ ngu·ªìn ph√π h·ª£p v·ªõi t·ª´ng lo·∫°i c√¢u h·ªèi"""
        if num_sources == 1:
            return "C√¢u tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ ngu·ªìn ƒë√£ cho"
        
        if data_type in ['word_matching', 'concept_understanding']:
            return f"C√¢u tr·∫£ l·ªùi c√≥ th·ªÉ d·ª±a tr√™n th√¥ng tin t·ª´ m·ªôt ho·∫∑c nhi·ªÅu ngu·ªìn trong {num_sources} ngu·ªìn ƒë√£ cho"
        else:
            return f"C√¢u tr·∫£ l·ªùi ph·∫£i t·ªïng h·ª£p th√¥ng tin t·ª´ √≠t nh·∫•t 2 ngu·ªìn trong {num_sources} ngu·ªìn ƒë√£ cho"

    def generate_structured_data_from_article(self, article, topic, data_type, num_samples):
        """Sinh d·ªØ li·ªáu c√≥ c·∫•u tr√∫c t·ª´ m·ªôt article c·ª• th·ªÉ"""
        
        # T·∫°o source reference t·ª´ article
        source_ref = SourceReference(
            article_number=str(article['metadata']['article_number']) if article['metadata']['article_number'] else "unknown",
            article_title=article['title'],
            document_title=article['document_title'],
            document_number=article.get('document_number', 'unknown')
        )
        
        # T·∫°o prompt v·ªõi structured output
        difficulty_description = {
            'word_matching': 'Word Matching - c√¢u h·ªèi ƒë∆°n gi·∫£n nh·∫•t, ch·ªâ c·∫ßn t√¨m ki·∫øm t·ª´ kh√≥a/c·ª•m t·ª´ trong vƒÉn b·∫£n',
            'concept_understanding': 'Concept Understanding - c·∫ßn hi·ªÉu kh√°i ni·ªám ph√°p l√Ω c∆° b·∫£n',
            'multi_paragraph_reading': 'Multi-Paragraph Reading - c·∫ßn ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ƒëo·∫°n',
            'multi_hop_reasoning': 'Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n'
        }
        
        prompt = f"""
        D·ª±a tr√™n ƒëi·ªÅu lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {article['content']}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}.
        
        Y√äU C√ÇU QUAN TR·ªåNG:
        1. M·ªói c√¢u h·ªèi ph·∫£i ƒê·ªòC L·∫¨P, r√µ r√†ng, c√≥ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        2. KH√îNG d√πng "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y", "ƒëi·ªÅu n√†y" - ph·∫£i n√≥i r√µ t√™n
        3. C√¢u tr·∫£ l·ªùi ph·∫£i CH√çNH X√ÅC t·ª´ n·ªôi dung ƒëi·ªÅu lu·∫≠t
        4. Ph·∫£i ghi r√µ ngu·ªìn tham chi·∫øu ƒë·∫øn ƒëi·ªÅu v√† t√†i li·ªáu c·ª• th·ªÉ
        
        Th√¥ng tin ngu·ªìn:
        - ƒêi·ªÅu s·ªë: {source_ref.article_number}
        - Ti√™u ƒë·ªÅ ƒëi·ªÅu: {source_ref.article_title}
        - T√†i li·ªáu: {source_ref.document_title}
        
        Tr·∫£ v·ªÅ theo format JSON structure y√™u c·∫ßu.
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                    max_output_tokens=4000,
                    response_mime_type="application/json",
                    response_schema=LegalQAResponse
                )
            )
            
            # Parse structured response
            structured_data: LegalQAResponse = response.parsed
            
            # Convert to legacy format v·ªõi metadata ƒë·∫ßy ƒë·ªß
            legacy_samples = []
            for qa_pair in structured_data.qa_pairs:
                legacy_sample = {
                    'question': qa_pair.question,
                    'answer': qa_pair.answer,
                    'difficulty': qa_pair.difficulty,
                    'sources': [
                        {
                            'article_number': src.article_number,
                            'article_title': src.article_title,
                            'document_title': src.document_title,
                            'document_number': src.document_number
                        } for src in qa_pair.sources
                    ],
                    'metadata': {
                        'source_article': article['title'],
                        'source_document': article['document_title'],
                        'article_id': article['id'],
                        'article_number': article['metadata']['article_number'],
                        'generation_method': 'structured_article_based'
                    }
                }
                legacy_samples.append(legacy_sample)
            
            return legacy_samples
            
        except Exception as e:
            print(f"‚ùå Structured generation failed, fallback to legacy: {e}")
            # Fallback to legacy method
            context = self._create_article_context(article)
            return self._call_generation_method(context, topic, data_type, num_samples)
    
    def generate_word_matching_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Word Matching - ƒë∆°n gi·∫£n nh·∫•t, ch·ªâ c·∫ßn t√¨m t·ª´ kh√≥a trong vƒÉn b·∫£n"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Word Matching - ƒë√¢y l√† lo·∫°i c√¢u h·ªèi ƒë∆°n gi·∫£n nh·∫•t, ch·ªâ c·∫ßn t√¨m ki·∫øm t·ª´ kh√≥a/c·ª•m t·ª´ trong vƒÉn b·∫£n.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng c√°ch t√¨m ki·∫øm tr·ª±c ti·∫øp trong vƒÉn b·∫£n
        - Kh√¥ng c·∫ßn hi·ªÉu s√¢u v·ªÅ kh√°i ni·ªám ph√°p l√Ω
        - Th√¥ng tin c·∫ßn thi·∫øt n·∫±m r√µ r√†ng trong vƒÉn b·∫£n
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi r√µ r√†ng, ƒë·ªôc l·∫≠p, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "Tr·∫£ l·ªùi ch√≠nh x√°c t·ª´ vƒÉn b·∫£n",
                "difficulty": "word_matching"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,  # Th·∫•p h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c
                    max_output_tokens=4000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_word_matching_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Word Matching data: {e}")
            return self._generate_fallback_word_matching_data(topic, num_samples)
    
    def generate_concept_understanding_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Concept Understanding - c·∫ßn hi·ªÉu kh√°i ni·ªám ph√°p l√Ω"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Concept Understanding - y√™u c·∫ßu hi·ªÉu c√°c kh√°i ni·ªám ph√°p l√Ω ƒë·ªÉ tr·∫£ l·ªùi.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi y√™u c·∫ßu hi·ªÉu √Ω nghƒ©a c·ªßa c√°c thu·∫≠t ng·ªØ ph√°p l√Ω
        - C·∫ßn n·∫Øm ƒë∆∞·ª£c kh√°i ni·ªám ƒë·ªÉ √°p d·ª•ng v√†o t√¨nh hu·ªëng c·ª• th·ªÉ
        - Kh√¥ng ch·ªâ t√¨m t·ª´ kh√≥a m√† ph·∫£i hi·ªÉu nghƒ©a s√¢u h∆°n
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi r√µ r√†ng y√™u c·∫ßu hi·ªÉu kh√°i ni·ªám ph√°p l√Ω, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "Tr·∫£ l·ªùi d·ª±a tr√™n hi·ªÉu bi·∫øt v·ªÅ kh√°i ni·ªám",
                "difficulty": "concept_understanding"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.5,
                    max_output_tokens=4000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_concept_understanding_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Concept Understanding data: {e}")
            return self._generate_fallback_concept_understanding_data(topic, num_samples)
    
    def generate_multi_paragraph_reading_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Multi-Paragraph Reading - c·∫ßn ƒë·ªçc nhi·ªÅu ƒëo·∫°n ƒë·ªÉ t·∫≠p h·ª£p th√¥ng tin"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Multi-Paragraph Reading - y√™u c·∫ßu ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ƒëo·∫°n vƒÉn kh√°c nhau.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi kh√¥ng th·ªÉ tr·∫£ l·ªùi ch·ªâ b·∫±ng m·ªôt ƒëo·∫°n vƒÉn duy nh·∫•t
        - C·∫ßn t·∫≠p h·ª£p th√¥ng tin t·ª´ 2-3 ƒëo·∫°n vƒÉn kh√°c nhau
        - Ph·∫£i k·∫øt h·ª£p c√°c th√¥ng tin ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi r√µ r√†ng c·∫ßn ƒë·ªçc nhi·ªÅu ƒëo·∫°n vƒÉn, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "Tr·∫£ l·ªùi t·ªïng h·ª£p t·ª´ nhi·ªÅu ngu·ªìn",
                "difficulty": "multi_paragraph_reading"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.6,
                    max_output_tokens=4000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_multi_paragraph_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Multi-Paragraph Reading data: {e}")
            return self._generate_fallback_multi_paragraph_data(topic, num_samples)

    def generate_multi_hop_reasoning_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n logic"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, y√™u c·∫ßu nhi·ªÅu b∆∞·ªõc suy lu·∫≠n logic.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n logic ƒë·ªÉ tr·∫£ l·ªùi
        - Ph·∫£i k·∫øt h·ª£p hi·ªÉu kh√°i ni·ªám + ƒë·ªçc nhi·ªÅu ƒëo·∫°n + suy lu·∫≠n logic
        - Qu√° tr√¨nh reasoning ph·∫£i r√µ r√†ng v√† c√≥ th·ªÉ gi·∫£i th√≠ch ƒë∆∞·ª£c
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi ph·ª©c t·∫°p c·∫ßn suy lu·∫≠n nhi·ªÅu b∆∞·ªõc, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "K·∫øt lu·∫≠n cu·ªëi c√πng v·ªõi gi·∫£i th√≠ch qu√° tr√¨nh suy lu·∫≠n",
                "difficulty": "multi_hop_reasoning"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    max_output_tokens=5000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_multi_hop_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Multi-Hop Reasoning data: {e}")
            return self._generate_fallback_multi_hop_data(topic, num_samples)
    
    def _generate_fallback_word_matching_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Word Matching m·∫´u khi API l·ªói"""
        templates = [
            {
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, {topic} l√† g√¨?",
                "answer": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, {topic} ƒë∆∞·ª£c quy ƒë·ªãnh l√†...",
                "difficulty": "word_matching"
            },
            {
                "question": f"Ai c√≥ th·∫©m quy·ªÅn quy·∫øt ƒë·ªãnh v·ªÅ {topic} theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t?",
                "answer": f"Th·∫©m quy·ªÅn v·ªÅ {topic} thu·ªôc v·ªÅ c∆° quan...",
                "difficulty": "word_matching"
            }
        ]
        
        result = []
        for i in range(num_samples):
            template = templates[i % len(templates)]
            result.append({
                "question": template["question"],
                "answer": template["answer"] + f" (M·∫´u {i+1})",
                "difficulty": template["difficulty"]
            })
        
        return result
    
    def _generate_fallback_concept_understanding_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Concept Understanding m·∫´u khi API l·ªói"""
        result = []
        for i in range(num_samples):
            result.append({
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, trong tr∆∞·ªùng h·ª£p n√†o th√¨ h√†nh vi li√™n quan ƒë·∫øn {topic} ƒë∆∞·ª£c coi l√† vi ph·∫°m?",
                "answer": f"Theo quy ƒë·ªãnh c·ªßa Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, h√†nh vi vi ph·∫°m v·ªÅ {topic} bao g·ªìm c√°c tr∆∞·ªùng h·ª£p...",
                "difficulty": "concept_understanding"
            })
        
        return result
    
    def _generate_fallback_multi_paragraph_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Multi-Paragraph Reading m·∫´u khi API l·ªói"""
        result = []
        for i in range(num_samples):
            result.append({
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, quy tr√¨nh ho√†n ch·ªânh ƒë·ªÉ x·ª≠ l√Ω v·∫•n ƒë·ªÅ {topic} nh∆∞ th·∫ø n√†o?",
                "answer": f"Theo quy ƒë·ªãnh c·ªßa Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, quy tr√¨nh x·ª≠ l√Ω {topic} bao g·ªìm nhi·ªÅu giai ƒëo·∫°n...",
                "difficulty": "multi_paragraph_reading"
            })
        
        return result
    
    def _generate_fallback_multi_hop_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Multi-Hop Reasoning m·∫´u khi API l·ªói"""
        result = []
        for i in range(num_samples):
            result.append({
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, ph√¢n t√≠ch t√¨nh hu·ªëng ph·ª©c t·∫°p v·ªÅ {topic} v√† ƒë∆∞a ra gi·∫£i ph√°p ph√°p l√Ω ph√π h·ª£p",
                "answer": f"K·∫øt lu·∫≠n v·ªÅ t√¨nh hu·ªëng {topic}: D·ª±a tr√™n vi·ªác x√°c ƒë·ªãnh c√°c kh√°i ni·ªám ph√°p l√Ω li√™n quan, t√¨m hi·ªÉu quy ƒë·ªãnh t·ª´ nhi·ªÅu ƒëi·ªÅu lu·∫≠t kh√°c nhau, ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c√°c quy ƒë·ªãnh, v√† √°p d·ª•ng logic ph√°p l√Ω ƒë·ªÉ k·∫øt lu·∫≠n...",
                "difficulty": "multi_hop_reasoning"
            })
        
        return result
