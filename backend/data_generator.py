from google import genai
from google.genai import types
import json
import random
import os
import re
import time  # Add time import for rate limiting
import requests  # For cloud HuggingFace API calls
from typing import List, Dict, Any
from pydantic import BaseModel
from similarity_checker import QuestionSimilarityChecker
from document_parsers import LegalDocumentParser

# HuggingFace imports
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸ HuggingFace transformers not available. Install with: pip install transformers torch")

class SourceReference(BaseModel):
    """Tham chiáº¿u Ä‘áº¿n nguá»“n cá»§a thÃ´ng tin dá»±a trÃªn units"""
    unit_path: str       # ÄÆ°á»ng dáº«n unit (vÃ­ dá»¥: "Luáº­t Giao thÃ´ng > Äiá»u 60 > Khoáº£n 1")
    unit_id: str         # ID cá»§a unit (vÃ­ dá»¥: "unit_60_1_a")
    document_title: str  # TÃªn tÃ i liá»‡u (vÃ­ dá»¥: "Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™ 2008")

class LegalQA(BaseModel):
    """Cáº¥u trÃºc cÃ¢u há»i-Ä‘Ã¡p Ã¡n phÃ¡p lÃ½"""
    question: str
    answer: str

class LegalQAList(BaseModel):
    """Danh sÃ¡ch cÃ¢u há»i-Ä‘Ã¡p Ã¡n (khÃ´ng cáº§n sources vÃ¬ Ä‘Ã£ rule-based)"""
    qa_pairs: List[LegalQA]

class DataGenerator:
    """Class sinh dá»¯ liá»‡u huáº¥n luyá»‡n cho LegalSLM - Version gá»n gÃ ng"""
    
    def __init__(self, api_key: str = None, similarity_threshold: float = 0.75):
        # Set API key
        if api_key:
            os.environ['GEMINI_API_KEY'] = api_key
        elif not os.environ.get('GEMINI_API_KEY'):
            google_key = os.environ.get('GOOGLE_API_KEY')
            if google_key:
                os.environ['GEMINI_API_KEY'] = google_key
        
        self.client = genai.Client()
        self.model = "gemini-2.5-flash"
        
        # Khá»Ÿi táº¡o similarity checker
        self.similarity_checker = QuestionSimilarityChecker(similarity_threshold=similarity_threshold)
        print(f"ğŸ” Initialized similarity checker with threshold {similarity_threshold}")
        
        # HuggingFace model setup
        self.hf_model = None
        self.hf_tokenizer = None
        
        # Rate limiting for Gemini API (15 req/min = 4 seconds per request)
        self.last_api_call = 0
        self.min_interval = 4.0  # seconds between API calls
    
    def init_huggingface_model(self, model_name: str = "phamhoangf/qwen3-4b-generate-data"):
        """Initialize HuggingFace model"""
        if not HF_AVAILABLE:
            raise ImportError("HuggingFace transformers not available")
        
        try:
            print(f"ğŸ¤– Loading HuggingFace model: {model_name}")
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            self.hf_tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.hf_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            print(f"âœ… HuggingFace model loaded on {device}")
            
        except Exception as e:
            print(f"âŒ Failed to load HuggingFace model: {str(e)}")
            raise
    
    def generate_qa_with_gemini(self, prompt: str, temperature: float = 0.7) -> LegalQAList:
        """Sinh QA báº±ng Gemini API vá»›i rate limiting"""
        # Rate limiting: Ä‘áº£m báº£o 15 req/min (4 giÃ¢y/request)
        current_time = time.time()
        elapsed = current_time - self.last_api_call
        
        if elapsed < self.min_interval:
            sleep_time = self.min_interval - elapsed
            print(f"â³ Rate limiting: sleeping {sleep_time:.1f}s...")
            time.sleep(sleep_time)
        
        self.last_api_call = time.time()
        
        response = self.client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=temperature,
                top_p=random.uniform(0.85, 0.95),
                max_output_tokens=3000,
                response_mime_type="application/json",
                response_schema=LegalQAList,
                seed=random.randint(1, 1000000)
            )
        )
        return response.parsed
    
    def generate_qa_with_huggingface(self, prompt: str, temperature: float = 0.7) -> LegalQAList:
        """Sinh QA báº±ng HuggingFace model trÃªn cloud (Kaggle/ngrok)"""
        import requests
        
        # URL ngrok tá»« Kaggle notebook
        NGROK_URL = "https://evidently-cheerful-griffon.ngrok-free.app/generate"
        
        # Format messages nhÆ° trong test_ngrok.py
        messages = [{
            "role": "user",
            "content": prompt
        }]
        
        payload = {"messages": messages}
        
        try:
            print("ğŸŒ Äang gá»­i yÃªu cáº§u Ä‘áº¿n HuggingFace model trÃªn cloud...")
            response = requests.post(NGROK_URL, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            response_text = result.get('response', '')
            
            print(f"ğŸ“¥ Nháº­n Ä‘Æ°á»£c pháº£n há»“i tá»« cloud model")
            
            # Parse JSON response
            try:
                response_json = json.loads(response_text)
                return LegalQAList(**response_json)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ KhÃ´ng thá»ƒ parse JSON tá»« cloud model: {e}")
                print(f"Raw response: {response_text[:200]}...")
                # Fallback - táº¡o single QA náº¿u khÃ´ng parse Ä‘Æ°á»£c
                return LegalQAList(qa_pairs=[LegalQA(question="Sample question", answer="Sample answer")])
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ Lá»—i khi káº¿t ná»‘i Ä‘áº¿n cloud model: {e}")
            raise ValueError(f"Failed to connect to HuggingFace cloud model: {e}")
        except Exception as e:
            print(f"âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh vá»›i cloud model: {e}")
            raise ValueError(f"Error with HuggingFace cloud model: {e}")
    
    def generate_qa(self, prompt: str, llm_type: str = "gemini", temperature: float = 0.7) -> LegalQAList:
        """Sinh QA vá»›i LLM Ä‘Æ°á»£c chá»n"""
        if llm_type == "gemini":
            return self.generate_qa_with_gemini(prompt, temperature)
        elif llm_type == "huggingface":
            return self.generate_qa_with_huggingface(prompt, temperature)
        else:
            raise ValueError(f"Unsupported LLM type: {llm_type}")
    
    
    def get_rule_based_difficulty(self, data_type: str, num_sources: int) -> str:
        """Rule-based difficulty thay vÃ¬ yÃªu cáº§u LLM táº¡o ra"""
        if data_type == 'word_matching':
            return 'easy'
        elif data_type == 'concept_understanding':
            return 'easy' if num_sources == 1 else 'medium'
        elif data_type == 'multi_paragraph_reading':
            return 'medium' if num_sources <= 3 else 'hard'
        elif data_type == 'multi_hop_reasoning':
            return 'hard'
        else:
            return 'medium'

    def update_similarity_corpus(self, existing_questions_data: List[Dict[str, Any]]):
        """Cáº­p nháº­t corpus cho similarity checker vá»›i dá»¯ liá»‡u hiá»‡n cÃ³"""
        self.similarity_checker.update_corpus(existing_questions_data)
    
    def filter_duplicate_questions(self, new_samples: List[Dict[str, Any]], verbose: bool = True) -> List[Dict[str, Any]]:
        """Lá»c bá» cÃ¡c cÃ¢u há»i trÃ¹ng láº·p tá»« danh sÃ¡ch samples má»›i"""
        if not new_samples:
            return []
        
        questions = [sample.get('question', '') for sample in new_samples]
        similarity_results = self.similarity_checker.check_batch_similarity(questions)
        
        filtered_samples = []
        duplicates_found = 0
        
        for i, (sample, result) in enumerate(zip(new_samples, similarity_results)):
            if not result['is_duplicate']:
                filtered_samples.append(sample)
            else:
                duplicates_found += 1
                if verbose:
                    print(f"ğŸš« Filtered duplicate question {i+1}:")
                    print(f"   Question: {result['question'][:80]}...")
                    print(f"   Max similarity: {result['max_similarity']:.3f}")
        
        if verbose and duplicates_found > 0:
            print(f"ğŸ” Filtered {duplicates_found}/{len(new_samples)} duplicate questions")
        
        return filtered_samples

    def get_units_from_parsed_structure(self, document) -> List[Dict]:
        """Láº¥y units tá»« document sá»­ dá»¥ng LegalDocumentParser"""
        try:
            # Sá»­ dá»¥ng parser Ä‘á»ƒ láº¥y units trá»±c tiáº¿p
            parser = LegalDocumentParser()
            
            # Náº¿u cÃ³ parsed structure thÃ¬ sá»­ dá»¥ng
            if hasattr(document, 'parsed_structure') and document.parsed_structure:
                try:
                    parsed_data = json.loads(document.parsed_structure)
                    print(f"ğŸ” Using existing parsed structure for {document.title}")
                    print(f"   Articles found: {len(parsed_data.get('articles', []))}")
                    units = parser.get_all_units(parsed_data)
                    print(f"   Units extracted: {len(units)}")
                except Exception as e:
                    print(f"âš ï¸ Failed to use parsed structure: {str(e)}, re-parsing...")
                    # Fallback: parse láº¡i tá»« content
                    parsed_data = parser.parse_document(document.title, document.content)
                    print(f"   Re-parsed articles: {len(parsed_data.get('articles', []))}")
                    units = parser.get_all_units(parsed_data)
                    print(f"   Units from re-parse: {len(units)}")
            else:
                # Parse tá»« Ä‘áº§u náº¿u chÆ°a cÃ³ parsed structure
                print(f"ğŸ”„ Parsing document from scratch: {document.title}")
                parsed_data = parser.parse_document(document.title, document.content)
                print(f"   Articles parsed: {len(parsed_data.get('articles', []))}")
                units = parser.get_all_units(parsed_data)
                print(f"   Units generated: {len(units)}")
            
            # Convert to format cho data generator
            converted_units = []
            for unit in units:
                converted_units.append({
                    "id": f"unit_{unit['source_article']}_{unit['source_khoan']}_{unit['source_diem']}",
                    "title": unit['path'].split(' > ')[-1] if ' > ' in unit['path'] else f"Unit {unit['source_article']}",
                    "content": unit['content'],
                    "document_title": document.title,
                    "path": unit['path'],
                    "metadata": {
                        "source_article": unit['source_article'],
                        "source_khoan": unit['source_khoan'], 
                        "source_diem": unit['source_diem'],
                        "source_document": document.title,
                        "unit_type": "content_unit",
                        "length": unit.get('content_length', len(unit['content']))
                    }
                })
            
            print(f"âœ… Extracted {len(converted_units)} units from {document.title}")
            return converted_units
            
        except Exception as e:
            print(f"âŒ Failed to extract units from {document.title}: {str(e)}")
            return []

    def monte_carlo_sample_articles(self, all_articles: List[Dict], sample_size: int, iteration: int = 0) -> List[Dict]:
        """Monte Carlo sampling vá»›i Ä‘á»™ ngáº«u nhiÃªn cao hÆ¡n"""
        if not all_articles or sample_size <= 0:
            return []
            
        if sample_size >= len(all_articles):
            articles_copy = all_articles.copy()
            # ThÃªm iteration seed Ä‘á»ƒ má»—i láº§n gá»i khÃ¡c nhau
            random.seed(hash(f"shuffle_{iteration}_{time.time()}") % 100000)
            random.shuffle(articles_copy)
            random.seed()  # Reset seed
            return articles_copy
        
        # Táº¡o random seed khÃ¡c nhau cho má»—i iteration
        random.seed(hash(f"sampling_{iteration}_{time.time()}_{random.randint(1, 10000)}") % 100000)
        
        # TÃ­nh weights vá»›i nhiá»u yáº¿u tá»‘ ngáº«u nhiÃªn hÆ¡n
        weights = []
        for i, article in enumerate(all_articles):
            # Base weight tá»« content length vá»›i random factor lá»›n hÆ¡n
            content_length = article.get('metadata', {}).get('length', len(article.get('content', '')))
            length_weight = min(content_length / 800, 2.0)
            
            # Position weight vá»›i thÃªm random
            position_weight = random.uniform(0.8, 1.5)  # HoÃ n toÃ n random thay vÃ¬ dá»±a vÃ o article number
            
            # Iteration-based randomness Ä‘á»ƒ Ä‘áº£m báº£o má»—i láº§n khÃ¡c nhau
            iteration_factor = random.uniform(0.5, 2.0) * (1 + 0.1 * (iteration % 10))
            
            # Index-based diversity Ä‘á»ƒ trÃ¡nh bias vá»‹ trÃ­
            index_factor = random.uniform(0.7, 1.3) * (1 + 0.05 * (i % 7))
            
            # Time-based randomness
            time_factor = random.uniform(0.8, 1.2) * (1 + 0.001 * (int(time.time()) % 1000))
            
            # Káº¿t há»£p táº¥t cáº£ factors
            final_weight = length_weight * position_weight * iteration_factor * index_factor * time_factor
            weights.append(max(final_weight, 0.1))
        
        # ThÃªm noise vÃ o weights Ä‘á»ƒ tÄƒng entropy
        for i in range(len(weights)):
            noise = random.uniform(0.9, 1.1)
            weights[i] *= noise
        
        # Monte Carlo sampling vá»›i multiple rounds Ä‘á»ƒ tÄƒng randomness
        selected = []
        available_indices = list(range(len(all_articles)))
        available_weights = weights.copy()
        
        for round_idx in range(sample_size):
            if not available_indices:
                break
            
            # ThÃªm round-based randomness
            round_factor = random.uniform(0.9, 1.1)
            adjusted_weights = [w * round_factor * random.uniform(0.95, 1.05) for w in available_weights]
            
            total_weight = sum(adjusted_weights)
            if total_weight == 0:
                chosen_idx = random.randint(0, len(available_indices) - 1)
            else:
                # Multiple random attempts Ä‘á»ƒ tÄƒng entropy
                best_choice = None
                for attempt in range(3):  # Thá»­ 3 láº§n, chá»n láº§n cuá»‘i
                    rand_val = random.uniform(0, total_weight)
                    cumsum = 0
                    for i, weight in enumerate(adjusted_weights):
                        cumsum += weight
                        if rand_val <= cumsum:
                            best_choice = i
                            break
                    if best_choice is None:
                        best_choice = len(adjusted_weights) - 1
                
                chosen_idx = best_choice if best_choice is not None else random.randint(0, len(available_indices) - 1)
            
            # Add selected article
            selected.append(all_articles[available_indices[chosen_idx]])
            
            # Remove from available
            available_indices.pop(chosen_idx)
            available_weights.pop(chosen_idx)
        
        # Final shuffle vá»›i iteration seed
        random.seed(hash(f"final_{iteration}_{len(selected)}_{time.time()}") % 100000)
        random.shuffle(selected)
        random.seed()  # Reset seed
        
        print(f"ğŸ² Monte Carlo sampling (iteration {iteration}): chá»n {len(selected)}/{len(all_articles)} articles vá»›i high entropy")
        return selected

    def generate_from_multiple_documents(self, documents, topic_name, data_type, num_samples, llm_type="gemini"):
        """Sinh dá»¯ liá»‡u tá»« nhiá»u documents - main method"""
        if not documents:
            return []

        print(f"ğŸ” PhÃ¢n tÃ­ch {len(documents)} documents...")

        # Láº¥y units tá»« parsed structure
        all_units = []
        for doc in documents:
            units = self.get_units_from_parsed_structure(doc)
            all_units.extend(units)
            print(f"  ğŸ“‹ {doc.title}: {len(units)} units")

        print(f"ğŸ“Š Tá»•ng cá»™ng: {len(all_units)} units tá»« {len(documents)} tÃ i liá»‡u")

        # Monte Carlo sampling
        max_units = min(len(all_units), max(num_samples // 2, 10))
        selected_units = self.monte_carlo_sample_articles(all_units, max_units)
        print(f"  ğŸ¯ ÄÃ£ chá»n {len(selected_units)} units")

        # Sinh dá»¯ liá»‡u
        all_samples = self.generate_samples_from_units(selected_units, topic_name, data_type, num_samples, llm_type)

        # Lá»c trÃ¹ng láº·p
        print(f"ğŸ” Kiá»ƒm tra tÆ°Æ¡ng Ä‘á»“ng cho {len(all_samples)} samples...")
        filtered_samples = self.filter_duplicate_questions(all_samples, verbose=True)

        print(f"âœ… HoÃ n thÃ nh: {len(filtered_samples)} samples (Ä‘Ã£ lá»c {len(all_samples) - len(filtered_samples)} trÃ¹ng láº·p)")
        return filtered_samples[:num_samples]

    def generate_samples_from_units(self, units, topic, data_type, num_samples, llm_type="gemini"):
        """Sinh dá»¯ liá»‡u Ä‘Æ¡n giáº£n vá»›i sources chung cho táº¥t cáº£ cÃ¢u há»i"""
        if not units:
            return []
        
        # XÃ¡c Ä‘á»‹nh sá»‘ sources theo yÃªu cáº§u
        num_sources_map = {
            'word_matching': min(1, len(units)),
            'concept_understanding': min(1, len(units)),
            'multi_paragraph_reading': min(2, len(units)),
            'multi_hop_reasoning': min(3, len(units))
        }
        num_sources = num_sources_map.get(data_type, min(3, len(units)))
        
        # Táº¡o cÃ¢u há»i - má»—i iteration tá»± Monte Carlo chá»n units
        all_samples = []
        
        # Rate limiting info
        if llm_type == "gemini":
            estimated_time = num_samples * self.min_interval / 60  # minutes
            print(f"â³ Estimated time for {num_samples} samples with Gemini: {estimated_time:.1f} minutes")
        
        for i in range(num_samples):
            print(f"ğŸ”„ Generating sample {i+1}/{num_samples}...")
            
            # Monte Carlo sampling cho iteration nÃ y
            selected_units = self.monte_carlo_sample_articles(units, num_sources, iteration=i)
            
            # Táº¡o sources vÃ  content cho iteration nÃ y
            iteration_sources = []
            combined_content = []
            
            for unit in selected_units:
                source_ref = SourceReference(
                    unit_path=unit.get('path', unit['title']),
                    unit_id=unit.get('id', 'unknown_id'),
                    document_title=unit['document_title']
                )
                iteration_sources.append(source_ref)
                unit_path = unit.get('path', unit['title'])
                combined_content.append(f"--- {unit['title']} ({unit_path}) ---\n{unit['content']}")

            combined_text = "\n\n".join(combined_content)
            
            # Rule-based difficulty cho iteration nÃ y
            difficulty = self.get_rule_based_difficulty(data_type, len(selected_units))
            
            # Táº¡o prompt cho iteration nÃ y
            prompt = self.create_diverse_prompt(combined_text, topic, data_type, difficulty, i)
            
            try:
                temperature = random.uniform(0.6, 0.9)
                
                # Sá»­ dá»¥ng LLM Ä‘Æ°á»£c chá»n
                structured_data = self.generate_qa(prompt, llm_type, temperature)
                
                # Convert vá»›i sources chung vÃ  rule-based difficulty
                for qa_pair in structured_data.qa_pairs:
                    sample = {
                        'question': qa_pair.question,
                        'answer': qa_pair.answer,
                        'difficulty': difficulty,  
                        'sources': [
                            {
                                'unit_path': src.unit_path,
                                'unit_id': src.unit_id,
                                'document_title': src.document_title
                            } for src in iteration_sources  # Sources cho iteration nÃ y
                        ],
                        'metadata': {
                            'generation_method': 'per_iteration_monte_carlo',
                            'num_sources': len(selected_units),
                            'temperature': temperature,
                            'llm_type': llm_type
                        }
                    }
                    all_samples.append(sample)
                    
                print(f"âœ… Sample {i+1}/{num_samples} completed")
                    
            except Exception as e:
                print(f"âŒ Generation failed for sample {i+1}/{num_samples}: {e}")
                continue
        
        return all_samples

    def create_diverse_prompt(self, content, topic, data_type, difficulty, iteration):
        """HÃ m gá»‘c táº¡o prompt Ä‘a dáº¡ng - sá»­ dá»¥ng lÃ m base cho cÃ¡c loáº¡i cÃ¢u há»i"""
        # Cáº¥u trÃºc cÃ¢u há»i Ä‘a dáº¡ng
        question_starters = [
            "Khi nÃ o", "Trong trÆ°á»ng há»£p nÃ o", "Ai cÃ³ trÃ¡ch nhiá»‡m",
            "Viá»‡c...Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° tháº¿ nÃ o", "Äiá»u kiá»‡n...lÃ  gÃ¬",
            "Má»©c pháº¡t...lÃ  bao nhiÃªu", "Quy trÃ¬nh...diá»…n ra ra sao",
            "Táº¡i sao", "VÃ¬ sao", "LÃ m cÃ¡ch nÃ o", "Báº±ng phÆ°Æ¡ng thá»©c nÃ o",
            "CÃ³ Ä‘Æ°á»£c phÃ©p", "CÃ³ báº¯t buá»™c", "CÃ³ cáº§n thiáº¿t",
            "Thá»§ tá»¥c...nhÆ° tháº¿ nÃ o", "HÃ¬nh thá»©c...lÃ  gÃ¬", "Pháº¡m vi...ra sao"
        ]
        
        focus_areas = [
            "quy Ä‘á»‹nh thá»±c táº¿ vÃ  á»©ng dá»¥ng cá»¥ thá»ƒ",
            "trÆ°á»ng há»£p ngoáº¡i lá»‡ vÃ  Ä‘iá»u kiá»‡n Ä‘áº·c biá»‡t", 
            "nghÄ©a vá»¥ vÃ  quyá»n háº¡n cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng",
            "má»©c pháº¡t vÃ  háº­u quáº£ phÃ¡p lÃ½",
            "quy trÃ¬nh thá»§ tá»¥c phÃ¡p lÃ½ chi tiáº¿t",
            "Ä‘á»‹nh nghÄ©a thuáº­t ngá»¯ chuyÃªn mÃ´n",
            "tháº©m quyá»n vÃ  trÃ¡ch nhiá»‡m quáº£n lÃ½"
        ]
        
        # Random selection vá»›i seed tá»« iteration Ä‘á»ƒ táº¡o diversity
        random.seed(hash(f"{data_type}_{iteration}_{topic}") % 10000)
        starter = random.choice(question_starters)
        focus = random.choice(focus_areas)
        
        # Reset seed
        random.seed()
        
        # Gá»i hÃ m con tÆ°Æ¡ng á»©ng vá»›i data_type
        if data_type == "word_matching":
            return self.create_word_matching_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "concept_understanding":
            return self.create_concept_understanding_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "multi_paragraph_reading":
            return self.create_multi_paragraph_prompt(content, topic, starter, focus, difficulty)
        elif data_type == "multihop":
            return self.create_multihop_prompt(content, topic, starter, focus, difficulty)
        else:
            return self.create_concept_understanding_prompt(content, topic, starter, focus, difficulty)

    def create_word_matching_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Word Matching - tÃ¬m tá»« khÃ³a, thuáº­t ngá»¯ cá»¥ thá»ƒ trong vÄƒn báº£n"""
        return f"""
    DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":

    {content}

    HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i WORD MATCHING (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.

    Äáº¶C ÄIá»‚M CÃ‚U Há»I WORD MATCHING:
    - YÃªu cáº§u tÃ¬m tá»« khÃ³a, thuáº­t ngá»¯ cá»¥ thá»ƒ 
    - Há»i vá» Ä‘á»‹nh nghÄ©a chÃ­nh xÃ¡c cá»§a cÃ¡c khÃ¡i niá»‡m phÃ¡p lÃ½
    - CÃ¢u tráº£ lá»i lÃ  thÃ´ng tin cá»¥ thá»ƒ, rÃµ rÃ ng
    - Táº­p trung vÃ o thuáº­t ngá»¯ chuyÃªn mÃ´n, sá»‘ liá»‡u cá»¥ thá»ƒ

    YÃŠU Cáº¦U QUAN TRá»ŒNG:
    1. CÃ¢u há»i vÃ  Ä‘Ã¡p Ã¡n pháº£i HOÃ€N TOÃ€N Äá»˜C Láº¬P - cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n context bÃªn ngoÃ i
    2. TUYá»†T Äá»I KHÃ”NG dÃ¹ng "dá»±a trÃªn Ä‘iá»u luáº­t trÃªn", "theo quy Ä‘á»‹nh trÃªn", "cÄƒn cá»© vÃ o Ä‘iá»u trÃªn"
    3. KhÃ´ng cáº§n thiáº¿t pháº£i trÃ­ch dáº«n, Náº¾U cáº§n trÃ­ch dáº«n: pháº£i ghi Äáº¦Y Äá»¦ tÃªn vÄƒn báº£n (vÃ­ dá»¥: "Theo Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™ 2008, Äiá»u 25") hoáº·c ná»™i dung pháº§n vÄƒn báº£n cáº§n trÃ­ch dáº«n
    4. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..." 
    5. ÄÃ¡p Ã¡n pháº£i DIá»„N GIáº¢I Äáº¦Y Äá»¦, khÃ´ng cá»¥t lá»§n nhÆ° "11%" hay "Thá»‘ng Ä‘á»‘c"

    VÃ Dá»¤ Tá»T:
    Question: "Äá»™ tuá»•i tá»‘i thiá»ƒu Ä‘á»ƒ Ä‘Æ°á»£c cáº¥p báº±ng lÃ¡i xe Ã´ tÃ´ lÃ  bao nhiÃªu?"
    Answer: "Äá»™ tuá»•i tá»‘i thiá»ƒu Ä‘á»ƒ Ä‘Æ°á»£c cáº¥p báº±ng lÃ¡i xe Ã´ tÃ´ lÃ  18 tuá»•i Ä‘á»‘i vá»›i xe Ã´ tÃ´ con vÃ  21 tuá»•i Ä‘á»‘i vá»›i xe táº£i, xe khÃ¡ch theo quy Ä‘á»‹nh cá»§a Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™."

    VÃ Dá»¤ Xáº¤U (TRÃNH):
    Answer: "Dá»±a trÃªn Ä‘iá»u luáº­t trÃªn, Ä‘á»™ tuá»•i lÃ  18 tuá»•i."

    Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
    """

    def create_concept_understanding_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Concept Understanding - hiá»ƒu khÃ¡i niá»‡m, nguyÃªn táº¯c"""
        return f"""
    DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":

    {content}

    HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i CONCEPT UNDERSTANDING (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.

    Äáº¶C ÄIá»‚M CÃ‚U Há»I CONCEPT UNDERSTANDING:
    - Kiá»ƒm tra hiá»ƒu biáº¿t vá» khÃ¡i niá»‡m, nguyÃªn táº¯c phÃ¡p lÃ½
    - YÃªu cáº§u giáº£i thÃ­ch Ã½ nghÄ©a, má»¥c Ä‘Ã­ch cá»§a quy Ä‘á»‹nh
    - CÃ¢u tráº£ lá»i cáº§n diá»…n giáº£i, giáº£i thÃ­ch rÃµ rÃ ng
    - Táº­p trung vÃ o viá»‡c hiá»ƒu "táº¡i sao" vÃ  "nhÆ° tháº¿ nÃ o"

    YÃŠU Cáº¦U QUAN TRá»ŒNG:
    1. CÃ¢u há»i vÃ  Ä‘Ã¡p Ã¡n pháº£i HOÃ€N TOÃ€N Äá»˜C Láº¬P - cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n context bÃªn ngoÃ i
    2. TUYá»†T Äá»I KHÃ”NG dÃ¹ng "dá»±a trÃªn Ä‘iá»u luáº­t trÃªn", "theo quy Ä‘á»‹nh trÃªn", "cÄƒn cá»© vÃ o Ä‘iá»u trÃªn"
    3. KhÃ´ng cáº§n thiáº¿t pháº£i trÃ­ch dáº«n, Náº¾U cáº§n trÃ­ch dáº«n: pháº£i ghi Äáº¦Y Äá»¦ tÃªn vÄƒn báº£n (vÃ­ dá»¥: "Theo Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™ 2008, Äiá»u 25") hoáº·c ná»™i dung pháº§n vÄƒn báº£n cáº§n trÃ­ch dáº«n
    4. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..."
    5. ÄÃ¡p Ã¡n cáº§n giáº£i thÃ­ch khÃ¡i niá»‡m Ä‘áº§y Ä‘á»§, khÃ´ng cá»¥t lá»§n nhÆ° "LÃ  viá»‡c kiá»ƒm tra Ä‘á»‹nh ká»³" hay "Äá»ƒ Ä‘áº£m báº£o an toÃ n" nhÆ°ng cÅ©ng khÃ´ng diá»…n giáº£i quÃ¡ lÃª thÃª

    VÃ Dá»¤ Tá»T:
    Question: "Táº¡i sao viá»‡c kiá»ƒm Ä‘á»‹nh Ä‘á»‹nh ká»³ phÆ°Æ¡ng tiá»‡n giao thÃ´ng lÃ  báº¯t buá»™c?"
    Answer: "Viá»‡c kiá»ƒm Ä‘á»‹nh Ä‘á»‹nh ká»³ phÆ°Æ¡ng tiá»‡n giao thÃ´ng lÃ  báº¯t buá»™c nháº±m Ä‘áº£m báº£o an toÃ n giao thÃ´ng, kiá»ƒm tra tÃ¬nh tráº¡ng ká»¹ thuáº­t cá»§a xe, phÃ¡t hiá»‡n sá»›m cÃ¡c hÆ° há»ng cÃ³ thá»ƒ gÃ¢y tai náº¡n, Ä‘á»“ng thá»i kiá»ƒm soÃ¡t khÃ­ tháº£i báº£o vá»‡ mÃ´i trÆ°á»ng."

    Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
    Äá»ŠNH Dáº NG OUTPUT Báº®T BUá»˜C:
    Tráº£ vá» duy nháº¥t má»™t khá»‘i mÃ£ JSON há»£p lá»‡. KhÃ´ng thÃªm báº¥t ká»³ lá»i giáº£i thÃ­ch hay vÄƒn báº£n nÃ o khÃ¡c bÃªn ngoÃ i khá»‘i JSON.

    ```json
    {{
    "qa_pairs": [
        {{
        "question": "Ná»™i dung cÃ¢u há»i Ä‘Æ°á»£c táº¡o ra á»Ÿ Ä‘Ã¢y, báº¯t Ä‘áº§u báº±ng '{starter}' vÃ  táº­p trung vÃ o chá»§ Ä‘á» '{focus}'.",
        "answer": "Ná»™i dung cÃ¢u tráº£ lá»i chi tiáº¿t, giáº£i thÃ­ch rÃµ rÃ ng khÃ¡i niá»‡m, nguyÃªn táº¯c vÃ  cÃ³ thá»ƒ kÃ¨m theo vÃ­ dá»¥ cá»¥ thá»ƒ Ä‘á»ƒ minh há»a."
        }}
    ]
    }}
        """

    def create_multi_paragraph_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Multi-paragraph Reading - Ä‘á»c hiá»ƒu nhiá»u Ä‘oáº¡n vÄƒn"""
        return f"""
    DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":

    {content}

    HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i MULTI-PARAGRAPH READING (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.
    
    Äáº¶C ÄIá»‚M CÃ‚U Há»I MULTI-PARAGRAPH READING:
    - YÃªu cáº§u tá»•ng há»£p thÃ´ng tin tá»« nhiá»u quy Ä‘á»‹nh khÃ¡c nhau
    - So sÃ¡nh, Ä‘á»‘i chiáº¿u cÃ¡c Ä‘iá»u khoáº£n
    - TÃ¬m má»‘i liÃªn há»‡ giá»¯a cÃ¡c quy Ä‘á»‹nh
    - CÃ¢u tráº£ lá»i cáº§n káº¿t há»£p thÃ´ng tin tá»« nhiá»u nguá»“n
    
    YÃŠU Cáº¦U QUAN TRá»ŒNG:
    1. CÃ‚U Há»I PHáº¢I CÃ“ Äá»˜ BAO PHá»¦ Rá»˜NG: Cá»‘ gáº¯ng thiáº¿t káº¿ cÃ¢u há»i sao cho ngÆ°á»i tráº£ lá»i Báº®T BUá»˜C pháº£i Ä‘á»c vÃ  tá»•ng há»£p thÃ´ng tin tá»« Táº¤T Cáº¢ cÃ¡c Ä‘oáº¡n trÃ­ch Ä‘Ã£ cho Ä‘á»ƒ cÃ³ thá»ƒ tráº£ lá»i Ä‘áº§y Ä‘á»§.
    2. CÃ‚U Há»I NÃŠN Báº®T Äáº¦U Tá»ª Má»˜T TÃŒNH HUá»NG THá»°C Táº¾: HÃ£y tÆ°á»Ÿng tÆ°á»£ng má»™t ká»‹ch báº£n cá»¥ thá»ƒ mÃ  má»™t chá»§ thá»ƒ cÃ³ thá»ƒ gáº·p pháº£i, sau Ä‘Ã³ Ä‘áº·t cÃ¢u há»i phÃ¡p lÃ½ liÃªn quan Ä‘áº¿n ká»‹ch báº£n Ä‘Ã³.
    3. CÃ¢u há»i vÃ  Ä‘Ã¡p Ã¡n pháº£i HOÃ€N TOÃ€N Äá»˜C Láº¬P - cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n context bÃªn ngoÃ i
    4. TUYá»†T Äá»I KHÃ”NG dÃ¹ng "dá»±a trÃªn Ä‘iá»u luáº­t trÃªn", "theo cÃ¡c quy Ä‘á»‹nh trÃªn", "cÄƒn cá»© vÃ o Ä‘iá»u trÃªn"
    5. Náº¾U cáº§n trÃ­ch dáº«n: pháº£i ghi Äáº¦Y Äá»¦ tÃªn vÄƒn báº£n cá»¥ thá»ƒ cho tá»«ng Ä‘iá»u
    6. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..."
    7. ÄÃ¡p Ã¡n pháº£i tá»•ng há»£p, so sÃ¡nh rÃµ rÃ ng cÃ¡c quy Ä‘á»‹nh khÃ¡c nhau
    
    VÃ Dá»¤ Tá»T:
    Question: "Náº¿u má»™t ngÆ°á»i vá»«a muá»‘n Ä‘Äƒng kÃ½ kinh doanh há»™ cÃ¡ thá»ƒ, vá»«a muá»‘n má»Ÿ má»™t doanh nghiá»‡p tÆ° nhÃ¢n khÃ¡c, quy Ä‘á»‹nh phÃ¡p luáº­t vá» cÃ¡c trÆ°á»ng há»£p nÃ y cÃ³ Ä‘iá»ƒm gÃ¬ giá»‘ng vÃ  khÃ¡c nhau vá» quyá»n vÃ  nghÄ©a vá»¥?"
    Answer: "Äiá»ƒm giá»‘ng nhau lÃ  cáº£ hai Ä‘á»u do má»™t cÃ¡ nhÃ¢n lÃ m chá»§ vÃ  chá»‹u trÃ¡ch nhiá»‡m vÃ´ háº¡n. Tuy nhiÃªn, Ä‘iá»ƒm khÃ¡c biá»‡t lá»›n lÃ  theo quy Ä‘á»‹nh, má»™t cÃ¡ nhÃ¢n chá»‰ Ä‘Æ°á»£c thÃ nh láº­p má»™t doanh nghiá»‡p tÆ° nhÃ¢n, nhÆ°ng cÃ³ thá»ƒ Ä‘á»“ng thá»i lÃ  chá»§ há»™ kinh doanh. Do Ä‘Ã³, ngÆ°á»i nÃ y cÃ³ thá»ƒ tiáº¿p tá»¥c Ä‘Äƒng kÃ½ há»™ kinh doanh nhÆ°ng khÃ´ng thá»ƒ má»Ÿ thÃªm doanh nghiá»‡p tÆ° nhÃ¢n thá»© hai."
    
    Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
        """

    def create_multihop_prompt(self, content, topic, starter, focus, difficulty):
        """Prompt cho loáº¡i Multihop - suy luáº­n qua nhiá»u bÆ°á»›c"""
        return f"""
    DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»u luáº­t vá» chá»§ Ä‘á» "{topic}":

    {content}

    HÃ£y táº¡o 1 cÃ¢u há»i loáº¡i MULTIHOP (Ä‘á»™ khÃ³ {difficulty}) táº­p trung vÃ o {focus}.
    
    Äáº¶C ÄIá»‚M CÃ‚U Há»I MULTIHOP:
    - YÃªu cáº§u suy luáº­n logic qua nhiá»u bÆ°á»›c
    - Káº¿t há»£p nhiá»u quy Ä‘á»‹nh Ä‘á»ƒ Ä‘Æ°a ra káº¿t luáº­n
    - Ãp dá»¥ng quy táº¯c vÃ o tÃ¬nh huá»‘ng phá»©c táº¡p, thá»±c táº¿
    - CÃ¢u tráº£ lá»i cáº§n chuá»—i suy luáº­n cÃ³ logic rÃµ rÃ ng
    
    YÃŠU Cáº¦U QUAN TRá»ŒNG:
    1. CÃ‚U Há»I PHáº¢I CÃ“ Äá»˜ BAO PHá»¦ Rá»˜NG: Cá»‘ gáº¯ng thiáº¿t káº¿ cÃ¢u há»i sao cho ngÆ°á»i tráº£ lá»i Báº®T BUá»˜C pháº£i Ä‘á»c vÃ  tá»•ng há»£p thÃ´ng tin tá»« Táº¤T Cáº¢ cÃ¡c Ä‘oáº¡n trÃ­ch Ä‘Ã£ cho Ä‘á»ƒ cÃ³ thá»ƒ tráº£ lá»i Ä‘áº§y Ä‘á»§.
    2. CÃ‚U Há»I NÃŠN Báº®T Äáº¦U Tá»ª Má»˜T TÃŒNH HUá»NG THá»°C Táº¾: HÃ£y tÆ°á»Ÿng tÆ°á»£ng má»™t ká»‹ch báº£n cá»¥ thá»ƒ mÃ  má»™t chá»§ thá»ƒ cÃ³ thá»ƒ gáº·p pháº£i, sau Ä‘Ã³ Ä‘áº·t cÃ¢u há»i phÃ¡p lÃ½ liÃªn quan Ä‘áº¿n ká»‹ch báº£n Ä‘Ã³.                                                        
    3. CÃ¢u há»i vÃ  Ä‘Ã¡p Ã¡n pháº£i HOÃ€N TOÃ€N Äá»˜C Láº¬P - cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n context bÃªn ngoÃ i
    4. TUYá»†T Äá»I KHÃ”NG dÃ¹ng "dá»±a trÃªn Ä‘iá»u luáº­t trÃªn", "theo cÃ¡c quy Ä‘á»‹nh trÃªn", "cÄƒn cá»© vÃ o Ä‘iá»u trÃªn"
    5. Náº¾U cáº§n trÃ­ch dáº«n: pháº£i ghi Äáº¦Y Äá»¦ tÃªn vÄƒn báº£n vÃ  Ä‘iá»u cá»¥ thá»ƒ
    6. Báº¡n cÃ³ thá»ƒ tham kháº£o báº¯t Ä‘áº§u cÃ¢u há»i báº±ng "{starter}..."
    7. ÄÃ¡p Ã¡n cáº§n cÃ³ chuá»—i suy luáº­n tá»«ng bÆ°á»›c: tÃ¬nh huá»‘ng â†’ quy Ä‘á»‹nh Ã¡p dá»¥ng â†’ káº¿t luáº­n
    
    VÃ Dá»¤ Tá»T:
    Question: "Má»™t tÃ i xáº¿ lÃ¡i xe táº£i chá»Ÿ hÃ ng quÃ¡ táº£i 50% vÃ  khÃ´ng cÃ³ báº±ng lÃ¡i phÃ¹ há»£p sáº½ bá»‹ xá»­ lÃ½ nhÆ° tháº¿ nÃ o?"
    Answer: "TÃ i xáº¿ nÃ y sáº½ bá»‹ xá»­ pháº¡t kÃ©p: Ä‘áº§u tiÃªn bá»‹ pháº¡t tiá»n 12-15 triá»‡u Ä‘á»“ng vÃ  tÆ°á»›c báº±ng lÃ¡i 2-4 thÃ¡ng do chá»Ÿ quÃ¡ táº£i theo Nghá»‹ Ä‘á»‹nh 100/2019, Ä‘á»“ng thá»i bá»‹ pháº¡t 16-18 triá»‡u vÃ  tÆ°á»›c báº±ng lÃ¡i 10-12 thÃ¡ng do khÃ´ng cÃ³ báº±ng lÃ¡i phÃ¹ há»£p. Tá»•ng cá»™ng cÃ³ thá»ƒ bá»‹ pháº¡t Ä‘áº¿n 33 triá»‡u Ä‘á»“ng vÃ  tÆ°á»›c báº±ng lÃ¡i tá»‘i Ä‘a 16 thÃ¡ng."
    
    VÃ Dá»¤ Xáº¤U (TRÃNH):
    Answer: "CÄƒn cá»© vÃ o cÃ¡c Ä‘iá»u luáº­t trÃªn, tÃ i xáº¿ sáº½ bá»‹ xá»­ pháº¡t..."
    
    Tráº£ vá» output dÆ°á»›i dáº¡ng JSON vá»›i qa_pairs.
        """