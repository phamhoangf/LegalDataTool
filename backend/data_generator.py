from google import genai
from google.genai import types
import json
import random
import os
import re
import time
from typing import List, Dict, Any
from pydantic import BaseModel
from similarity_checker import QuestionSimilarityChecker
from document_parsers import LegalDocumentParser

class SourceReference(BaseModel):
    """Tham chi·∫øu ƒë·∫øn ngu·ªìn c·ªßa th√¥ng tin"""
    article_number: str  # S·ªë ƒëi·ªÅu (v√≠ d·ª•: "60", "61")
    article_title: str   # Ti√™u ƒë·ªÅ ƒëi·ªÅu (v√≠ d·ª•: "ƒêi·ªÅu 60. ƒê·ªô tu·ªïi c·ªßa ng∆∞·ªùi l√°i xe")
    document_title: str  # T√™n t√†i li·ªáu (v√≠ d·ª•: "Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô 2008")

class LegalQA(BaseModel):
    """C·∫•u tr√∫c c√¢u h·ªèi-ƒë√°p √°n ph√°p l√Ω"""
    question: str
    answer: str

class LegalQAList(BaseModel):
    """Danh s√°ch c√¢u h·ªèi-ƒë√°p √°n (kh√¥ng c·∫ßn sources v√¨ ƒë√£ rule-based)"""
    qa_pairs: List[LegalQA]

class DataGenerator:
    """Class sinh d·ªØ li·ªáu hu·∫•n luy·ªán cho LegalSLM - Version g·ªçn g√†ng"""
    
    def __init__(self, api_key: str = None, similarity_threshold: float = 0.75):
        # Set API key
        if api_key:
            os.environ['GEMINI_API_KEY'] = api_key
        elif not os.environ.get('GEMINI_API_KEY'):
            google_key = os.environ.get('GOOGLE_API_KEY')
            if google_key:
                os.environ['GEMINI_API_KEY'] = google_key
        
        self.client = genai.Client()
        self.model = "gemini-2.5-flash"
        
        # Kh·ªüi t·∫°o similarity checker
        self.similarity_checker = QuestionSimilarityChecker(similarity_threshold=similarity_threshold)
        print(f"üîç Initialized similarity checker with threshold {similarity_threshold}")
    
    def get_rule_based_difficulty(self, data_type: str, num_sources: int) -> str:
        """Rule-based difficulty thay v√¨ y√™u c·∫ßu LLM t·∫°o ra"""
        if data_type == 'word_matching':
            return 'easy'
        elif data_type == 'concept_understanding':
            return 'easy' if num_sources == 1 else 'medium'
        elif data_type == 'multi_paragraph_reading':
            return 'medium' if num_sources <= 3 else 'hard'
        elif data_type == 'multi_hop_reasoning':
            return 'hard'
        else:
            return 'medium'

    def update_similarity_corpus(self, existing_questions_data: List[Dict[str, Any]]):
        """C·∫≠p nh·∫≠t corpus cho similarity checker v·ªõi d·ªØ li·ªáu hi·ªán c√≥"""
        self.similarity_checker.update_corpus(existing_questions_data)
    
    def filter_duplicate_questions(self, new_samples: List[Dict[str, Any]], verbose: bool = True) -> List[Dict[str, Any]]:
        """L·ªçc b·ªè c√°c c√¢u h·ªèi tr√πng l·∫∑p t·ª´ danh s√°ch samples m·ªõi"""
        if not new_samples:
            return []
        
        questions = [sample.get('question', '') for sample in new_samples]
        similarity_results = self.similarity_checker.check_batch_similarity(questions)
        
        filtered_samples = []
        duplicates_found = 0
        
        for i, (sample, result) in enumerate(zip(new_samples, similarity_results)):
            if not result['is_duplicate']:
                filtered_samples.append(sample)
            else:
                duplicates_found += 1
                if verbose:
                    print(f"üö´ Filtered duplicate question {i+1}:")
                    print(f"   Question: {result['question'][:80]}...")
                    print(f"   Max similarity: {result['max_similarity']:.3f}")
        
        if verbose and duplicates_found > 0:
            print(f"üîç Filtered {duplicates_found}/{len(new_samples)} duplicate questions")
        
        return filtered_samples

    def get_articles_from_parsed_structure(self, document) -> List[Dict]:
        """L·∫•y articles t·ª´ parsed structure ho·∫∑c fallback"""
        # Ki·ªÉm tra parsed structure
        if hasattr(document, 'parsed_structure') and document.parsed_structure:
            try:
                parsed_data = json.loads(document.parsed_structure)
                parser = LegalDocumentParser()
                articles = parser.get_all_articles(parsed_data)
                
                # Convert to format cho data generator
                units = []
                for article in articles:
                    units.append({
                        "id": f"article_{article['number']}",
                        "title": f"ƒêi·ªÅu {article['number']}. {article['title']}",
                        "content": article['content'],
                        "document_title": document.title,
                        "metadata": {
                            "article_number": article['number'],
                            "source_document": document.title,
                            "unit_type": "article",
                            "length": article['content_length']
                        }
                    })
                
                print(f"‚úÖ Using parsed structure: {len(units)} articles from {document.title}")
                return units
                
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to use parsed structure: {str(e)}, fallback to simple parsing")
        
        # Fallback - simple article extraction
        return self.split_law_by_article(document.content, document.title)

    def split_law_by_article(self, text: str, document_title: str = "") -> List[Dict]:
        """T√°ch vƒÉn b·∫£n lu·∫≠t th√†nh c√°c ƒëi·ªÅu"""
        units = []
        split_pattern = r'(?m)(?=^\s*ƒêi·ªÅu \d+\.)'
        chunks = re.split(split_pattern, text.strip())
        
        for chunk in chunks:
            chunk = chunk.strip()
            lines = chunk.split('\n')
            dieu_line = None
            for line in lines:
                if re.match(r'^\s*ƒêi·ªÅu \d+\.', line):
                    dieu_line = line.strip()
                    break
            
            if dieu_line:
                match = re.search(r'ƒêi·ªÅu (\d+)', dieu_line)
                if match:
                    article_number = match.group(1)
                    units.append({
                        "id": f"article_{article_number}",
                        "title": dieu_line,
                        "content": chunk,
                        "document_title": document_title,
                        "metadata": {
                            "article_number": article_number,
                            "source_document": document_title,
                            "unit_type": "article",
                            "length": len(chunk)
                        }
                    })
                
        return units

    def monte_carlo_sample_articles(self, all_articles: List[Dict], sample_size: int) -> List[Dict]:
        """Monte Carlo sampling ƒë∆°n gi·∫£n v·ªõi weights d·ª±a tr√™n content length v√† position"""
        if not all_articles or sample_size <= 0:
            return []
            
        if sample_size >= len(all_articles):
            articles_copy = all_articles.copy()
            random.shuffle(articles_copy)
            return articles_copy
        
        # T√≠nh weights ƒë∆°n gi·∫£n
        weights = []
        for article in all_articles:
            # Base weight t·ª´ content length
            content_length = article.get('metadata', {}).get('length', len(article.get('content', '')))
            length_weight = min(content_length / 800, 2.0)
            
            # Position weight (strategic articles)
            article_num = article.get('metadata', {}).get('article_number')
            position_weight = 1.0
            if article_num:
                try:
                    num = int(article_num)
                    if num <= 5 or num % 20 == 0:
                        position_weight = 1.5
                    elif num <= 20:
                        position_weight = 1.2
                except:
                    pass
            
            # Random factor cho diversity
            random_factor = random.uniform(0.7, 1.3)
            final_weight = length_weight * position_weight * random_factor
            weights.append(max(final_weight, 0.1))
        
        # Monte Carlo sampling
        selected = []
        available_indices = list(range(len(all_articles)))
        available_weights = weights.copy()
        
        for _ in range(sample_size):
            if not available_indices:
                break
            
            total_weight = sum(available_weights)
            if total_weight == 0:
                chosen_idx = random.randint(0, len(available_indices) - 1)
            else:
                rand_val = random.uniform(0, total_weight)
                cumsum = 0
                chosen_idx = len(available_weights) - 1
                
                for i, weight in enumerate(available_weights):
                    cumsum += weight
                    if rand_val <= cumsum:
                        chosen_idx = i
                        break
            
            # Add selected article
            selected.append(all_articles[available_indices[chosen_idx]])
            
            # Remove from available
            available_indices.pop(chosen_idx)
            available_weights.pop(chosen_idx)
        
        random.shuffle(selected)
        print(f"üé≤ Monte Carlo sampling: ch·ªçn {len(selected)}/{len(all_articles)} articles")
        return selected

    def generate_from_multiple_documents(self, documents, topic_name, data_type, num_samples):
        """Sinh d·ªØ li·ªáu t·ª´ nhi·ªÅu documents - main method"""
        if not documents:
            return []

        print(f"üîç Ph√¢n t√≠ch {len(documents)} documents...")

        # L·∫•y articles t·ª´ parsed structure
        all_articles = []
        for doc in documents:
            articles = self.get_articles_from_parsed_structure(doc)
            all_articles.extend(articles)
            print(f"  üìã {doc.title}: {len(articles)} ƒëi·ªÅu")

        print(f"üìä T·ªïng c·ªông: {len(all_articles)} ƒëi·ªÅu t·ª´ {len(documents)} t√†i li·ªáu")

        # Monte Carlo sampling
        max_articles = min(len(all_articles), max(num_samples // 2, 10))
        selected_articles = self.monte_carlo_sample_articles(all_articles, max_articles)
        print(f"  üéØ ƒê√£ ch·ªçn {len(selected_articles)} articles")

        # Sinh d·ªØ li·ªáu
        all_samples = self.generate_samples_from_articles(selected_articles, topic_name, data_type, num_samples)

        # L·ªçc tr√πng l·∫∑p
        print(f"üîç Ki·ªÉm tra t∆∞∆°ng ƒë·ªìng cho {len(all_samples)} samples...")
        filtered_samples = self.filter_duplicate_questions(all_samples, verbose=True)

        print(f"‚úÖ Ho√†n th√†nh: {len(filtered_samples)} samples (ƒë√£ l·ªçc {len(all_samples) - len(filtered_samples)} tr√πng l·∫∑p)")
        return filtered_samples[:num_samples]

    def generate_samples_from_articles(self, articles, topic, data_type, num_samples):
        """Sinh d·ªØ li·ªáu ƒë∆°n gi·∫£n v·ªõi sources chung cho t·∫•t c·∫£ c√¢u h·ªèi"""
        if not articles:
            return []
        
        # X√°c ƒë·ªãnh s·ªë sources
        num_sources_map = {
            'word_matching': min(1, len(articles)),
            'concept_understanding': min(1, len(articles)),
            'multi_paragraph_reading': min(2, len(articles)),
            'multi_hop_reasoning': min(3, len(articles))
        }
        num_sources = num_sources_map.get(data_type, min(3, len(articles)))
        
        # Ch·ªçn articles cho sources chung
        if len(articles) <= num_sources:
            selected_articles = articles.copy()
            random.shuffle(selected_articles)
        else:
            selected_articles = random.sample(articles, num_sources)
        
        # T·∫°o sources chung
        common_sources = []
        combined_content = []
        
        for article in selected_articles:
            source_ref = SourceReference(
                article_number=str(article['metadata']['article_number']) if article['metadata']['article_number'] else "unknown",
                article_title=article['title'],
                document_title=article['document_title']
            )
            common_sources.append(source_ref)
            article_path = article.get('path', article['title'])
            combined_content.append(f"--- {article['title']} ({article_path}) ---\n{article['content']}")

        combined_text = "\n\n".join(combined_content)
        
        # Rule-based difficulty
        difficulty = self.get_rule_based_difficulty(data_type, len(selected_articles))
        
        # T·∫°o m·ªôt prompt batch ƒë·ªÉ sinh nhi·ªÅu c√¢u h·ªèi c√πng l√∫c
        prompt = self.create_batch_prompt(combined_text, topic, data_type, difficulty, num_samples)
        
        try:
            temperature = random.uniform(0.6, 0.9)
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=temperature,
                    top_p=random.uniform(0.85, 0.95),
                    max_output_tokens=5000,  # TƒÉng token limit cho batch
                    response_mime_type="application/json",
                    response_schema=LegalQAList,
                    seed=random.randint(1, 1000000)
                )
            )
            
            structured_data: LegalQAList = response.parsed
            
            # Convert v·ªõi sources chung v√† rule-based difficulty
            all_samples = []
            for qa_pair in structured_data.qa_pairs:
                sample = {
                    'question': qa_pair.question,
                    'answer': qa_pair.answer,
                    'difficulty': difficulty,  # Rule-based, kh√¥ng t·ª´ LLM
                    'sources': [
                        {
                            'article_number': src.article_number,
                            'article_title': src.article_title,
                            'document_title': src.document_title
                        } for src in common_sources  # Sources chung
                    ],
                    'metadata': {
                        'generation_method': 'batch_prompt',
                        'num_sources': len(selected_articles),
                        'temperature': temperature
                    }
                }
                all_samples.append(sample)
            
            print(f"T·∫°o ƒë∆∞·ª£c {len(all_samples)} c√¢u h·ªèi t·ª´ batch prompt")
            return all_samples
            
        except Exception as e:
            print(f"L·ªói t·∫°o batch questions: {e}")
            return []

    def create_batch_prompt(self, content, topic, data_type, difficulty, num_samples):
        """T·∫°o prompt ƒë·ªÉ sinh nhi·ªÅu c√¢u h·ªèi c√πng l√∫c v·ªõi c√°c h∆∞·ªõng kh√°c nhau"""
        # T·∫°o instructions ƒëa d·∫°ng v·ªõi randomization t·ªët h∆°n
        instructions = self._generate_diverse_instructions(num_samples, data_type)
        
        # G·ªçi h√†m t·∫°o prompt theo lo·∫°i data_type
        if data_type == "word_matching":
            return self._create_batch_prompt_template(
                content, topic, data_type, difficulty, num_samples, instructions,
                characteristics="""ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI WORD MATCHING:
- Y√™u c·∫ßu t√¨m t·ª´ kh√≥a, thu·∫≠t ng·ªØ c·ª• th·ªÉ trong vƒÉn b·∫£n
- H·ªèi v·ªÅ ƒë·ªãnh nghƒ©a ch√≠nh x√°c c·ªßa c√°c kh√°i ni·ªám ph√°p l√Ω  
- C√¢u tr·∫£ l·ªùi l√† t·ª´/c·ª•m t·ª´ xu·∫•t hi·ªán tr·ª±c ti·∫øp trong vƒÉn b·∫£n
- T·∫≠p trung v√†o thu·∫≠t ng·ªØ chuy√™n m√¥n, s·ªë li·ªáu c·ª• th·ªÉ""",
                requirements="""4. ƒê√°p √°n ph·∫£i l√† di·ªÖn gi·∫£i ƒë·∫ßy ƒë·ªß ph·∫ßn t·ª´/c·ª•m t·ª´ c√¢u h·ªèi y√™u c·∫ßu, ch√≠nh x√°c t·ª´ vƒÉn b·∫£n g·ªëc"""
            )
        elif data_type == "concept_understanding":
            return self._create_batch_prompt_template(
                content, topic, data_type, difficulty, num_samples, instructions,
                characteristics="""ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI CONCEPT UNDERSTANDING:
- Ki·ªÉm tra hi·ªÉu bi·∫øt v·ªÅ kh√°i ni·ªám, nguy√™n t·∫Øc ph√°p l√Ω
- Y√™u c·∫ßu gi·∫£i th√≠ch √Ω nghƒ©a, m·ª•c ƒë√≠ch c·ªßa quy ƒë·ªãnh
- C√¢u tr·∫£ l·ªùi c·∫ßn di·ªÖn gi·∫£i, kh√¥ng ch·ªâ tr√≠ch d·∫´n nguy√™n vƒÉn
- T·∫≠p trung v√†o vi·ªác hi·ªÉu "t·∫°i sao" v√† "nh∆∞ th·∫ø n√†o" """,
                requirements="""4. ƒê√°p √°n c·∫ßn gi·∫£i th√≠ch kh√°i ni·ªám, kh√¥ng ch·ªâ li·ªát k√™"""
            )
        elif data_type == "multi_paragraph_reading":
            return self._create_batch_prompt_template(
                content, topic, data_type, difficulty, num_samples, instructions,
                characteristics="""ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI MULTI-PARAGRAPH READING:
- Y√™u c·∫ßu ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ƒëo·∫°n vƒÉn
- So s√°nh, ƒë·ªëi chi·∫øu c√°c quy ƒë·ªãnh kh√°c nhau
- T√¨m m·ªëi li√™n h·ªá gi·ªØa c√°c ƒëi·ªÅu kho·∫£n
- C√¢u tr·∫£ l·ªùi c·∫ßn k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn trong vƒÉn b·∫£n""",
                requirements="""4. ƒê√°p √°n ph·∫£i t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ph·∫ßn kh√°c nhau c·ªßa vƒÉn b·∫£n"""
            )
        elif data_type == "multihop":
            return self._create_batch_prompt_template(
                content, topic, data_type, difficulty, num_samples, instructions,
                characteristics="""ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI MULTIHOP:
- Y√™u c·∫ßu suy lu·∫≠n logic qua nhi·ªÅu b∆∞·ªõc
- K·∫øt h·ª£p nhi·ªÅu quy ƒë·ªãnh ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n
- √Åp d·ª•ng quy t·∫Øc v√†o t√¨nh hu·ªëng ph·ª©c t·∫°p, th·ª±c t·∫ø
- C√¢u tr·∫£ l·ªùi c·∫ßn tr·∫£i qua chu·ªói suy lu·∫≠n c√≥ logic""",
                requirements="""4. ƒê√°p √°n c·∫ßn c√≥ chu·ªói suy lu·∫≠n r√µ r√†ng, kh√¥ng ch·ªâ k·∫øt lu·∫≠n"""
            )
        else:
            # Default to concept understanding
            return self._create_batch_prompt_template(
                content, topic, "concept_understanding", difficulty, num_samples, instructions,
                characteristics="""ƒê·∫∂C ƒêI·ªÇM C√ÇU H·ªéI CONCEPT UNDERSTANDING:
- Ki·ªÉm tra hi·ªÉu bi·∫øt v·ªÅ kh√°i ni·ªám, nguy√™n t·∫Øc ph√°p l√Ω
- Y√™u c·∫ßu gi·∫£i th√≠ch √Ω nghƒ©a, m·ª•c ƒë√≠ch c·ªßa quy ƒë·ªãnh
- C√¢u tr·∫£ l·ªùi c·∫ßn di·ªÖn gi·∫£i, kh√¥ng ch·ªâ tr√≠ch d·∫´n nguy√™n vƒÉn
- T·∫≠p trung v√†o vi·ªác hi·ªÉu "t·∫°i sao" v√† "nh∆∞ th·∫ø n√†o" """,
                requirements="""4. ƒê√°p √°n c·∫ßn gi·∫£i th√≠ch kh√°i ni·ªám, kh√¥ng ch·ªâ li·ªát k√™"""
            )
    
    def _generate_diverse_instructions(self, num_samples, data_type):
        """T·∫°o instructions ƒëa d·∫°ng v·ªõi randomization t·ªët h∆°n"""
        question_starters = [
            "Khi n√†o", "Trong tr∆∞·ªùng h·ª£p n√†o", "Ai c√≥ tr√°ch nhi·ªám",
            "Vi·ªác...ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o", "ƒêi·ªÅu ki·ªán...l√† g√¨",
            "M·ª©c ph·∫°t...l√† bao nhi√™u", "Quy tr√¨nh...di·ªÖn ra ra sao",
            "T·∫°i sao", "V√¨ sao", "L√†m c√°ch n√†o", "B·∫±ng ph∆∞∆°ng th·ª©c n√†o",
            "C√≥ ƒë∆∞·ª£c ph√©p", "C√≥ b·∫Øt bu·ªôc", "C√≥ c·∫ßn thi·∫øt",
            "Th·ªß t·ª•c...nh∆∞ th·∫ø n√†o", "H√¨nh th·ª©c...l√† g√¨", "Ph·∫°m vi...ra sao"
        ]
        
        focus_areas = [
            "quy ƒë·ªãnh th·ª±c t·∫ø v√† ·ª©ng d·ª•ng c·ª• th·ªÉ",
            "tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá v√† ƒëi·ªÅu ki·ªán ƒë·∫∑c bi·ªát", 
            "nghƒ©a v·ª• v√† quy·ªÅn h·∫°n c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng",
            "m·ª©c ph·∫°t v√† h·∫≠u qu·∫£ ph√°p l√Ω",
            "quy tr√¨nh th·ªß t·ª•c ph√°p l√Ω chi ti·∫øt",
            "ƒë·ªãnh nghƒ©a thu·∫≠t ng·ªØ chuy√™n m√¥n",
            "th·∫©m quy·ªÅn v√† tr√°ch nhi·ªám qu·∫£n l√Ω"
        ]
        
        # ƒê·∫£m b·∫£o randomization t·ªët v·ªõi seed t·ª´ th·ªùi gian hi·ªán t·∫°i
        import time
        random.seed(int(time.time() * 1000) % 10000)
        
        # Shuffle ƒë·ªÉ tƒÉng t√≠nh ng·∫´u nhi√™n
        random.shuffle(question_starters)
        random.shuffle(focus_areas)
        
        # T·∫°o c√°c c·∫∑p starter-focus ng·∫´u nhi√™n v√† kh√¥ng tr√πng l·∫∑p
        instructions = []
        used_combinations = set()
        
        for i in range(num_samples):
            # Th·ª≠ t√¨m combination ch∆∞a ƒë∆∞·ª£c s·ª≠ d·ª•ng
            attempts = 0
            while attempts < 20:  # Tr√°nh v√≤ng l·∫∑p v√¥ h·∫°n
                starter = random.choice(question_starters)
                focus = random.choice(focus_areas)
                combination = (starter, focus)
                
                if combination not in used_combinations or len(used_combinations) >= len(question_starters) * len(focus_areas):
                    used_combinations.add(combination)
                    instructions.append(f"C√¢u {i+1}: B·∫Øt ƒë·∫ßu b·∫±ng \"{starter}...\" v√† t·∫≠p trung v√†o \"{focus}\"")
                    break
                attempts += 1
        
        # Reset seed ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng c√°c random kh√°c
        random.seed()
        
        return "\n".join(instructions)
    
    def _create_batch_prompt_template(self, content, topic, data_type, difficulty, num_samples, instructions, characteristics, requirements):
        """Template chung cho t·∫•t c·∫£ c√°c lo·∫°i batch prompt"""
        return f"""
D∆∞·ªõi ƒë√¢y l√† c√°c ƒëi·ªÅu lu·∫≠t v·ªÅ ch·ªß ƒë·ªÅ "{topic}":

{content}

H√£y t·∫°o {num_samples} c√¢u h·ªèi lo·∫°i {data_type.upper()} (ƒë·ªô kh√≥ {difficulty}) theo h∆∞·ªõng d·∫´n:

{instructions}

{characteristics}

Y√äU C·∫¶U QUAN TR·ªåNG:
1. T·∫°o ƒë√∫ng {num_samples} c√¢u h·ªèi theo h∆∞·ªõng d·∫´n tr√™n
2. H·∫°n ch·∫ø d√πng "Theo ƒêi·ªÅu X c·ªßa Lu·∫≠t..."
3. C√¢u h·ªèi ph·∫£i ƒë·ªôc l·∫≠p, n·∫øu c·∫ßn tham chi·∫øu th√¨ ph·∫£i tr√¨nh b√†y n·ªôi dung c·ª• th·ªÉ, tr√°nh n√≥i lu·∫≠t n√†y lu·∫≠t kia, lu·∫≠t s·ªë bao nhi√™u...
{requirements}
5. M·ªói c√¢u h·ªèi ph·∫£i kh√°c bi·ªát v√† t·∫≠p trung v√†o kh√≠a c·∫°nh ri√™ng

Tr·∫£ v·ªÅ output d∆∞·ªõi d·∫°ng JSON v·ªõi qa_pairs.
        """
