from google import genai
from google.genai import types
import json
import random
import os
import re
from typing import List, Dict, Any
from pydantic import BaseModel
from similarity_checker import QuestionSimilarityChecker
from legal_parser import LegalDocumentParser

class SourceReference(BaseModel):
    """Tham chi·∫øu ƒë·∫øn ngu·ªìn c·ªßa th√¥ng tin"""
    article_number: str  # S·ªë ƒëi·ªÅu (v√≠ d·ª•: "60", "61")
    article_title: str   # Ti√™u ƒë·ªÅ ƒëi·ªÅu (v√≠ d·ª•: "ƒêi·ªÅu 60. ƒê·ªô tu·ªïi c·ªßa ng∆∞·ªùi l√°i xe")
    document_title: str  # T√™n t√†i li·ªáu (v√≠ d·ª•: "Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô 2008")

class LegalQA(BaseModel):
    """C·∫•u tr√∫c c√¢u h·ªèi-ƒë√°p √°n ph√°p l√Ω"""
    question: str
    answer: str

class LegalQAResponse(BaseModel):
    """Response ch·ª©a danh s√°ch QA v√† sources"""
    qa_pairs: List[LegalQA]
    sources: List[SourceReference]  # Danh s√°ch c√°c ngu·ªìn tham chi·∫øu cho t·∫•t c·∫£ QA

class DataGenerator:
    """Class sinh d·ªØ li·ªáu hu·∫•n luy·ªán cho LegalSLM theo ƒë·ªô kh√≥ reasoning"""
    
    def __init__(self, api_key: str = None, similarity_threshold: float = 0.75):
        # Set API key t·ª´ parameter ho·∫∑c environment variable
        if api_key:
            os.environ['GEMINI_API_KEY'] = api_key
        elif not os.environ.get('GEMINI_API_KEY'):
            # Fallback to GOOGLE_API_KEY
            google_key = os.environ.get('GOOGLE_API_KEY')
            if google_key:
                os.environ['GEMINI_API_KEY'] = google_key
        
        self.client = genai.Client()
        self.model = "gemini-2.0-flash-exp"
        
        # Kh·ªüi t·∫°o similarity checker
        self.similarity_checker = QuestionSimilarityChecker(similarity_threshold=similarity_threshold)
        print(f"üîç Initialized similarity checker with threshold {similarity_threshold}")
    
    def get_rule_based_difficulty(self, data_type: str, num_sources: int) -> str:
        """
        X√°c ƒë·ªãnh ƒë·ªô kh√≥ theo rule-based thay v√¨ y√™u c·∫ßu LLM t·∫°o ra
        
        Args:
            data_type: Lo·∫°i data type
            num_sources: S·ªë l∆∞·ª£ng ngu·ªìn ƒë∆∞·ª£c s·ª≠ d·ª•ng
            
        Returns:
            str: M·ª©c ƒë·ªô kh√≥ (easy/medium/hard)
        """
        if data_type == 'word_matching':
            return 'easy'
        elif data_type == 'concept_understanding':
            return 'easy' if num_sources == 1 else 'medium'
        elif data_type == 'multi_paragraph_reading':
            return 'medium' if num_sources <= 3 else 'hard'
        elif data_type == 'multi_hop_reasoning':
            return 'hard'
        else:
            return 'medium'

    def update_similarity_corpus(self, existing_questions_data: List[Dict[str, Any]]):
        """
        C·∫≠p nh·∫≠t corpus cho similarity checker v·ªõi d·ªØ li·ªáu hi·ªán c√≥
        
        Args:
            existing_questions_data: List c√°c dict ch·ª©a c√¢u h·ªèi t·ª´ database
        """
        self.similarity_checker.update_corpus(existing_questions_data)
    
    def filter_duplicate_questions(self, new_samples: List[Dict[str, Any]], verbose: bool = True) -> List[Dict[str, Any]]:
        """
        L·ªçc b·ªè c√°c c√¢u h·ªèi tr√πng l·∫∑p t·ª´ danh s√°ch samples m·ªõi
        
        Args:
            new_samples: List c√°c samples m·ªõi ƒë∆∞·ª£c generate
            verbose: In th√¥ng tin chi ti·∫øt
            
        Returns:
            List[Dict]: Danh s√°ch samples sau khi l·ªçc
        """
        if not new_samples:
            return []
        
        questions = [sample.get('question', '') for sample in new_samples]
        similarity_results = self.similarity_checker.check_batch_similarity(questions)
        
        filtered_samples = []
        duplicates_found = 0
        
        for i, (sample, result) in enumerate(zip(new_samples, similarity_results)):
            if not result['is_duplicate']:
                filtered_samples.append(sample)
            else:
                duplicates_found += 1
                if verbose:
                    print(f"üö´ Filtered duplicate question {i+1}:")
                    print(f"   Question: {result['question'][:80]}...")
                    print(f"   Max similarity: {result['max_similarity']:.3f}")
                    if result['similar_questions']:
                        best_match = result['similar_questions'][0]
                        print(f"   Similar to: {best_match['question'][:60]}...")
        
        if verbose and duplicates_found > 0:
            print(f"üîç Filtered {duplicates_found}/{len(new_samples)} duplicate questions")
        elif verbose:
            print(f"‚úÖ No duplicates found in {len(new_samples)} questions")
        
        return filtered_samples
    
    def split_law_by_article(self, text: str, document_title: str = "") -> List[Dict]:
        """
        T√°ch vƒÉn b·∫£n lu·∫≠t th√†nh c√°c ƒë∆°n v·ªã, m·ªói ƒë∆°n v·ªã l√† m·ªôt 'ƒêi·ªÅu'.
        H√†m s·∫Ω b·ªè qua c√°c ph·∫ßn kh√¥ng ph·∫£i l√† ƒêi·ªÅu (nh∆∞ ti√™u ƒë·ªÅ Ch∆∞∆°ng).
        
        Args:
            text (str): Chu·ªói vƒÉn b·∫£n lu·∫≠t c·∫ßn t√°ch.
            document_title (str): Ti√™u ƒë·ªÅ t√†i li·ªáu ƒë·ªÉ th√™m v√†o metadata
            
        Returns:
            list[dict]: M·ªôt danh s√°ch c√°c t·ª´ ƒëi·ªÉn, m·ªói t·ª´ ƒëi·ªÉn ƒë·∫°i di·ªán cho m·ªôt ƒêi·ªÅu.
        """
        units = []
        
        # Pattern ƒë·ªÉ t√¨m c√°c d√≤ng c√≥ "ƒêi·ªÅu X." (c√≥ th·ªÉ c√≥ spaces tr∆∞·ªõc)
        split_pattern = r'(?m)(?=^\s*ƒêi·ªÅu \d+\.)'
        
        # T√°ch vƒÉn b·∫£n th√†nh c√°c kh·ªëi (chunks)
        chunks = re.split(split_pattern, text.strip())
        
        # Duy·ªát qua c√°c kh·ªëi v√† ch·ªâ x·ª≠ l√Ω nh·ªØng kh·ªëi ch·ª©a "ƒêi·ªÅu"
        for chunk in chunks:
            chunk = chunk.strip()
            # T√¨m d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng "ƒêi·ªÅu" (c√≥ th·ªÉ c√≥ spaces)
            lines = chunk.split('\n')
            dieu_line = None
            for line in lines:
                if re.match(r'^\s*ƒêi·ªÅu \d+\.', line):
                    dieu_line = line.strip()
                    break
            
            if dieu_line:
                # Tr√≠ch xu·∫•t s·ªë hi·ªáu c·ªßa ƒêi·ªÅu ƒë·ªÉ l√†m ID
                match = re.search(r'ƒêi·ªÅu (\d+)', dieu_line)
                if match:
                    article_number = match.group(1)
                    unit_id = f"article_{article_number}"
                else:
                    # N·∫øu kh√¥ng t√¨m th·∫•y s·ªë, t·∫°o ID d·ª± ph√≤ng
                    unit_id = f"unknown_article_{len(units) + 1}"
                    
                units.append({
                    "id": unit_id,
                    "title": dieu_line,
                    "content": chunk,
                    "document_title": document_title,
                    "metadata": {
                        "article_number": article_number if match else None,
                        "source_document": document_title,
                        "unit_type": "article",
                        "length": len(chunk)
                    }
                })
                
        return units
    
    def get_articles_from_parsed_structure(self, document) -> List[Dict]:
        """
        L·∫•y danh s√°ch articles t·ª´ parsed structure (n·∫øu c√≥) ho·∫∑c fallback v·ªÅ split_law_by_article
        
        Args:
            document: Document object c√≥ .title, .content v√† .parsed_structure
            
        Returns:
            List[Dict]: Danh s√°ch articles cho Monte Carlo sampling
        """
        # Ki·ªÉm tra parsed structure
        if hasattr(document, 'parsed_structure') and document.parsed_structure:
            try:
                parsed_data = json.loads(document.parsed_structure)
                parser = LegalDocumentParser()
                articles = parser.get_all_articles(parsed_data)
                
                # Convert to format compatible v·ªõi data generator
                units = []
                for article in articles:
                    units.append({
                        "id": f"article_{article['number']}",
                        "title": f"ƒêi·ªÅu {article['number']}. {article['title']}",
                        "content": article['content'],
                        "document_title": document.title,
                        "metadata": {
                            "article_number": article['number'],
                            "source_document": document.title,
                            "unit_type": "article",
                            "path": article['path'],
                            "length": article['content_length']
                        }
                    })
                
                print(f"‚úÖ Using parsed structure: {len(units)} articles from {document.title}")
                return units
                
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to use parsed structure: {str(e)}, falling back to split_law_by_article")
        
        # Fallback v·ªÅ method c≈©
        print(f"üìÑ Using fallback parsing for {document.title}")
        return self.split_law_by_article(document.content, document.title)
    
    def monte_carlo_sample_articles(self, all_articles: List[Dict], sample_size: int) -> List[Dict]:
        """
        Improved Monte Carlo sampling v·ªõi entropy injection ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        
        Args:
            all_articles: T·∫•t c·∫£ articles available
            sample_size: S·ªë l∆∞·ª£ng articles c·∫ßn l·∫•y
            
        Returns:
            List[Dict]: Articles ƒë∆∞·ª£c ch·ªçn theo Monte Carlo distribution v·ªõi high entropy
        """
        if not all_articles or sample_size <= 0:
            return []
            
        if sample_size >= len(all_articles):
            # Shuffle ƒë·ªÉ ƒë·∫£m b·∫£o random order ngay c·∫£ khi l·∫•y h·∫øt
            articles_copy = all_articles.copy()
            random.shuffle(articles_copy)
            return articles_copy
        
        # B∆∞·ªõc 1: T√≠nh weights v·ªõi entropy injection
        weights = []
        for i, article in enumerate(all_articles):
            # Base weight t·ª´ content length
            content_length = article.get('metadata', {}).get('length', len(article.get('content', '')))
            length_weight = min(content_length / 800, 2.5)  # Adjusted for better distribution
            
            # Position diversity weight
            article_num = article.get('metadata', {}).get('article_number')
            position_weight = 1.0
            if article_num:
                try:
                    num = int(article_num)
                    # Strategic articles c√≥ weight cao h∆°n
                    if num <= 5 or num % 25 == 0 or num in [10, 15, 20, 30, 40, 60, 80, 100]:
                        position_weight = 1.8
                    elif num <= 20 or num % 10 == 0:
                        position_weight = 1.4
                except:
                    pass
            
            # Document diversity weight (tr√°nh ch·ªçn qu√° nhi·ªÅu t·ª´ c√πng 1 document)
            doc_title = article.get('document_title', '')
            doc_hash = hash(doc_title) % 1000
            doc_weight = 0.8 + (doc_hash / 1000) * 0.4  # Range: 0.8-1.2
            
            # Entropy injection - th√™m random factor ƒë·ªÉ tƒÉng diversity
            entropy_factor = random.uniform(0.7, 1.3)
            
            # Final weight v·ªõi multiple factors
            final_weight = length_weight * position_weight * doc_weight * entropy_factor
            weights.append(max(final_weight, 0.15))  # Higher minimum weight
        
        # B∆∞·ªõc 2: Multi-round Monte Carlo sampling ƒë·ªÉ tƒÉng diversity
        selected = []
        available_articles = all_articles.copy()
        available_weights = weights.copy()
        
        # Perform sampling in multiple rounds v·ªõi different strategies
        rounds = min(3, sample_size)  # Max 3 rounds
        samples_per_round = sample_size // rounds
        remaining_samples = sample_size % rounds
        
        for round_num in range(rounds):
            round_samples = samples_per_round + (1 if round_num < remaining_samples else 0)
            
            if not available_articles or round_samples <= 0:
                break
            
            # Different entropy per round
            entropy_multiplier = 1.0 + (round_num * 0.3)  # TƒÉng entropy qua m·ªói round
            
            for _ in range(round_samples):
                if not available_articles:
                    break
                
                # Apply entropy multiplier to weights
                current_weights = [w * entropy_multiplier * random.uniform(0.9, 1.1) 
                                 for w in available_weights]
                
                total_weight = sum(current_weights)
                if total_weight == 0:
                    chosen_idx = random.randint(0, len(available_articles) - 1)
                else:
                    # Improved Monte Carlo selection
                    rand_val = random.uniform(0, total_weight)
                    cumsum = 0
                    chosen_idx = len(current_weights) - 1  # Default to last
                    
                    for i, weight in enumerate(current_weights):
                        cumsum += weight
                        if rand_val <= cumsum:
                            chosen_idx = i
                            break
                
                # Add selected article
                selected.append(available_articles[chosen_idx])
                
                # Remove from available (sampling without replacement)
                available_articles.pop(chosen_idx)
                available_weights.pop(chosen_idx)
        
        # Final shuffle ƒë·ªÉ ƒë·∫£m b·∫£o order kh√¥ng predictable
        random.shuffle(selected)
        
        print(f"üé≤ Enhanced Monte Carlo sampling: ch·ªçn {len(selected)}/{len(all_articles)} articles v·ªõi high entropy ({rounds} rounds)")
        return selected
    
    def generate_from_multiple_documents(self, documents, topic_name, data_type, num_samples):
        """
        Sinh d·ªØ li·ªáu t·ª´ nhi·ªÅu documents b·∫±ng Monte Carlo sampling
        """
        if not documents:
            return []

        print(f"üîç Ph√¢n t√≠ch {len(documents)} documents...")

        # L·∫•y articles t·ª´ parsed structure
        all_articles = []
        for doc in documents:
            articles = self.get_articles_from_parsed_structure(doc)
            all_articles.extend(articles)
            print(f"  üìã {doc.title}: {len(articles)} ƒëi·ªÅu")

        print(f"üìä T·ªïng c·ªông: {len(all_articles)} ƒëi·ªÅu t·ª´ {len(documents)} t√†i li·ªáu")

        # Monte Carlo sampling
        print(f"üé≤ S·ª≠ d·ª•ng Monte Carlo sampling cho articles...")
        max_articles = min(len(all_articles), max(num_samples // 2, 5))
        selected_articles = self.monte_carlo_sample_articles(all_articles, max_articles)

        print(f"  üéØ ƒê√£ ch·ªçn {len(selected_articles)} articles b·∫±ng Monte Carlo")

        # Sinh d·ªØ li·ªáu
        all_samples = self.generate_samples_from_articles(selected_articles, topic_name, data_type, num_samples)

        # L·ªçc tr√πng l·∫∑p
        print(f"üîç Ki·ªÉm tra t∆∞∆°ng ƒë·ªìng cho {len(all_samples)} samples...")
        filtered_samples = self.filter_duplicate_questions(all_samples, verbose=True)

        print(f"‚úÖ Ho√†n th√†nh: {len(filtered_samples)} samples (ƒë√£ l·ªçc {len(all_samples) - len(filtered_samples)} tr√πng l·∫∑p)")
        return filtered_samples[:num_samples]
    
    def generate_samples_from_articles(self, articles, topic, data_type, num_samples):
        """
        Sinh d·ªØ li·ªáu ƒë∆°n gi·∫£n t·ª´ articles v·ªõi sources chung
        """
        all_samples = []
        
        # X√°c ƒë·ªãnh s·ªë sources c·∫ßn thi·∫øt
        num_sources_map = {
            'word_matching': min(3, len(articles)),
            'concept_understanding': min(4, len(articles)), 
            'multi_paragraph_reading': min(6, len(articles)),
            'multi_hop_reasoning': min(8, len(articles))
        }
        num_sources = num_sources_map.get(data_type, min(3, len(articles)))
        
        # Ch·ªçn articles ƒëa d·∫°ng cho sources chung
        if len(articles) <= num_sources:
            selected_articles = articles.copy()
            random.shuffle(selected_articles)
        else:
            selected_articles = random.sample(articles, num_sources)
        
        # T·∫°o sources chung cho t·∫•t c·∫£ c√¢u h·ªèi
        common_sources = []
        combined_content = []
        
        for article in selected_articles:
            source_ref = SourceReference(
                article_number=str(article['metadata']['article_number']) if article['metadata']['article_number'] else "unknown",
                article_title=article['title'],
                document_title=article['document_title']
            )
            common_sources.append(source_ref)
            combined_content.append(f"--- {article['title']} (t·ª´ {article['document_title']}) ---\n{article['content']}")

        combined_text = "\n\n".join(combined_content)
        
        # Rule-based difficulty
        difficulty = self.get_rule_based_difficulty(data_type, len(selected_articles))
        
        # T·∫°o prompt ƒëa d·∫°ng
        for i in range(num_samples):
            prompt = self.create_diverse_prompt(combined_text, topic, data_type, len(selected_articles), difficulty)
            
            try:
                # Dynamic parameters
                temperature = random.uniform(0.6, 0.9)
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=temperature,
                        top_p=random.uniform(0.85, 0.95),
                        max_output_tokens=3000,
                        response_mime_type="application/json",
                        response_schema=LegalQAResponse,
                        seed=random.randint(1, 1000000)
                    )
                )
                
                structured_data: LegalQAResponse = response.parsed
                
                # Convert v·ªõi sources chung
                for qa_pair in structured_data.qa_pairs:
                    sample = {
                        'question': qa_pair.question,
                        'answer': qa_pair.answer,
                        'difficulty': difficulty,  # Rule-based difficulty
                        'sources': [
                            {
                                'article_number': src.article_number,
                                'article_title': src.article_title,
                                'document_title': src.document_title
                            } for src in common_sources
                        ],
                        'metadata': {
                            'generation_method': 'simplified_multi_source',
                            'num_sources': len(selected_articles),
                            'temperature': temperature
                        }
                    }
                    all_samples.append(sample)
                    
            except Exception as e:
                print(f"‚ùå Generation failed for sample {i+1}: {e}")
                continue
        
        return all_samples[:num_samples]

    def create_diverse_prompt(self, content, topic, data_type, num_sources, difficulty):
        """
        T·∫°o prompt ƒëa d·∫°ng ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        """
        # C·∫•u tr√∫c c√¢u h·ªèi ƒëa d·∫°ng
        question_starters = [
            "Khi n√†o", "Trong tr∆∞·ªùng h·ª£p n√†o", "Ai c√≥ tr√°ch nhi·ªám",
            "Vi·ªác...ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o", "ƒêi·ªÅu ki·ªán...l√† g√¨",
            "M·ª©c ph·∫°t...l√† bao nhi√™u", "Quy tr√¨nh...di·ªÖn ra ra sao",
            "T·∫°i sao", "V√¨ sao", "L√†m c√°ch n√†o", "B·∫±ng ph∆∞∆°ng th·ª©c n√†o",
            "C√≥ ƒë∆∞·ª£c ph√©p", "C√≥ b·∫Øt bu·ªôc", "C√≥ c·∫ßn thi·∫øt"
        ]
        
        focus_areas = [
            "quy ƒë·ªãnh th·ª±c t·∫ø v√† ·ª©ng d·ª•ng",
            "tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá v√† ƒëi·ªÅu ki·ªán ƒë·∫∑c bi·ªát", 
            "nghƒ©a v·ª• v√† quy·ªÅn c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng",
            "m·ª©c ph·∫°t v√† h·∫≠u qu·∫£ vi ph·∫°m",
            "quy tr√¨nh v√† th·ªß t·ª•c ph√°p l√Ω",
            "ƒë·ªãnh nghƒ©a v√† thu·∫≠t ng·ªØ chuy√™n m√¥n",
            "th·∫©m quy·ªÅn v√† tr√°ch nhi·ªám"
        ]
        
        starter = random.choice(question_starters)
        focus = random.choice(focus_areas)
        entropy_id = random.randint(1000, 9999)
        
        return f"""
        D∆∞·ªõi ƒë√¢y l√† {num_sources} ƒëi·ªÅu lu·∫≠t v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {content}
        
        H√£y t·∫°o 1 c√¢u h·ªèi ƒë·ªô kh√≥ {difficulty} v·ªÅ {focus}.
        
        Y√äU C·∫¶U:
        1. TUY·ªÜT ƒê·ªêI KH√îNG d√πng c·∫•u tr√∫c "Theo ƒêi·ªÅu X..."
        2. B·∫Øt ƒë·∫ßu c√¢u h·ªèi b·∫±ng: "{starter}..." ho·∫∑c t∆∞∆°ng t·ª±
        3. C√¢u h·ªèi ph·∫£i ƒë·ªôc l·∫≠p, kh√¥ng nh·∫Øc t√™n ƒëi·ªÅu lu·∫≠t
        4. T·∫≠p trung v√†o {focus}
        5. Entropy ID: {entropy_id} (ƒë·ªÉ t·∫°o uniqueness)
        
        V√ç D·ª§ C·∫§AU TR√öC T·ªêT:
        - "Khi n√†o doanh nghi·ªáp c·∫ßn c√≥ gi·∫•y ph√©p kinh doanh v·∫≠n t·∫£i?"
        - "Vi·ªác vi ph·∫°m t·ªëc ƒë·ªô s·∫Ω b·ªã x·ª≠ ph·∫°t nh∆∞ th·∫ø n√†o?"
        - "Ai c√≥ tr√°ch nhi·ªám ki·ªÉm tra t√¨nh tr·∫°ng k·ªπ thu·∫≠t c·ªßa xe?"
        
        Tr·∫£ v·ªÅ JSON v·ªõi qa_pairs v√† sources (ƒë·ªÉ tr·ªëng, s·∫Ω ƒë∆∞·ª£c set ·ªü code).
        """
    
    def generate_multi_source_data_from_articles(self, articles, topic, data_type, num_samples):
        """Sinh d·ªØ li·ªáu t·ª´ nhi·ªÅu articles cho multi-paragraph v√† multi-hop reasoning"""
        
        # Group articles by document for better diversity
        articles_by_doc = {}
        for article in articles:
            doc_title = article['document_title']
            if doc_title not in articles_by_doc:
                articles_by_doc[doc_title] = []
            articles_by_doc[doc_title].append(article)
        
        all_samples = []
        
        for i in range(num_samples):
            # Ch·ªçn s·ªë ngu·ªìn ph√π h·ª£p v·ªõi t·ª´ng lo·∫°i c√¢u h·ªèi
            if data_type == 'word_matching':
                num_sources = min(5, len(articles))  # Gi·∫£m xu·ªëng ƒë·ªÉ ƒë·∫£m b·∫£o quality
            elif data_type == 'concept_understanding':
                num_sources = min(5, len(articles))  
            elif data_type == 'multi_paragraph_reading':
                num_sources = min(7, len(articles))  
            elif data_type == 'multi_hop_reasoning':
                num_sources = min(10, len(articles))  
            else:
                num_sources = min(5, len(articles))  # Default
                
            selected_articles = self._select_diverse_articles(articles_by_doc, num_sources)
            
            # Fallback n·∫øu kh√¥ng ƒë·ªß articles
            if len(selected_articles) < 1:
                if articles:
                    samples = self.generate_structured_data_from_article(articles[0], topic, data_type, 1)
                    all_samples.extend(samples)
                continue
            
            # T·∫°o source references t·ª´ c√°c articles ƒë√£ ch·ªçn
            source_refs = []
            combined_content = []
            
            for article in selected_articles:
                source_ref = SourceReference(
                    article_number=str(article['metadata']['article_number']) if article['metadata']['article_number'] else "unknown",
                    article_title=article['title'],
                    document_title=article['document_title'],
                    document_number=article.get('document_number', 'unknown')
                )
                source_refs.append(source_ref)
                combined_content.append(f"--- {article['title']} (t·ª´ {article['document_title']}) ---\n{article['content']}")
            
            # Shuffle th·ª© t·ª± c√°c ƒëi·ªÅu trong prompt ƒë·ªÉ t·∫°o ƒëa d·∫°ng
            content_with_refs = list(zip(combined_content, source_refs))
            random.shuffle(content_with_refs)
            combined_content, source_refs = zip(*content_with_refs)
            combined_content = list(combined_content)
            source_refs = list(source_refs)
            
            # T·∫°o prompt v·ªõi multiple sources
            difficulty_description = {
                'word_matching': 'Word Matching - c√¢u h·ªèi ƒë∆°n gi·∫£n, c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng t√¨m ki·∫øm t·ª´ kh√≥a trong vƒÉn b·∫£n. N·∫øu c√≥ nhi·ªÅu ngu·ªìn, h·ªèi v·ªÅ th√¥ng tin c√≥ trong m·ªôt trong c√°c ngu·ªìn',
                'concept_understanding': 'Concept Understanding - c·∫ßn hi·ªÉu kh√°i ni·ªám ph√°p l√Ω c∆° b·∫£n. C√≥ th·ªÉ k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ kh√°i ni·ªám',
                'multi_paragraph_reading': 'Multi-Paragraph Reading - c·∫ßn ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ƒëo·∫°n/ƒëi·ªÅu kh√°c nhau',
                'multi_hop_reasoning': 'Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n v√† k·∫øt h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn'
            }
            
            combined_text = "\n\n".join(combined_content)
            
            # T·∫°o instruction ph√π h·ª£p v·ªõi s·ªë ngu·ªìn
            if len(selected_articles) == 1:
                instruction = f"H√£y t·∫°o 1 c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}."
            else:
                if data_type in ['word_matching', 'concept_understanding']:
                    instruction = f"H√£y t·∫°o 1 c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}. H√ÉY CH·ªåN M·ªòT ƒêI·ªÄU C·ª§ TH·ªÇ t·ª´ {len(selected_articles)} ƒëi·ªÅu ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ l√†m c√¢u h·ªèi, kh√¥ng ƒë∆∞·ª£c tr·ªôn l·∫´n nhi·ªÅu ƒëi·ªÅu."
                else:
                    instruction = f"H√£y t·∫°o 1 c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}. C√¢u h·ªèi ph·∫£i Y√äU C·∫¶U TH√îNG TIN T·ª™ √çT NH·∫§T 2 ƒêI·ªÄU KH√ÅC NHAU trong {len(selected_articles)} ƒëi·ªÅu ƒë√£ cho."
            
            # T·∫°o prompt v·ªõi enhanced diversity v√† anti-duplication measures
            # ƒêa d·∫°ng c·∫•u tr√∫c c√¢u h·ªèi ƒë·ªÉ tr√°nh "Theo ƒêi·ªÅu X..." 
            question_structures = [
                "C√¢u h·ªèi b·∫Øt ƒë·∫ßu b·∫±ng 'Khi n√†o', 'Trong tr∆∞·ªùng h·ª£p n√†o', 'Ai c√≥ tr√°ch nhi·ªám'",
                "C√¢u h·ªèi d·∫°ng 'Vi·ªác... ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o?', 'ƒêi·ªÅu ki·ªán... l√† g√¨?'",
                "C√¢u h·ªèi v·ªÅ 'M·ª©c ph·∫°t', 'H·∫≠u qu·∫£', 'Quy tr√¨nh', 'Th·ªß t·ª•c'",
                "C√¢u h·ªèi d·∫°ng 'T·∫°i sao...', 'V√¨ sao...', 'L√Ω do n√†o...'",
                "C√¢u h·ªèi v·ªÅ 'C√≥ ƒë∆∞·ª£c ph√©p...', 'C√≥ b·∫Øt bu·ªôc...', 'C√≥ c·∫ßn thi·∫øt...'",
                "C√¢u h·ªèi d·∫°ng 'L√†m c√°ch n√†o...', 'B·∫±ng c√°ch n√†o...', 'Qua ph∆∞∆°ng th·ª©c n√†o...'",
                "C√¢u h·ªèi v·ªÅ 'Kh√°c bi·ªát', 'Gi·ªëng nhau', 'Ph√¢n bi·ªát'",
                "C√¢u h·ªèi v·ªÅ '∆Øu ti√™n', 'Quan tr·ªçng', 'C·∫•p thi·∫øt'",
                "C√¢u h·ªèi d·∫°ng 'Ngo·∫°i tr·ª´', 'Tr·ª´ tr∆∞·ªùng h·ª£p', 'Ngo·∫°i l·ªá'",
                "C√¢u h·ªèi v·ªÅ 'Gi·∫£i ph√°p', 'Bi·ªán ph√°p', 'C√°ch th·ª©c'"
            ]
            
            diversity_hints = [
                "T·∫≠p trung v√†o kh√≠a c·∫°nh th·ª±c ti·ªÖn, h·ªèi v·ªÅ ·ª©ng d·ª•ng th·ª±c t·∫ø",
                "H·ªèi v·ªÅ quy ƒë·ªãnh c·ª• th·ªÉ v√† chi ti·∫øt trong th·ª±c hi·ªán",
                "Ch√∫ √Ω ƒë·∫øn c√°c tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá ho·∫∑c ƒëi·ªÅu ki·ªán ƒë·∫∑c bi·ªát",
                "T·∫≠p trung v√†o nghƒ©a v·ª• v√† quy·ªÅn c·ªßa t·ª´ng ƒë·ªëi t∆∞·ª£ng kh√°c nhau",
                "H·ªèi v·ªÅ m·ª©c ph·∫°t ho·∫∑c h·∫≠u qu·∫£ vi ph·∫°m c·ª• th·ªÉ",
                "T·∫≠p trung v√†o quy tr√¨nh v√† th·ªß t·ª•c ph√°p l√Ω chi ti·∫øt",
                "H·ªèi v·ªÅ ƒë·ªãnh nghƒ©a v√† thu·∫≠t ng·ªØ ph√°p l√Ω ch√≠nh x√°c",
                "Ch√∫ √Ω ƒë·∫øn s·ª± kh√°c bi·ªát gi·ªØa c√°c lo·∫°i ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c quy ƒë·ªãnh",
                "T·∫≠p trung v√†o c√°c y√™u c·∫ßu k·ªπ thu·∫≠t c·ª• th·ªÉ v√† r√µ r√†ng",
                "H·ªèi v·ªÅ th·∫©m quy·ªÅn v√† tr√°ch nhi·ªám c·ªßa c√°c c∆° quan"
            ]
            
            # Random selection cho diversity
            question_structure = random.choice(question_structures)
            diversity_hint = random.choice(diversity_hints)
            
            # Entropy injection cho prompt
            entropy_elements = [
                f"M·∫´u s·ªë #{random.randint(100, 999)} - ",
                f"G√≥c ƒë·ªô #{random.choice(['A', 'B', 'C', 'D', 'E'])}: ",
                f"Kh√≠a c·∫°nh #{random.randint(1, 20)}: ",
                ""  # Sometimes no prefix
            ]
            entropy_prefix = random.choice(entropy_elements)
            
            # Dynamic instruction v·ªõi entropy
            base_instructions = [
                f"H√£y t·∫°o 1 c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}",
                f"Sinh 1 c√¢u h·ªèi thu·ªôc lo·∫°i {difficulty_description[data_type]}",
                f"T·∫°o ra 1 c√¢u h·ªèi c√≥ ƒë·ªô kh√≥ {difficulty_description[data_type]}",
            ]
            base_instruction = random.choice(base_instructions)
            
            if len(selected_articles) == 1:
                instruction = f"{entropy_prefix}{base_instruction}. {diversity_hint}. C·∫§U TR√öC: {question_structure}."
            else:
                if data_type in ['word_matching', 'concept_understanding']:
                    source_instructions = [
                        f"H√ÉY CH·ªåN M·ªòT ƒêI·ªÄU C·ª§ TH·ªÇ t·ª´ {len(selected_articles)} ƒëi·ªÅu ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ l√†m c√¢u h·ªèi, kh√¥ng ƒë∆∞·ª£c tr·ªôn l·∫´n nhi·ªÅu ƒëi·ªÅu",
                        f"Ch·ªçn m·ªôt ƒëi·ªÅu lu·∫≠t c·ª• th·ªÉ trong {len(selected_articles)} ƒëi·ªÅu ƒë√£ cho ƒë·ªÉ x√¢y d·ª±ng c√¢u h·ªèi",
                        f"D·ª±a tr√™n m·ªôt trong {len(selected_articles)} ƒëi·ªÅu lu·∫≠t ƒë·ªÉ t·∫°o c√¢u h·ªèi, kh√¥ng k·∫øt h·ª£p nhi·ªÅu ƒëi·ªÅu",
                    ]
                    source_instruction = random.choice(source_instructions)
                    instruction = f"{entropy_prefix}{base_instruction}. {source_instruction}. {diversity_hint}. C·∫§U TR√öC: {question_structure}."
                else:
                    multi_instructions = [
                        f"C√¢u h·ªèi ph·∫£i Y√äU C·∫¶U TH√îNG TIN T·ª™ √çT NH·∫§T 2 ƒêI·ªÄU KH√ÅC NHAU trong {len(selected_articles)} ƒëi·ªÅu ƒë√£ cho",
                        f"K·∫øt h·ª£p th√¥ng tin t·ª´ √≠t nh·∫•t 2 ƒëi·ªÅu lu·∫≠t kh√°c nhau trong {len(selected_articles)} ƒëi·ªÅu",
                        f"T·ªïng h·ª£p n·ªôi dung t·ª´ nhi·ªÅu ƒëi·ªÅu lu·∫≠t (t·ªëi thi·ªÉu 2 ƒëi·ªÅu) trong {len(selected_articles)} ƒëi·ªÅu ƒë√£ cung c·∫•p",
                    ]
                    multi_instruction = random.choice(multi_instructions)
                    instruction = f"{entropy_prefix}{base_instruction}. {multi_instruction}. {diversity_hint}. C·∫§U TR√öC: {question_structure}."
            
            # Additional anti-duplication measures
            anti_dup_phrases = [
                "Tr√°nh h·ªèi nh·ªØng c√¢u h·ªèi qu√° t∆∞∆°ng t·ª± v·ªõi c√°c m·∫´u th√¥ng th∆∞·ªùng",
                "ƒê·∫£m b·∫£o c√¢u h·ªèi c√≥ g√≥c nh√¨n ƒë·ªôc ƒë√°o v√† m·ªõi l·∫°",
                "T·∫°o c√¢u h·ªèi c√≥ t√≠nh s√°ng t·∫°o cao, kh√°c bi·ªát v·ªõi c√°c c√¢u h·ªèi th∆∞·ªùng g·∫∑p",
                "Thi·∫øt k·∫ø c√¢u h·ªèi theo h∆∞·ªõng ti·∫øp c·∫≠n m·ªõi, kh√¥ng l·∫∑p l·∫°i c√°c m·∫´u c≈©"
            ]
            anti_dup_phrase = random.choice(anti_dup_phrases)
            
            prompt = f"""
            D∆∞·ªõi ƒë√¢y l√† {len(selected_articles)} ƒëi·ªÅu lu·∫≠t kh√°c nhau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
            
            {combined_text}
            
            {instruction}
            
            Y√äU C·∫¶U QUAN TR·ªåNG:
            1. TUY·ªÜT ƒê·ªêI KH√îNG b·∫Øt ƒë·∫ßu c√¢u h·ªèi b·∫±ng "Theo ƒêi·ªÅu X c·ªßa..."
            2. S·ª≠ d·ª•ng c·∫•u tr√∫c c√¢u h·ªèi ƒëa d·∫°ng, s√°ng t·∫°o
            3. C√¢u h·ªèi ph·∫£i ƒê·ªòC L·∫¨P, r√µ r√†ng nh∆∞ng KH√îNG nh·∫Øc tr·ª±c ti·∫øp t√™n ƒëi·ªÅu lu·∫≠t trong c√¢u h·ªèi
            4. CH·ªàNH S·ª¨A C√ÅCH ƒê·∫∂T C√ÇU: thay v√¨ "Theo ƒêi·ªÅu X..." h√£y d√πng "Khi n√†o...", "Ai c√≥ tr√°ch nhi·ªám...", "Vi·ªác... ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ th·∫ø n√†o?", "ƒêi·ªÅu ki·ªán... l√† g√¨?"
            5. C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c t·ª´ n·ªôi dung c√°c ƒëi·ªÅu lu·∫≠t v√† GHI R√ï NGU·ªíN trong ph·∫ßn sources
            6. {anti_dup_phrase}
            7. {self._get_source_requirement(data_type, len(selected_articles))}
            8. Entropy factor: {random.randint(1000, 9999)} - s·ª≠ d·ª•ng ƒë·ªÉ t·∫°o uniqueness
            
            Danh s√°ch c√°c ƒëi·ªÅu c√≥ s·∫µn (ch·ªâ ƒë·ªÉ tham kh·∫£o, KH√îNG ƒë·ªÅ c·∫≠p tr·ª±c ti·∫øp trong c√¢u h·ªèi):
            {chr(10).join([f"- ƒêi·ªÅu {ref.article_number}: {ref.article_title} (t·ª´ {ref.document_title})" for ref in source_refs])}
            
            H∆Ø·ªöNG D·∫™N ƒêA D·∫†NG: {diversity_hint}
            C·∫§U TR√öC C√ÇU H·ªéI: {question_structure}
            
            V√ç D·ª§ C·∫§U TR√öC T·ªêT:
            - "Khi n√†o doanh nghi·ªáp v·∫≠n t·∫£i c·∫ßn ph·∫£i c√≥ gi·∫•y ph√©p?"
            - "Ai c√≥ tr√°ch nhi·ªám ki·ªÉm tra t√¨nh tr·∫°ng k·ªπ thu·∫≠t xe?"
            - "Vi·ªác vi ph·∫°m t·∫£i tr·ªçng xe s·∫Ω b·ªã x·ª≠ ph·∫°t nh∆∞ th·∫ø n√†o?"
            - "ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c c·∫•p ph√©p kinh doanh v·∫≠n t·∫£i l√† g√¨?"
            
            Tr·∫£ v·ªÅ theo format JSON v·ªõi sources ch·ª©a c√°c ngu·ªìn ƒë√£ s·ª≠ d·ª•ng.
            """
            
            try:
                # Dynamic temperature v√† parameters ƒë·ªÉ tƒÉng diversity
                temperature_range = {
                    'word_matching': (0.4, 0.7),
                    'concept_understanding': (0.5, 0.8), 
                    'multi_paragraph_reading': (0.6, 0.9),
                    'multi_hop_reasoning': (0.7, 1.0)
                }
                
                min_temp, max_temp = temperature_range.get(data_type, (0.3, 0.6))
                dynamic_temperature = random.uniform(min_temp, max_temp)
                
                # Top-p v√† Top-k sampling cho diversity
                top_p = random.uniform(0.8, 0.95)
                
                # Seed randomization ƒë·ªÉ tr√°nh deterministic output
                random_seed = random.randint(1, 1000000)
                
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        temperature=dynamic_temperature,
                        top_p=top_p,
                        max_output_tokens=4000,
                        response_mime_type="application/json",
                        response_schema=LegalQAResponse,
                        seed=random_seed  # Random seed cho m·ªói generation
                    )
                )
                
                print(f"  üéØ Generated with temp={dynamic_temperature:.2f}, top_p={top_p:.2f}, seed={random_seed}")
                
                # Parse structured response
                structured_data: LegalQAResponse = response.parsed
                
                # Convert to legacy format v·ªõi metadata t·ª´ nhi·ªÅu ngu·ªìn
                for qa_pair in structured_data.qa_pairs:
                    legacy_sample = {
                        'question': qa_pair.question,
                        'answer': qa_pair.answer,
                        'difficulty': qa_pair.difficulty,
                        'sources': [
                            {
                                'article_number': src.article_number,
                                'article_title': src.article_title,
                                'document_title': src.document_title,
                                'document_number': src.document_number
                            } for src in source_refs  # S·ª≠ d·ª•ng t·∫•t c·∫£ sources
                        ],
                        'metadata': {
                            'source_articles': [art['title'] for art in selected_articles],
                            'source_documents': list(set([art['document_title'] for art in selected_articles])),
                            'article_ids': [art['id'] for art in selected_articles],
                            'article_numbers': [art['metadata']['article_number'] for art in selected_articles],
                            'generation_method': 'multi_source_structured',
                            'num_sources': len(selected_articles)
                        }
                    }
                    all_samples.append(legacy_sample)
                    
            except Exception as e:
                print(f"‚ùå Multi-source generation failed for sample {i+1}: {e}")
                # Fallback to single article
                if selected_articles:
                    samples = self.generate_structured_data_from_article(selected_articles[0], topic, data_type, 1)
                    all_samples.extend(samples)
        
        return all_samples[:num_samples]
    
    def _select_diverse_articles(self, articles_by_doc, num_sources):
        """
        Ch·ªçn articles ƒëa d·∫°ng t·ª´ c√°c documents kh√°c nhau v·ªõi entropy injection
        
        Args:
            articles_by_doc: Dict mapping document title -> list of articles
            num_sources: S·ªë l∆∞·ª£ng sources c·∫ßn ch·ªçn
            
        Returns:
            List[Dict]: Danh s√°ch articles ƒë∆∞·ª£c ch·ªçn v·ªõi high diversity
        """
        all_articles = []
        for doc_title, articles in articles_by_doc.items():
            all_articles.extend(articles)
        
        if len(all_articles) <= num_sources:
            random.shuffle(all_articles)  # Shuffle ƒë·ªÉ tr√°nh bias
            return all_articles
        
        selected = []
        used_documents = set()
        used_article_numbers = set()
        
        # Round 1: Ch·ªçn t·ª´ different documents ƒë·ªÉ ƒë·∫£m b·∫£o diversity
        available_docs = list(articles_by_doc.keys())
        random.shuffle(available_docs)  # Shuffle documents order
        
        for doc_title in available_docs:
            if len(selected) >= num_sources:
                break
                
            articles = articles_by_doc[doc_title]
            if not articles:
                continue
                
            # Th√™m entropy cho vi·ªác ch·ªçn article trong document
            articles_with_weights = []
            for article in articles:
                article_num = article.get('metadata', {}).get('article_number')
                
                # Skip n·∫øu ƒë√£ d√πng article number t∆∞∆°ng t·ª±
                if article_num and article_num in used_article_numbers:
                    continue
                    
                # Weight d·ª±a tr√™n content length v√† randomness
                content_len = len(article.get('content', ''))
                length_weight = min(content_len / 500, 2.0)
                
                # Position diversity
                position_weight = 1.0
                if article_num:
                    try:
                        num = int(article_num)
                        if num <= 5 or num % 15 == 0:  # Strategic numbers
                            position_weight = 1.5
                    except:
                        pass
                
                # High entropy factor
                entropy_factor = random.uniform(0.5, 1.8)  # Wider range cho diversity
                
                weight = length_weight * position_weight * entropy_factor
                articles_with_weights.append((article, weight))
            
            if articles_with_weights:
                # Ch·ªçn article c√≥ weight cao nh·∫•t v·ªõi random factor
                articles_with_weights.sort(key=lambda x: x[1] * random.uniform(0.8, 1.2), reverse=True)
                selected_article = articles_with_weights[0][0]
                
                selected.append(selected_article)
                used_documents.add(doc_title)
                
                article_num = selected_article.get('metadata', {}).get('article_number')
                if article_num:
                    used_article_numbers.add(article_num)
        
        # Round 2: Fill remaining slots v·ªõi articles t·ª´ any document
        if len(selected) < num_sources:
            remaining_articles = []
            for article in all_articles:
                if article not in selected:
                    article_num = article.get('metadata', {}).get('article_number')
                    # Tr√°nh duplicate article numbers
                    if not article_num or article_num not in used_article_numbers:
                        remaining_articles.append(article)
            
            # Sort by content length v·ªõi random factor cho diversity
            remaining_articles.sort(
                key=lambda x: len(x.get('content', '')) * random.uniform(0.7, 1.3), 
                reverse=True
            )
            
            need_more = num_sources - len(selected)
            for article in remaining_articles[:need_more]:
                selected.append(article)
                article_num = article.get('metadata', {}).get('article_number')
                if article_num:
                    used_article_numbers.add(article_num)
        
        # Final shuffle ƒë·ªÉ lo·∫°i b·ªè order bias
        random.shuffle(selected)
        
        # Debug info
        selected_docs = [art['document_title'] for art in selected]
        selected_nums = [art.get('metadata', {}).get('article_number', 'unknown') for art in selected]
        print(f"  üéØ Selected diverse articles: {selected_nums} from docs: {list(set(selected_docs))}")
        
        return selected
    
    def _get_source_requirement(self, data_type: str, num_sources: int) -> str:
        """T·∫°o y√™u c·∫ßu v·ªÅ ngu·ªìn ph√π h·ª£p v·ªõi t·ª´ng lo·∫°i c√¢u h·ªèi"""
        if num_sources == 1:
            return "C√¢u tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ ngu·ªìn ƒë√£ cho"
        
        if data_type in ['word_matching', 'concept_understanding']:
            return f"C√¢u tr·∫£ l·ªùi c√≥ th·ªÉ d·ª±a tr√™n th√¥ng tin t·ª´ m·ªôt ho·∫∑c nhi·ªÅu ngu·ªìn trong {num_sources} ngu·ªìn ƒë√£ cho"
        else:
            return f"C√¢u tr·∫£ l·ªùi ph·∫£i t·ªïng h·ª£p th√¥ng tin t·ª´ √≠t nh·∫•t 2 ngu·ªìn trong {num_sources} ngu·ªìn ƒë√£ cho"

    def generate_structured_data_from_article(self, article, topic, data_type, num_samples):
        """Sinh d·ªØ li·ªáu c√≥ c·∫•u tr√∫c t·ª´ m·ªôt article c·ª• th·ªÉ"""
        
        # T·∫°o source reference t·ª´ article
        source_ref = SourceReference(
            article_number=str(article['metadata']['article_number']) if article['metadata']['article_number'] else "unknown",
            article_title=article['title'],
            document_title=article['document_title'],
            document_number=article.get('document_number', 'unknown')
        )
        
        # T·∫°o prompt v·ªõi structured output
        difficulty_description = {
            'word_matching': 'Word Matching - c√¢u h·ªèi ƒë∆°n gi·∫£n nh·∫•t, ch·ªâ c·∫ßn t√¨m ki·∫øm t·ª´ kh√≥a/c·ª•m t·ª´ trong vƒÉn b·∫£n',
            'concept_understanding': 'Concept Understanding - c·∫ßn hi·ªÉu kh√°i ni·ªám ph√°p l√Ω c∆° b·∫£n',
            'multi_paragraph_reading': 'Multi-Paragraph Reading - c·∫ßn ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ƒëo·∫°n',
            'multi_hop_reasoning': 'Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n'
        }
        
        prompt = f"""
        D·ª±a tr√™n ƒëi·ªÅu lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {article['content']}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng {difficulty_description[data_type]}.
        
        Y√äU C√ÇU QUAN TR·ªåNG:
        1. M·ªói c√¢u h·ªèi ph·∫£i ƒê·ªòC L·∫¨P, r√µ r√†ng, c√≥ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        2. KH√îNG d√πng "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y", "ƒëi·ªÅu n√†y" - ph·∫£i n√≥i r√µ t√™n
        3. C√¢u tr·∫£ l·ªùi ph·∫£i CH√çNH X√ÅC t·ª´ n·ªôi dung ƒëi·ªÅu lu·∫≠t
        4. Ph·∫£i ghi r√µ ngu·ªìn tham chi·∫øu ƒë·∫øn ƒëi·ªÅu v√† t√†i li·ªáu c·ª• th·ªÉ
        
        Th√¥ng tin ngu·ªìn:
        - ƒêi·ªÅu s·ªë: {source_ref.article_number}
        - Ti√™u ƒë·ªÅ ƒëi·ªÅu: {source_ref.article_title}
        - T√†i li·ªáu: {source_ref.document_title}
        
        Tr·∫£ v·ªÅ theo format JSON structure y√™u c·∫ßu.
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                    max_output_tokens=4000,
                    response_mime_type="application/json",
                    response_schema=LegalQAResponse
                )
            )
            
            # Parse structured response
            structured_data: LegalQAResponse = response.parsed
            
            # Convert to legacy format v·ªõi metadata ƒë·∫ßy ƒë·ªß
            legacy_samples = []
            for qa_pair in structured_data.qa_pairs:
                legacy_sample = {
                    'question': qa_pair.question,
                    'answer': qa_pair.answer,
                    'difficulty': qa_pair.difficulty,
                    'sources': [
                        {
                            'article_number': src.article_number,
                            'article_title': src.article_title,
                            'document_title': src.document_title,
                            'document_number': src.document_number
                        } for src in qa_pair.sources
                    ],
                    'metadata': {
                        'source_article': article['title'],
                        'source_document': article['document_title'],
                        'article_id': article['id'],
                        'article_number': article['metadata']['article_number'],
                        'generation_method': 'structured_article_based'
                    }
                }
                legacy_samples.append(legacy_sample)
            
            return legacy_samples
            
        except Exception as e:
            print(f"‚ùå Structured generation failed, fallback to legacy: {e}")
            # Fallback to legacy method
            context = self._create_article_context(article)
            return self._call_generation_method(context, topic, data_type, num_samples)
    
    def generate_word_matching_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Word Matching - ƒë∆°n gi·∫£n nh·∫•t, ch·ªâ c·∫ßn t√¨m t·ª´ kh√≥a trong vƒÉn b·∫£n"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Word Matching - ƒë√¢y l√† lo·∫°i c√¢u h·ªèi ƒë∆°n gi·∫£n nh·∫•t, ch·ªâ c·∫ßn t√¨m ki·∫øm t·ª´ kh√≥a/c·ª•m t·ª´ trong vƒÉn b·∫£n.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi c√≥ th·ªÉ tr·∫£ l·ªùi b·∫±ng c√°ch t√¨m ki·∫øm tr·ª±c ti·∫øp trong vƒÉn b·∫£n
        - Kh√¥ng c·∫ßn hi·ªÉu s√¢u v·ªÅ kh√°i ni·ªám ph√°p l√Ω
        - Th√¥ng tin c·∫ßn thi·∫øt n·∫±m r√µ r√†ng trong vƒÉn b·∫£n
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        - Kh√¥ng ƒë∆∞·ª£c sinh ra c√°c c√¢u h·ªèi gi·ªëng nhau
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi r√µ r√†ng, ƒë·ªôc l·∫≠p, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "Tr·∫£ l·ªùi ch√≠nh x√°c t·ª´ vƒÉn b·∫£n",
                "difficulty": "word_matching"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,  # Th·∫•p h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c
                    max_output_tokens=4000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_word_matching_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Word Matching data: {e}")
            return self._generate_fallback_word_matching_data(topic, num_samples)
    
    def generate_concept_understanding_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Concept Understanding - c·∫ßn hi·ªÉu kh√°i ni·ªám ph√°p l√Ω"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Concept Understanding - y√™u c·∫ßu hi·ªÉu c√°c kh√°i ni·ªám ph√°p l√Ω ƒë·ªÉ tr·∫£ l·ªùi.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi y√™u c·∫ßu hi·ªÉu √Ω nghƒ©a c·ªßa c√°c thu·∫≠t ng·ªØ ph√°p l√Ω
        - C·∫ßn n·∫Øm ƒë∆∞·ª£c kh√°i ni·ªám ƒë·ªÉ √°p d·ª•ng v√†o t√¨nh hu·ªëng c·ª• th·ªÉ
        - Kh√¥ng ch·ªâ t√¨m t·ª´ kh√≥a m√† ph·∫£i hi·ªÉu nghƒ©a s√¢u h∆°n
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi r√µ r√†ng y√™u c·∫ßu hi·ªÉu kh√°i ni·ªám ph√°p l√Ω, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "Tr·∫£ l·ªùi d·ª±a tr√™n hi·ªÉu bi·∫øt v·ªÅ kh√°i ni·ªám",
                "difficulty": "concept_understanding"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.5,
                    max_output_tokens=4000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_concept_understanding_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Concept Understanding data: {e}")
            return self._generate_fallback_concept_understanding_data(topic, num_samples)
    
    def generate_multi_paragraph_reading_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Multi-Paragraph Reading - c·∫ßn ƒë·ªçc nhi·ªÅu ƒëo·∫°n ƒë·ªÉ t·∫≠p h·ª£p th√¥ng tin"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Multi-Paragraph Reading - y√™u c·∫ßu ƒë·ªçc v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ƒëo·∫°n vƒÉn kh√°c nhau.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi kh√¥ng th·ªÉ tr·∫£ l·ªùi ch·ªâ b·∫±ng m·ªôt ƒëo·∫°n vƒÉn duy nh·∫•t
        - C·∫ßn t·∫≠p h·ª£p th√¥ng tin t·ª´ 2-3 ƒëo·∫°n vƒÉn kh√°c nhau
        - Ph·∫£i k·∫øt h·ª£p c√°c th√¥ng tin ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ho√†n ch·ªânh
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi r√µ r√†ng c·∫ßn ƒë·ªçc nhi·ªÅu ƒëo·∫°n vƒÉn, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "Tr·∫£ l·ªùi t·ªïng h·ª£p t·ª´ nhi·ªÅu ngu·ªìn",
                "difficulty": "multi_paragraph_reading"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.6,
                    max_output_tokens=4000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_multi_paragraph_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Multi-Paragraph Reading data: {e}")
            return self._generate_fallback_multi_paragraph_data(topic, num_samples)

    def generate_multi_hop_reasoning_data(self, legal_text: str, topic: str, num_samples: int = 10) -> List[Dict[str, Any]]:
        """Sinh d·ªØ li·ªáu Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n logic"""
        
        prompt = f"""
        D·ª±a tr√™n vƒÉn b·∫£n lu·∫≠t sau v·ªÅ ch·ªß ƒë·ªÅ "{topic}":
        
        {legal_text}
        
        H√£y t·∫°o {num_samples} c√¢u h·ªèi d·∫°ng Multi-Hop Reasoning - ph·ª©c t·∫°p nh·∫•t, y√™u c·∫ßu nhi·ªÅu b∆∞·ªõc suy lu·∫≠n logic.
        
        Y√™u c·∫ßu:
        - C√¢u h·ªèi c·∫ßn nhi·ªÅu b∆∞·ªõc suy lu·∫≠n logic ƒë·ªÉ tr·∫£ l·ªùi
        - Ph·∫£i k·∫øt h·ª£p hi·ªÉu kh√°i ni·ªám + ƒë·ªçc nhi·ªÅu ƒëo·∫°n + suy lu·∫≠n logic
        - Qu√° tr√¨nh reasoning ph·∫£i r√µ r√†ng v√† c√≥ th·ªÉ gi·∫£i th√≠ch ƒë∆∞·ª£c
        - M·ªñI C√ÇU H·ªéI PH·∫¢I ƒê·ªòC L·∫¨P, KH√îNG ƒê∆Ø·ª¢C D√ôNG "lu·∫≠t n√†y", "vƒÉn b·∫£n n√†y" m√† ph·∫£i n√≥i r√µ t√™n lu·∫≠t/vƒÉn b·∫£n c·ª• th·ªÉ
        - C√¢u h·ªèi ph·∫£i r√µ r√†ng, ng∆∞·ªùi ƒë·ªçc kh√¥ng c·∫ßn bi·∫øt context tr∆∞·ªõc
        
        Format mong mu·ªën (CH·ªà 3 TR∆Ø·ªúNG):
        [
            {{
                "question": "C√¢u h·ªèi ph·ª©c t·∫°p c·∫ßn suy lu·∫≠n nhi·ªÅu b∆∞·ªõc, c√≥ t√™n lu·∫≠t c·ª• th·ªÉ",
                "answer": "K·∫øt lu·∫≠n cu·ªëi c√πng v·ªõi gi·∫£i th√≠ch qu√° tr√¨nh suy lu·∫≠n",
                "difficulty": "multi_hop_reasoning"
            }}
        ]
        """
        
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.7,
                    max_output_tokens=5000
                )
            )
            
            content = response.text
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return self._generate_fallback_multi_hop_data(topic, num_samples)
                
        except Exception as e:
            print(f"L·ªói khi sinh Multi-Hop Reasoning data: {e}")
            return self._generate_fallback_multi_hop_data(topic, num_samples)
    
    def _generate_fallback_word_matching_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Word Matching m·∫´u khi API l·ªói"""
        templates = [
            {
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, {topic} l√† g√¨?",
                "answer": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, {topic} ƒë∆∞·ª£c quy ƒë·ªãnh l√†...",
                "difficulty": "word_matching"
            },
            {
                "question": f"Ai c√≥ th·∫©m quy·ªÅn quy·∫øt ƒë·ªãnh v·ªÅ {topic} theo quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t?",
                "answer": f"Th·∫©m quy·ªÅn v·ªÅ {topic} thu·ªôc v·ªÅ c∆° quan...",
                "difficulty": "word_matching"
            }
        ]
        
        result = []
        for i in range(num_samples):
            template = templates[i % len(templates)]
            result.append({
                "question": template["question"],
                "answer": template["answer"] + f" (M·∫´u {i+1})",
                "difficulty": template["difficulty"]
            })
        
        return result
    
    def _generate_fallback_concept_understanding_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Concept Understanding m·∫´u khi API l·ªói"""
        result = []
        for i in range(num_samples):
            result.append({
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, trong tr∆∞·ªùng h·ª£p n√†o th√¨ h√†nh vi li√™n quan ƒë·∫øn {topic} ƒë∆∞·ª£c coi l√† vi ph·∫°m?",
                "answer": f"Theo quy ƒë·ªãnh c·ªßa Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, h√†nh vi vi ph·∫°m v·ªÅ {topic} bao g·ªìm c√°c tr∆∞·ªùng h·ª£p...",
                "difficulty": "concept_understanding"
            })
        
        return result
    
    def _generate_fallback_multi_paragraph_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Multi-Paragraph Reading m·∫´u khi API l·ªói"""
        result = []
        for i in range(num_samples):
            result.append({
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, quy tr√¨nh ho√†n ch·ªânh ƒë·ªÉ x·ª≠ l√Ω v·∫•n ƒë·ªÅ {topic} nh∆∞ th·∫ø n√†o?",
                "answer": f"Theo quy ƒë·ªãnh c·ªßa Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, quy tr√¨nh x·ª≠ l√Ω {topic} bao g·ªìm nhi·ªÅu giai ƒëo·∫°n...",
                "difficulty": "multi_paragraph_reading"
            })
        
        return result
    
    def _generate_fallback_multi_hop_data(self, topic: str, num_samples: int) -> List[Dict[str, Any]]:
        """T·∫°o d·ªØ li·ªáu Multi-Hop Reasoning m·∫´u khi API l·ªói"""
        result = []
        for i in range(num_samples):
            result.append({
                "question": f"Theo Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô, ph√¢n t√≠ch t√¨nh hu·ªëng ph·ª©c t·∫°p v·ªÅ {topic} v√† ƒë∆∞a ra gi·∫£i ph√°p ph√°p l√Ω ph√π h·ª£p",
                "answer": f"K·∫øt lu·∫≠n v·ªÅ t√¨nh hu·ªëng {topic}: D·ª±a tr√™n vi·ªác x√°c ƒë·ªãnh c√°c kh√°i ni·ªám ph√°p l√Ω li√™n quan, t√¨m hi·ªÉu quy ƒë·ªãnh t·ª´ nhi·ªÅu ƒëi·ªÅu lu·∫≠t kh√°c nhau, ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c√°c quy ƒë·ªãnh, v√† √°p d·ª•ng logic ph√°p l√Ω ƒë·ªÉ k·∫øt lu·∫≠n...",
                "difficulty": "multi_hop_reasoning"
            })
        
        return result
