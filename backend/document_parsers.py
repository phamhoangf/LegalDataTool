# --- H√†m x·ª≠ l√Ω ƒë·∫ßu v√†o file/crawl cho backend ---
def process_input(input_data, input_type, title, parser=None):
    """
    X·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o (file ho·∫∑c crawl):
    - input_data: ƒë∆∞·ªùng d·∫´n file ho·∫∑c chu·ªói text crawl
    - input_type: 'pdf', 'docx', 'txt', 'text' (crawl)
    - title: ti√™u ƒë·ªÅ vƒÉn b·∫£n
    - parser: instance c·ªßa LegalDocumentParser (n·∫øu c√≥)
    """
    if input_type in ['pdf', 'docx', 'txt']:
        return process_uploaded_file(input_data, input_type, title, parser)
    elif input_type == 'text':
        if parser is None:
            from document_parsers import LegalDocumentParser
            parser = LegalDocumentParser()
        return parser.parse_document(title, input_data)
    else:
        raise ValueError("Unsupported input type")
# --- C√°c h√†m x·ª≠ l√Ω file ƒëa ngu·ªìn, kh√¥ng ·∫£nh h∆∞·ªüng code g·ªëc ---
import pdfplumber
import docx
import os

def extract_text_from_pdf(pdf_path):
    """Tr√≠ch xu·∫•t text t·ª´ file PDF"""
    with pdfplumber.open(pdf_path) as pdf:
        return "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])

def extract_text_from_docx(docx_path):
    """Tr√≠ch xu·∫•t text t·ª´ file DOCX"""
    doc = docx.Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs])

def extract_text_from_txt(txt_path):
    """Tr√≠ch xu·∫•t text t·ª´ file TXT"""
    with open(txt_path, encoding='utf-8') as f:
        return f.read()

def process_uploaded_file(file_path, file_type, title, parser=None):
    """
    Nh·∫≠n file b·∫•t k·ª≥, chuy·ªÉn v·ªÅ text, parse b·∫±ng LegalDocumentParser
    file_type: 'pdf', 'docx', 'txt'
    title: ti√™u ƒë·ªÅ vƒÉn b·∫£n
    parser: instance c·ªßa LegalDocumentParser (n·∫øu c√≥), n·∫øu kh√¥ng s·∫Ω t·∫°o m·ªõi
    """
    if file_type == 'pdf':
        text = extract_text_from_pdf(file_path)
    elif file_type == 'docx':
        text = extract_text_from_docx(file_path)
    elif file_type == 'txt':
        text = extract_text_from_txt(file_path)
    else:
        raise ValueError("Unsupported file type")
    if parser is None:
        from document_parsers import LegalDocumentParser
        parser = LegalDocumentParser()
    return parser.parse_document(title, text)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Legal Document Parser Module
Ch·ª©a c√°c class v√† function ƒë·ªÉ parse vƒÉn b·∫£n ph√°p lu·∫≠t th√†nh c·∫•u tr√∫c JSON
"""

import re
import json
import random
from typing import Dict, List, Any

"""
Legal Document Parser Module - Version with Header Normalization & Quote State Tracking
"""

import re
import pandas as pd
from typing import Dict, List, Any, Tuple

class LegalDocumentParser:
    """
    [N√ÇNG C·∫§P L·∫¶N 3] Th√™m c∆° ch·∫ø chu·∫©n h√≥a header ƒëa d√≤ng (ƒêi·ªÅu \n 1) v√†
    x·ª≠ l√Ω c·∫•u tr√∫c ƒêi·ªÅu -> ƒêi·ªÉm (kh√¥ng c√≥ Kho·∫£n).
    """
    
    def __init__(self):
        self.patterns = {
            'chuong_header': r'^Ch∆∞∆°ng\s+([IVXLCDM]+)',
            'muc_header': r'^\s*M·ª•c\s+(\d+)',
            'dieu_header': r'(?m)^\s*ƒêi·ªÅu(?:\s|\n)*(\d+)\.?\s*',
            'khoan_header': r'^\s*(\d+)[\.\-]?\s*(.*)',
            'diem_header': r'^\s*([a-z])\)\s*(.*)'
        }
        self.MIN_UNIT_LENGTH = 800
        self.MAX_UNIT_LENGTH = 1200


    def _normalize_khoan_headers(self, lines: List[str]) -> List[str]:
        """
        [H√ÄM C≈®] Ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ g·ªôp c√°c header c·ªßa Kho·∫£n b·ªã ng·∫Øt d√≤ng.
        """
        content_str = "\n".join(lines)
        normalized_content = re.sub(r'\n\s*(\d+)\s*\n\s*\.\s*', r'\n\1. ', content_str)
        return normalized_content.split('\n')

    def _normalize_multiline_headers(self, content: str) -> str:
        """
        [H√ÄM M·ªöI] Ti·ªÅn x·ª≠ l√Ω ƒë·ªÉ g·ªôp c√°c header ch√≠nh b·ªã ng·∫Øt d√≤ng.
        V√≠ d·ª•: "ƒêi·ªÅu\n1" -> "ƒêi·ªÅu 1", "Ch∆∞∆°ng\nI" -> "Ch∆∞∆°ng I"
        S·ª≠ d·ª•ng c·ªù re.MULTILINE ƒë·ªÉ `^` kh·ªõp v·ªõi ƒë·∫ßu m·ªói d√≤ng.
        """
        # G·ªôp d√≤ng cho Ch∆∞∆°ng, M·ª•c, ƒêi·ªÅu
        content = re.sub(r'^(Ch∆∞∆°ng|M·ª•c|ƒêi·ªÅu)\s*\n\s*([IVXLCDM\d]+)', r'\1 \2', content, flags=re.MULTILINE | re.IGNORECASE)
        return content

    def parse_document(self, title: str, content: str) -> Dict[str, Any]:
        """[C√ì S·ª¨A L·ªñI] C·∫§P 1: T√°ch kh·ªëi ƒêi·ªÅu, b·ªè qua c√°c header 'ƒêi·ªÅu' trong tr√≠ch d·∫´n."""
        print(f"üìÑ Parsing document: {title}")
        content = self._clean_content(content)
        
        # # << B∆Ø·ªöC S·ª¨A L·ªñI M·ªöI >>
        content = self._normalize_multiline_headers(content)

        lines = content.split('\n')
        document_structure = {'title': title, 'articles': []}
        
        article_blocks, current_block_lines = [], []
        parsing_context = {'chuong': None, 'muc': None}
        last_valid_article_num = 0
        in_quote = False
        
        i = 0
        while i < len(lines):
            line = lines[i]
            # Lo·∫°i b·ªè d·∫•u ch·∫•m sau s·ªë th·ª© t·ª± ƒêi·ªÅu n·∫øu c√≥, ƒë·ªÉ x·ª≠ l√Ω nh·∫•t qu√°n
            # line = re.sub(r'^(ƒêi·ªÅu\s+\d+)\.', r'\1', line)
            line = re.sub(r'^\s*(ƒêi·ªÅu\s+\d+)\.', r'\1', line)

            is_header_candidate = not in_quote
            chuong_match = is_header_candidate and re.match(self.patterns['chuong_header'], line.strip(), re.IGNORECASE)
            if chuong_match:
                if current_block_lines: article_blocks.append((parsing_context.copy(), current_block_lines))
                chuong_id = chuong_match.group(1); i, chuong_title = self._extract_multiline_title(lines, i)
                chuong_title = re.sub(self.patterns['chuong_header'], '', chuong_title, 1, re.IGNORECASE).strip()
                parsing_context['chuong'] = f"Ch∆∞∆°ng {chuong_id}: {chuong_title}"; parsing_context['muc'] = None; 
                # last_valid_article_num = 0; current_block_lines = []
                i += 1; continue
            muc_match = is_header_candidate and re.match(self.patterns['muc_header'], line.strip(), re.IGNORECASE)
            if muc_match:
                if current_block_lines: article_blocks.append((parsing_context.copy(), current_block_lines))
                muc_id = muc_match.group(1); i, muc_title = self._extract_multiline_title(lines, i)
                muc_title = re.sub(self.patterns['muc_header'], '', muc_title, 1, re.IGNORECASE).strip()
                parsing_context['muc'] = f"M·ª•c {muc_id}: {muc_title}"; 
                # last_valid_article_num = 0; current_block_lines = []
                i += 1; continue
            # dieu_match = is_header_candidate and re.match(self.patterns['dieu_header'], line)
            dieu_match = is_header_candidate and re.match(self.patterns['dieu_header'], line.strip())

            is_new_valid_article = False
            if dieu_match and int(dieu_match.group(1)) == last_valid_article_num + 1: is_new_valid_article = True
            if is_new_valid_article:
                if current_block_lines: article_blocks.append((parsing_context.copy(), current_block_lines))
                current_block_lines = [line]; last_valid_article_num = int(dieu_match.group(1))
            elif line.strip() or current_block_lines:
                current_block_lines.append(line)
            
            quote_char_count = line.count('‚Äú') + line.count('‚Äù')
            # quote_char_count = sum(line.count(q) for q in self.ALL_QUOTES)
            if quote_char_count % 2 != 0: in_quote = not in_quote
            i += 1
        if current_block_lines: article_blocks.append((parsing_context.copy(), current_block_lines))
        for context, block_lines in article_blocks:
            if block_lines and re.match(self.patterns['dieu_header'], block_lines[0]):
                article_object = self._process_article_block(title, context, block_lines)
                if article_object: document_structure['articles'].append(article_object)
        print(f"‚úÖ Parsed successfully: Found and processed {len(document_structure['articles'])} articles."); 
        return document_structure
        # return len(document_structure['articles']), document_structure

    def _parse_article_content(self, article_number: str, base_path: str, article_content_lines: List[str]) -> List[Dict]:
        """[C√ì S·ª¨A L·ªñI] C·∫§P 2: Chu·∫©n h√≥a header, t√°ch kh·ªëi Kho·∫£n ho·∫∑c x·ª≠ l√Ω tr·ª±c ti·∫øp ƒêi·ªÉm."""
        article_content_lines = self._normalize_khoan_headers(article_content_lines)
        
        units = []
        khoan_blocks_of_lines, current_block = [], []
        last_valid_khoan_num = 0
        in_quote = False
        for line in article_content_lines:
            is_new_valid_khoan = False
            if not in_quote:
                match = re.match(self.patterns['khoan_header'], line.strip())
                if match:
                    current_num = int(match.group(1))
                    if current_num == last_valid_khoan_num + 1: is_new_valid_khoan = True
            if is_new_valid_khoan:
                if current_block: khoan_blocks_of_lines.append(current_block)
                current_block = [line]; last_valid_khoan_num = current_num
            elif line.strip() or current_block:
                if not current_block and not line.strip(): continue
                current_block.append(line)
            quote_char_count = line.count('‚Äú') + line.count('‚Äù')
            if quote_char_count % 2 != 0: in_quote = not in_quote
        if current_block: khoan_blocks_of_lines.append(current_block)
        
        # <<< S·ª¨A L·ªñI LOGIC FALLBACK ƒêA C·∫§P >>>
        if last_valid_khoan_num == 0:
            # N·∫øu kh√¥ng c√≥ Kho·∫£n, ki·ªÉm tra xem c√≥ c·∫•u tr√∫c ƒêi·ªÉm kh√¥ng
            has_diem_structure = any(re.match(self.patterns['diem_header'], line.strip()) for line in article_content_lines)
            
            if has_diem_structure:
                # N·∫øu c√≥ ƒêi·ªÉm, x·ª≠ l√Ω to√†n b·ªô n·ªôi dung ƒêi·ªÅu nh∆∞ m·ªôt "Kho·∫£n" kh√¥ng t√™n
                # Ta truy·ªÅn m·ªôt list c√°c d√≤ng gi·∫£ m·∫°o v√†o _group_diem_within_khoan
                # ƒë·ªÉ h√†m ƒë√≥ c√≥ th·ªÉ ch·∫°y m√† kh√¥ng c·∫ßn header Kho·∫£n th·∫≠t.
                # D√≤ng header gi·∫£ "0. " s·∫Ω b·ªã b·ªè qua khi t·∫°o intro_text.
                fake_khoan_lines = ["0. "] + article_content_lines
                diem_units = self._group_diem_within_khoan(article_number, base_path, fake_khoan_lines)
                # S·ª≠a l·∫°i path v√† source_khoan cho ƒë√∫ng
                for unit in diem_units:
                    unit['path'] = unit['path'].replace(" > Kho·∫£n 0", "")
                    unit['source_khoan'] = 'N/A'
                return diem_units
            else:
                # N·∫øu kh√¥ng c√≥ Kho·∫£n v√† kh√¥ng c√≥ ƒêi·ªÉm, coi c·∫£ ƒêi·ªÅu l√† 1 unit
                full_content = "\n".join(article_content_lines).strip()
                if full_content:
                    units.append({'path': base_path, 'content': full_content, 'content_length_no_spaces': len(re.sub(r'\s', '', full_content)), 'source_article': article_number, 'source_khoan': 'N/A', 'source_diem': 'N/A'})
                return units

        for khoan_lines in khoan_blocks_of_lines:
            khoan_units = self._group_diem_within_khoan(article_number, base_path, khoan_lines)
            units.extend(khoan_units)
        return units

    def _group_diem_within_khoan(self, article_number: str, base_path: str, khoan_lines: List[str]) -> List[Dict]:
        """[C√ì S·ª¨A L·ªñI] C·∫§P 3: X·ª≠ l√Ω Kho·∫£n, b·ªè qua header 'ƒêi·ªÉm' trong tr√≠ch d·∫´n."""
        units = []
        khoan_header_match = re.match(self.patterns['khoan_header'], khoan_lines[0].strip())
        # S·ª≠a ƒë·ªïi: N·∫øu kh√¥ng kh·ªõp header, v·∫´n c√≥ th·ªÉ x·ª≠ l√Ω (tr∆∞·ªùng h·ª£p ƒêi·ªÅu -> ƒêi·ªÉm)
        khoan_id = khoan_header_match.group(1) if khoan_header_match else "0"

        diem_blocks_of_lines, current_block = [], []
        # N·∫øu kh√¥ng c√≥ header Kho·∫£n, to√†n b·ªô d√≤ng ƒë·∫ßu ti√™n l√† n·ªôi dung
        khoan_intro_lines = [khoan_lines[0]] if khoan_header_match else []
        content_start_index = 1 if khoan_header_match else 0
        
        diem_started = False
        in_quote = False
        for line in khoan_lines[content_start_index:]:
            is_new_diem = False
            if not in_quote:
                match = re.match(self.patterns['diem_header'], line.strip())
                if match and not re.match(r'^\d+[a-z]\.', line.strip()):
                    is_new_diem = True
            if is_new_diem:
                if not diem_started:
                    diem_started = True; current_block = [line]
                else:
                    if current_block: diem_blocks_of_lines.append(current_block)
                    current_block = [line]
            elif diem_started:
                current_block.append(line)
            else:
                khoan_intro_lines.append(line)
            quote_char_count = line.count('‚Äú') + line.count('‚Äù')
            if quote_char_count % 2 != 0: in_quote = not in_quote
        if current_block: diem_blocks_of_lines.append(current_block)
        
        if not diem_started:
            khoan_content = "\n".join(khoan_lines).strip()
            # Ch·ªâ t·∫°o unit n·∫øu ƒë√¢y l√† m·ªôt Kho·∫£n th·ª±c s·ª± (c√≥ header)
            if khoan_content and khoan_header_match:
                units.append({'path': f"{base_path} > Kho·∫£n {khoan_id}", 'content': khoan_content, 'content_length_no_spaces': len(re.sub(r'\s', '', khoan_content)), 'source_article': article_number, 'source_khoan': khoan_id, 'source_diem': 'N/A'})
            return units
            
        khoan_intro_text = "\n".join(khoan_intro_lines).strip()
        # B·ªè header gi·∫£ "0." n·∫øu c√≥
        if khoan_intro_text == "0.": khoan_intro_text = ""
        
        current_group_blocks, current_group_length = [], 0
        for block in diem_blocks_of_lines:
            block_content = "\n".join(block).strip(); block_length = len(re.sub(r'\s', '', block_content))
            if not current_group_blocks:
                current_group_blocks.append(block); current_group_length = block_length
            elif current_group_length + block_length > self.MAX_UNIT_LENGTH and current_group_length >= self.MIN_UNIT_LENGTH:
                unit = self._create_unit_from_diem_group(article_number, base_path, khoan_id, khoan_intro_text, current_group_blocks)
                if unit: units.append(unit)
                current_group_blocks, current_group_length = [block], block_length
            else:
                current_group_blocks.append(block); current_group_length += block_length
            if current_group_length >= self.MIN_UNIT_LENGTH:
                unit = self._create_unit_from_diem_group(article_number, base_path, khoan_id, khoan_intro_text, current_group_blocks)
                if unit: units.append(unit)
                current_group_blocks, current_group_length = [], 0
        if current_group_blocks:
            unit = self._create_unit_from_diem_group(article_number, base_path, khoan_id, khoan_intro_text, current_group_blocks)
            if unit: units.append(unit)
        return units

    # ==============================================================================
    # C√ÅC H√ÄM TR·ª¢ GI√öP (KH√îNG THAY ƒê·ªîI)
    # ==============================================================================
    def _extract_multiline_title(self, lines: List[str], start_index: int) -> Tuple[int, str]:
        title_lines = [lines[start_index].strip()]; i = start_index + 1
        while i < len(lines):
            line = lines[i].strip()
            if re.match(self.patterns['dieu_header'], line) or re.match(self.patterns['muc_header'], line) or re.match(self.patterns['khoan_header'], line) or re.match(self.patterns['chuong_header'], line, re.IGNORECASE) or (not line and title_lines): break
            if line: title_lines.append(line)
            i += 1
        return i - 1, ' '.join(title_lines)
    def _process_article_block(self, doc_title: str, context: Dict, article_lines: List[str]) -> Dict:
        match = re.match(self.patterns['dieu_header'], article_lines[0])
        if not match: return None
        
        article_number = match.group(1)
        # T√°ch ti√™u ƒë·ªÅ c√≥ th·ªÉ c√≥ tr√™n c√πng d√≤ng v·ªõi 'ƒêi·ªÅu X.'
        title_on_first_line = re.sub(self.patterns['dieu_header'], '', article_lines[0]).strip()
        title_lines = [title_on_first_line] if title_on_first_line else []
        
        content_start_index = 1
        for i in range(1, len(article_lines)):
            line_strip = article_lines[i].strip()
            
            # ƒêi·ªÅu ki·ªán d·ª´ng 1: G·∫∑p m·ªôt c·∫•u tr√∫c r√µ r√†ng nh∆∞ Kho·∫£n ho·∫∑c ƒêi·ªÉm.
            is_content_marker = (
                re.match(self.patterns['khoan_header'], line_strip) or
                re.match(self.patterns['diem_header'], line_strip)
            )
            
            # ƒêi·ªÅu ki·ªán d·ª´ng 2: G·∫∑p m·ªôt ƒëo·∫°n vƒÉn m·ªõi.
            # Ch·ªâ coi l√† ƒëo·∫°n vƒÉn m·ªõi n·∫øu ƒë√£ c√≥ √≠t nh·∫•t m·ªôt d√≤ng ti√™u ƒë·ªÅ.
            has_title = any(t.strip() for t in title_lines)
            is_new_paragraph = has_title and re.match(r'^[A-Z√Ä-·ª∏]', line_strip)

            if is_content_marker or is_new_paragraph:
                break

            # N·∫øu ch∆∞a d·ª´ng, ti·∫øp t·ª•c th√™m d√≤ng n√†y v√†o ti√™u ƒë·ªÅ
            if line_strip:
                title_lines.append(line_strip)
            content_start_index = i + 1
            
        article_title = ' '.join(filter(None, title_lines))
        article_content_lines = article_lines[content_start_index:]
        
        path_parts = [doc_title]
        if context.get('chuong'): path_parts.append(context['chuong'])
        if context.get('muc'): path_parts.append(context['muc'])
        path_parts.append(f"ƒêi·ªÅu {article_number}: {article_title}"); base_path = " > ".join(path_parts)
        
        units = self._parse_article_content(article_number, base_path, article_content_lines)
        return {'number': article_number, 'title': article_title, 'units': units}

    def _create_unit_from_diem_group(self, article_number: str, base_path: str, khoan_id: str, khoan_intro_text: str, group_of_diem_blocks: List[List[str]]) -> Dict:
        if not group_of_diem_blocks: return None
        first_diem_match = re.match(self.patterns['diem_header'], group_of_diem_blocks[0][0].strip()); last_diem_match = re.match(self.patterns['diem_header'], group_of_diem_blocks[-1][0].strip())
        if not first_diem_match or not last_diem_match: return None
        start_diem_id, end_diem_id = first_diem_match.group(1), last_diem_match.group(1)
        diem_range_str = start_diem_id if start_diem_id == end_diem_id else f"{start_diem_id}-{end_diem_id}"
        combined_diem_content = "\n\n".join(["\n".join(block).strip() for block in group_of_diem_blocks])
        final_content = (f"{khoan_intro_text}\n{combined_diem_content}").strip()
        path = f"{base_path} > Kho·∫£n {khoan_id} > ƒêi·ªÉm {diem_range_str}"
        return {'path': path, 'content': final_content, 'content_length_no_spaces': len(re.sub(r'\s', '', final_content)), 'source_article': article_number, 'source_khoan': khoan_id, 'source_diem': diem_range_str}
    def _clean_content(self, content: str) -> str: 
        return re.sub(r'\r\n|\r', '\n', content)
    
    def get_all_units(self, parsed_data: Dict) -> List[Dict]:
        """L·∫•y t·∫•t c·∫£ units t·ª´ parsed structure"""
        all_units = []
        
        if 'articles' not in parsed_data:
            return all_units
            
        for article in parsed_data['articles']:
            if 'units' in article:
                for unit in article['units']:
                    # Convert unit format for data generator
                    converted_unit = {
                        'path': unit.get('path', ''),
                        'content': unit.get('content', ''),
                        'content_length': unit.get('content_length_no_spaces', 0),
                        'source_article': unit.get('source_article', ''),
                        'source_khoan': unit.get('source_khoan', 'N/A'),
                        'source_diem': unit.get('source_diem', 'N/A')
                    }
                    all_units.append(converted_unit)
        
        return all_units


